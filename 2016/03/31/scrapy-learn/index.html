<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="learn python and invest"><meta name="keywords" content="金融, Python"><title>Scrapy框架初探 | KeKeFund</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.3.0"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=1.3.0"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Scrapy框架初探</h1><a id="logo" href="/.">KeKeFund</a><p class="description">金融 · Python · 技术博客</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Scrapy框架初探</h1><div class="post-meta">Mar 31, 2016<span> | </span><span class="category"><a href="/categories/Python/">Python</a></span><span> | </span><span id="busuanzi_value_page_pv"></span><span> Hits</span></div><div class="post-content"><p>scrapy爬虫框架在业内大大有名，自己写过静态网页和动态网页的爬虫，一直没拿scrapy来写，近来看了scrapy的官方文档，了解了大致的流程，故拿来练手实践了一个项目。</p>
<p>本文主要抓取股吧的文章，内容包括:</p>
<ul>
<li>定义抓取Spider</li>
<li>数据字段的定义</li>
<li>内容解析</li>
<li>数据存储到mysql</li>
<li>PyCharm调试scrapy<a id="more"></a>
</li>
</ul>
<h1 id="一、定义抓取Spider"><a href="#一、定义抓取Spider" class="headerlink" title="一、定义抓取Spider"></a>一、定义抓取Spider</h1><p>创建一个新的Spider</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">scrapy </span>startproject tutorial</div><div class="line"><span class="keyword">scrapy </span>genspider guba_spider eastmoney.com</div></pre></td></tr></table></figure>
<p>默认创建的Spider是继承与BaseSpider，一般我们继承功能更多的CrawlSpider。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GubaSpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">'guba'</span></div><div class="line">    allowed_domains = [<span class="string">'eastmoney.com'</span>]</div></pre></td></tr></table></figure>
<p>定义好GubaSpider类后，然后要指定开始网页start_urls和rules抓取网页规则。</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">start_urls = [</div><div class="line">    <span class="string">'http://guba.eastmoney.com/default_%d.html'</span> % index <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)</div><div class="line">]</div><div class="line"></div><div class="line">rules = (</div><div class="line">    <span class="comment"># 提取匹配 文章 的链接并使用spider的parse_article方法进行分析</span></div><div class="line">    Rule(LinkExtractor(allow=(<span class="string">r'^http://guba.eastmoney.com/news.'</span>, )), callback=<span class="string">'parse_article'</span>),</div><div class="line"></div><div class="line">    Rule(LinkExtractor(allow=(<span class="string">r'^http://iguba.eastmoney.com/\d+.'</span>,)), callback=<span class="string">'parse_auther'</span>)</div><div class="line">)</div></pre></td></tr></table></figure>
<p>我们主要爬取的是下面这块区域：</p>
<p><img src="http://pic.cbbing.com/2016-03-31/guba.png" alt=""></p>
<p>rules规则定义了两类链接，一个是文章链接，一个是作者链接。</p>
<p>链接的规则可以通过Chrome的开发者工具得到，如下：<br><img src="http://pic.cbbing.com/2016-03-31/guba2.png" alt=""></p>
<p>Rule中的LinkExtractor是从网页(scrapy.http.Response)中抽取满足allow条件的链接，callback回调至指定函数。</p>
<p>启动Spider，执行的流程是：</p>
<ol>
<li><p>从start_urls中开始爬取网页，</p>
</li>
<li><p>找到满足文章链接的规则，跳转到self.parse_article()函数进一步处理。</p>
</li>
</ol>
<h1 id="二、定义数据字段"><a href="#二、定义数据字段" class="headerlink" title="二、定义数据字段"></a>二、定义数据字段</h1><p>在scrapy目录下的items.py中定义结构化数据字段。</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="type">ArticleItem</span>(<span class="title">scrapy</span>.<span class="type">Item</span>):</span></div><div class="line">    uuid = scrapy.<span class="type">Field</span>() # 唯一标识符</div><div class="line">    user_id = scrapy.<span class="type">Field</span>()</div><div class="line">    user_name = scrapy.<span class="type">Field</span>()</div><div class="line">    title = scrapy.<span class="type">Field</span>()</div><div class="line">    classify = scrapy.<span class="type">Field</span>()</div><div class="line">    content = scrapy.<span class="type">Field</span>()</div><div class="line">    readed_count = scrapy.<span class="type">Field</span>()</div><div class="line">    comment_count = scrapy.<span class="type">Field</span>()</div><div class="line">    href = scrapy.<span class="type">Field</span>()</div><div class="line">    source = scrapy.<span class="type">Field</span>()</div><div class="line">    published_date = scrapy.<span class="type">Field</span>()</div><div class="line">    scrapy_date = scrapy.<span class="type">Field</span>()    # 抓取日期</div></pre></td></tr></table></figure>
<h1 id="三、内容解析"><a href="#三、内容解析" class="headerlink" title="三、内容解析"></a>三、内容解析</h1><p>scrapy默认的是用xpath解析网页，由于对Beautifulsoup更熟悉，我在本文中用的Beautifulsoup来解析网页内容，道理都是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs</div><div class="line"><span class="keyword">import</span> re</div><div class="line"><span class="keyword">from</span> hashlib <span class="keyword">import</span> md5</div><div class="line"></div><div class="line"><span class="keyword">from</span> gubademo.items <span class="keyword">import</span> ArticleItem</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_article</span><span class="params">(self, response)</span>:</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"content news:%s"</span> % response.url</div><div class="line"></div><div class="line">    soup = bs(response.body, <span class="string">'lxml'</span>)</div><div class="line"></div><div class="line">    item = ArticleItem()</div><div class="line"></div><div class="line">    item[<span class="string">'title'</span>] = response.url</div><div class="line">    item[<span class="string">'href'</span>] = response.url</div><div class="line"></div><div class="line">    div_name = soup.find(<span class="string">'div'</span>,&#123;<span class="string">'id'</span>:<span class="string">'zwconttbn'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> div_name:</div><div class="line">        item[<span class="string">'user_name'</span>] = div_name.find(<span class="string">'a'</span>).text</div><div class="line">        <span class="keyword">if</span> div_name.find(<span class="string">'a'</span>).has_attr(<span class="string">'data-popper'</span>):</div><div class="line">            item[<span class="string">'user_id'</span>] = div_name.find(<span class="string">'a'</span>)[<span class="string">'data-popper'</span>]</div><div class="line"></div><div class="line">    span_stockname = soup.find(<span class="string">'span'</span>, &#123;<span class="string">'id'</span>:<span class="string">'stockname'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> span_stockname:</div><div class="line">        item[<span class="string">'classify'</span>] = span_stockname.find(<span class="string">'a'</span>).text</div><div class="line"></div><div class="line">    <span class="comment"># 内容</span></div><div class="line">    c_div = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'stockcodec'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> c_div:</div><div class="line">        item[<span class="string">'content'</span>] = <span class="string">''</span></div><div class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> c_div.strings:</div><div class="line">            item[<span class="string">'content'</span>] += s</div><div class="line"></div><div class="line">    <span class="comment"># 时间</span></div><div class="line">    t_div = soup.find(<span class="string">'div'</span>, &#123;<span class="string">'class'</span>:<span class="string">'zwfbtime'</span>&#125;)</div><div class="line">    <span class="keyword">if</span> t_div:</div><div class="line">        s1 = re.search(<span class="string">'\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125; \d&#123;2&#125;:\d&#123;2&#125;:\d&#123;2&#125;'</span>,t_div.text)</div><div class="line">        <span class="keyword">if</span> s1:</div><div class="line">            item[<span class="string">'published_date'</span>] = s1.group()</div><div class="line"></div><div class="line">    <span class="comment"># 阅读数和评论数</span></div><div class="line">    m = re.search(<span class="string">'num=(\d+).*?var count=\d+'</span>, response.body)</div><div class="line">    <span class="keyword">if</span> m:</div><div class="line">        item[<span class="string">'readed_count'</span>] = m.group(<span class="number">1</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        item[<span class="string">'readed_count'</span>] = <span class="string">u'0'</span></div><div class="line"></div><div class="line">    m = re.search(<span class="string">'var pinglun_num=(\d+)'</span>, response.body)</div><div class="line">    <span class="keyword">if</span> m:</div><div class="line">        item[<span class="string">'comment_count'</span>] = m.group(<span class="number">1</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        item[<span class="string">'comment_count'</span>] = <span class="string">u'0'</span></div><div class="line">    item[<span class="string">'source'</span>] = <span class="string">'guba_eastmoney'</span></div><div class="line">    item[<span class="string">'scrapy_date'</span>] =  GetNowTime()</div><div class="line">    item[<span class="string">'uuid'</span>] = md5(item[<span class="string">'href'</span>]).hexdigest()</div><div class="line"></div><div class="line">    <span class="keyword">print</span> item</div><div class="line">    <span class="keyword">yield</span> item</div></pre></td></tr></table></figure>
<h1 id="四、写入数据库"><a href="#四、写入数据库" class="headerlink" title="四、写入数据库"></a>四、写入数据库</h1><p>解析完数据，接下来是要保存数据以便以后分析使用。</p>
<p>自定义Pipeline，spider将item传递到pipeline，默认调用的是process_item()函数，我们可以在processs_item中根据item的类型进行差异化处理。</p>
<p>需要先在setting.py中设置如下内容，scrapy才能走着这一步。</p>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># Configure item pipelines</div><div class="line"># See http:<span class="comment">//scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">   #<span class="string">'wealth_tech.pipelines.DuplicatePipeline'</span>:<span class="number">200</span>,</div><div class="line">   <span class="string">'wealth_tech.pipelines.MySQLStorePipeline'</span>: <span class="number">300</span>,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>MySQLStorePipeline定义了一个article_items集合用于存储spider爬到的item，当items数量达到1000时，批量写入数据库。如果接受到item就单条写入数据库，会比批量写入慢很对，爬虫的效率会慢一个数量级。<br>存入mysql之前，先查询数据库，若不存在则insert，存在则update。<br>数据库的host,port等信息一般存在setting.py中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> MySQLdb</div><div class="line"><span class="keyword">import</span> MySQLdb.cursors</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> wealth_tech.settings <span class="keyword">as</span> settings</div><div class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</div><div class="line">engine = create_engine(<span class="string">'mysql+mysqldb://%s:%s@%s:%d/%s'</span> % (settings.MYSQL_USER, settings.MYSQL_PASSWD, settings.MYSQL_HOST, <span class="number">3306</span>, settings.MYSQL_DBNAME), connect_args=&#123;<span class="string">'charset'</span>:<span class="string">'utf8'</span>&#125;)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySQLStorePipeline</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    写入mysql数据库</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.article_items = &#123;&#125;</div><div class="line"></div><div class="line">    <span class="comment"># pipeline默认调用</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        <span class="keyword">print</span> spider.name</div><div class="line">        <span class="keyword">if</span> type(item) <span class="keyword">is</span> ArticleItem:</div><div class="line">            self.process_article_item(item, spider) <span class="comment"># 文章</span></div><div class="line"></div><div class="line">        <span class="keyword">return</span> item</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_article_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        保存文章</div><div class="line">        """</div><div class="line">        table = <span class="string">'article_guba_easymoney'</span></div><div class="line"></div><div class="line">        self.article_items.setdefault(spider.name, [])</div><div class="line">        self.article_items[spider.name].append(item)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> len(self.article_items[spider.name]) &gt;= <span class="number">1000</span>: <span class="comment"># 积累到1000条就写入数据库</span></div><div class="line"></div><div class="line">            conn=MySQLdb.connect(host=settings.MYSQL_HOST,user=settings.MYSQL_USER,passwd= settings.MYSQL_PASSWD,db=settings.MYSQL_DBNAME,charset=<span class="string">"utf8"</span>)</div><div class="line">            cursor = conn.cursor()</div><div class="line"></div><div class="line">            df = pd.read_sql(<span class="string">'select uuid from &#123;&#125;'</span>.format(table), engine)</div><div class="line">            uuids = df[<span class="string">'uuid'</span>].get_values()</div><div class="line">            uuids = set(uuids)</div><div class="line"></div><div class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> self.article_items[spider.name]:</div><div class="line">                <span class="keyword">try</span>:</div><div class="line">                    <span class="keyword">if</span> item[<span class="string">'uuid'</span>] <span class="keyword">not</span> <span class="keyword">in</span> uuids:</div><div class="line">                        <span class="comment"># 插入</span></div><div class="line">                        sql = <span class="string">'insert into &#123;&#125; values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)'</span>.format(table)</div><div class="line">                        param = (item[<span class="string">'uuid'</span>],item[<span class="string">'user_id'</span>],item[<span class="string">'user_name'</span>],item[<span class="string">'title'</span>],item[<span class="string">'classify'</span>],item[<span class="string">'content'</span>],</div><div class="line">                                 item[<span class="string">'readed_count'</span>],item[<span class="string">'comment_count'</span>],item[<span class="string">'href'</span>],item[<span class="string">'source'</span>],item[<span class="string">'published_date'</span>],item[<span class="string">'scrapy_date'</span>])</div><div class="line">                        n = cursor.execute(sql,param)</div><div class="line">                        <span class="keyword">print</span> <span class="string">'insert '</span>,n, item[<span class="string">'uuid'</span>]</div><div class="line">                    <span class="keyword">else</span>:</div><div class="line">                        <span class="comment">#更新</span></div><div class="line">                        sql = <span class="string">"update &#123;&#125; set fans_count=%s, article_count=%s,visit_count=%s,comment_count=%s,scrapy_date=%s where user_id='&#123;&#125;'"</span>.format(table, item[<span class="string">'user_id'</span>])</div><div class="line">                        <span class="comment">#print sql</span></div><div class="line">                        param = (item[<span class="string">'fans_count'</span>],item[<span class="string">'article_count'</span>],item[<span class="string">'visit_count'</span>],item[<span class="string">'comment_count'</span>],item[<span class="string">'scrapy_date'</span>])</div><div class="line">                        n = cursor.execute(sql,param)</div><div class="line">                        <span class="keyword">print</span> <span class="string">'update'</span>, n,item[<span class="string">'uuid'</span>]</div><div class="line">                <span class="keyword">except</span> Exception,e:</div><div class="line">                    <span class="keyword">print</span> e</div><div class="line"></div><div class="line">             <span class="comment">#提交</span></div><div class="line">            conn.commit()</div><div class="line">            <span class="comment">#关闭</span></div><div class="line">            conn.close()</div><div class="line">            self.article_items[spider.name] = []</div></pre></td></tr></table></figure>
<p>数据库呈现的结果：</p>
<p><img src="http://pic.cbbing.com/2016-03-31/guba3.png" alt=""></p>
<h1 id="五、PyCharm-Debug调试scrapy"><a href="#五、PyCharm-Debug调试scrapy" class="headerlink" title="五、PyCharm Debug调试scrapy"></a>五、PyCharm Debug调试scrapy</h1><p>scrapy通常在命令行里运行，但仅通过log显示的信息来调试时非常费劲的，程序猿需要的是单步调试，step by step。</p>
<p>在PyCharm中调试也是很容易的。</p>
<p>在scrapy项目的根目录下（与scrapy.cfg同级）新建一个文件run.py，内容如下：</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"></div><div class="line">from scrapy.cmdline import<span class="built_in"> execute</span></div><div class="line">execute()</div></pre></td></tr></table></figure>
<p>新建一个Run/Debug Configurations，Script选择run.py，Script parameters输入crawl guba。其中guba是在GubaSpider中定义的name。这样启动Debug就能单步调试了。<br><img src="http://pic.cbbing.com/2016-03-31/guba4.png" alt=""></p>
</div><div class="tags"><a href="/tags/Python/">Python</a><a href="/tags/scrapy/">scrapy</a></div><div class="post-nav"><a href="/2016/02/23/pandas-anlysis-basic/" class="pre">Pandas数据分析基础</a><a href="/2016/05/03/fund-qualification-examination/" class="next">基金从业资格考试笔记</a></div><div id="comments"><div id="uyan_frame"></div><script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2141135"></script></div><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></div><div class="layout-r"><div id="sidebar"><div class="widget"><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="请输入关键字..."/></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Finance/">Finance</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/IOS/">IOS</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Nginx/">Nginx</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Others/">Others</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/服务器/">服务器</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/架构/">架构</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/系统/">系统</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/运维/">运维</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/图片上传/" style="font-size: 15px;">图片上传</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/模式匹配/" style="font-size: 15px;">模式匹配</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/IP代理/" style="font-size: 15px;">IP代理</a> <a href="/tags/期货/" style="font-size: 15px;">期货</a> <a href="/tags/股票/" style="font-size: 15px;">股票</a> <a href="/tags/PyQt/" style="font-size: 15px;">PyQt</a> <a href="/tags/exe/" style="font-size: 15px;">exe</a> <a href="/tags/K线/" style="font-size: 15px;">K线</a> <a href="/tags/蜡烛图/" style="font-size: 15px;">蜡烛图</a> <a href="/tags/requests/" style="font-size: 15px;">requests</a> <a href="/tags/cookie/" style="font-size: 15px;">cookie</a> <a href="/tags/crawler/" style="font-size: 15px;">crawler</a> <a href="/tags/selenium/" style="font-size: 15px;">selenium</a> <a href="/tags/招聘/" style="font-size: 15px;">招聘</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/人工智能/" style="font-size: 15px;">人工智能</a> <a href="/tags/系统化交易/" style="font-size: 15px;">系统化交易</a> <a href="/tags/multiprocessing/" style="font-size: 15px;">multiprocessing</a> <a href="/tags/多进程/" style="font-size: 15px;">多进程</a> <a href="/tags/多线程/" style="font-size: 15px;">多线程</a> <a href="/tags/pytagcloud/" style="font-size: 15px;">pytagcloud</a> <a href="/tags/标签云/" style="font-size: 15px;">标签云</a> <a href="/tags/贝叶斯分类器/" style="font-size: 15px;">贝叶斯分类器</a> <a href="/tags/nltk/" style="font-size: 15px;">nltk</a> <a href="/tags/贝叶斯原理/" style="font-size: 15px;">贝叶斯原理</a> <a href="/tags/scrapy/" style="font-size: 15px;">scrapy</a> <a href="/tags/pandas/" style="font-size: 15px;">pandas</a> <a href="/tags/DataFrame/" style="font-size: 15px;">DataFrame</a> <a href="/tags/数据分析/" style="font-size: 15px;">数据分析</a> <a href="/tags/相似度评价/" style="font-size: 15px;">相似度评价</a> <a href="/tags/groupby/" style="font-size: 15px;">groupby</a> <a href="/tags/分组/" style="font-size: 15px;">分组</a> <a href="/tags/推荐算法/" style="font-size: 15px;">推荐算法</a> <a href="/tags/gensim/" style="font-size: 15px;">gensim</a> <a href="/tags/主题模型/" style="font-size: 15px;">主题模型</a> <a href="/tags/相似度/" style="font-size: 15px;">相似度</a> <a href="/tags/文档去重/" style="font-size: 15px;">文档去重</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/IOS开发/" style="font-size: 15px;">IOS开发</a> <a href="/tags/Objective-C/" style="font-size: 15px;">Objective-C</a> <a href="/tags/WKWebView/" style="font-size: 15px;">WKWebView</a> <a href="/tags/XCode/" style="font-size: 15px;">XCode</a> <a href="/tags/g2/" style="font-size: 15px;">g2</a> <a href="/tags/scrapyd/" style="font-size: 15px;">scrapyd</a> <a href="/tags/部署/" style="font-size: 15px;">部署</a> <a href="/tags/Scrapy/" style="font-size: 15px;">Scrapy</a> <a href="/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/tags/mac/" style="font-size: 15px;">mac</a> <a href="/tags/eclipse/" style="font-size: 15px;">eclipse</a> <a href="/tags/debug/" style="font-size: 15px;">debug</a> <a href="/tags/Django/" style="font-size: 15px;">Django</a> <a href="/tags/表单/" style="font-size: 15px;">表单</a> <a href="/tags/re/" style="font-size: 15px;">re</a> <a href="/tags/iMac/" style="font-size: 15px;">iMac</a> <a href="/tags/SSD/" style="font-size: 15px;">SSD</a> <a href="/tags/USB3-0/" style="font-size: 15px;">USB3.0</a> <a href="/tags/移动硬盘盒/" style="font-size: 15px;">移动硬盘盒</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/dockerfile/" style="font-size: 15px;">dockerfile</a> <a href="/tags/ireport/" style="font-size: 15px;">ireport</a> <a href="/tags/pdf/" style="font-size: 15px;">pdf</a> <a href="/tags/报告/" style="font-size: 15px;">报告</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/centos7/" style="font-size: 15px;">centos7</a> <a href="/tags/字体/" style="font-size: 15px;">字体</a> <a href="/tags/VNC/" style="font-size: 15px;">VNC</a> <a href="/tags/gnome/" style="font-size: 15px;">gnome</a> <a href="/tags/bokeh/" style="font-size: 15px;">bokeh</a> <a href="/tags/绘图/" style="font-size: 15px;">绘图</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/branch/" style="font-size: 15px;">branch</a> <a href="/tags/SourceTree/" style="font-size: 15px;">SourceTree</a> <a href="/tags/DjangoRestFramework/" style="font-size: 15px;">DjangoRestFramework</a> <a href="/tags/API/" style="font-size: 15px;">API</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/ownCloud/" style="font-size: 15px;">ownCloud</a> <a href="/tags/网盘/" style="font-size: 15px;">网盘</a> <a href="/tags/Splash/" style="font-size: 15px;">Splash</a> <a href="/tags/JavaScript/" style="font-size: 15px;">JavaScript</a> <a href="/tags/highcharts/" style="font-size: 15px;">highcharts</a> <a href="/tags/AI/" style="font-size: 15px;">AI</a> <a href="/tags/云计算/" style="font-size: 15px;">云计算</a> <a href="/tags/金融/" style="font-size: 15px;">金融</a> <a href="/tags/镜像/" style="font-size: 15px;">镜像</a> <a href="/tags/腾讯云/" style="font-size: 15px;">腾讯云</a> <a href="/tags/阿里云/" style="font-size: 15px;">阿里云</a> <a href="/tags/build/" style="font-size: 15px;">build</a> <a href="/tags/内网映射/" style="font-size: 15px;">内网映射</a> <a href="/tags/端口转发/" style="font-size: 15px;">端口转发</a> <a href="/tags/Registry/" style="font-size: 15px;">Registry</a> <a href="/tags/supervisor/" style="font-size: 15px;">supervisor</a> <a href="/tags/ftp/" style="font-size: 15px;">ftp</a> <a href="/tags/nginx/" style="font-size: 15px;">nginx</a> <a href="/tags/日志/" style="font-size: 15px;">日志</a> <a href="/tags/seafile/" style="font-size: 15px;">seafile</a> <a href="/tags/Dockerfile/" style="font-size: 15px;">Dockerfile</a> <a href="/tags/postman/" style="font-size: 15px;">postman</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-fei"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">七月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">三月 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">六月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">五月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">三月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">二月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">十二月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">十一月 2015</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/09/12/redash-use/">redash部署使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/29/jenkins-jobs/">jenkins 批量添加任务job</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/24/django-gunicorn/">Django中配置Gunicorn</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/31/attack-miner/">服务器被矿机程序攻击</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/15/api-test/">API自动化测试</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/24/lanproxy/">内网映射方案(lanproxy)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/24/seafile-server/">seafile服务器配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/03/30/supervisor/">Docker容器运行多条命令(supervisor)</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/12/06/scrapy-and-selenium/">scrapy+selenium爬取UC头条网站</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/10/27/nginx-log/">nginx日志分析</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.3.0" async></script><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |<a href="/atom.xml">订阅本站</a><span>联系博主：<a href="mailto:cbbing@163.com" target="_blank" class="fa fa-email"> </a><a href="http://weibo.com/cbbing" target="_blank" class="fa fa-weibo"></a><a href="https://github.com/cbbing" target="_blank" class="fa fa-github"> </a></span></p><p><span> Copyright &copy;<a href="/." rel="nofollow">Binger Chen</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span><span><a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> </a><span>本站总访问量<span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span>次</span></span></p></div><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div><script type="text/javascript" src="/js/search.json.js?v=1.3.0"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-69979657-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?336c5d7270cff49527647639835acb15";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></body></html>