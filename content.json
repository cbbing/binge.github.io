[{"title":"内网映射方案(lanproxy)","date":"2018-06-24T03:56:49.000Z","path":"2018/06/24/lanproxy/","text":"现状现在运营商提供的宽带服务，无论是动态IP，还是固定IP，默认都是禁止所有端口服务的（目前了解上海是这样的），在路由器上配置的端口映射和DMZ都失效。申请开通端口需要域名备案，过程比较繁琐。运营商这么做是为了防止个人随意开设各种非法服务，也防止黑客通过扫描器进行抓鸡和批量扫描。这样封禁，虽然一定程度上保证了我们的网络安全，比如说前段时间的勒索病毒正因为国内大部分用户没有独立的公网IP，并且操作系统最容易爆发漏洞的一些，135，139等端口被运营商封禁了，使得国内个人家庭电脑中招的概率小了很多；但是导致即使有公网IP，也无法使用常用端口向外网提供服务。 解决方案通过NAPT原理得知：NAPT实现了内网主机在没有公网IP的情况下访问公网主机。那么我们可以这样做：假设公网IP为23.23.23.23，内网IP为192.168.1.2。公网主机先监听80端口，监听这个端口是用于向外部提供一个HTTP服务，80是WEB服务器的默认端口。同时其他任意一个端口（这里我们假设为7777），监听这个端口是用于让内网服务器主动连接进来打通一个隧道。接着内网再主动向公网主机的7777发起一个请求，这样内网就成功与公网主机建立了一个连接通道。 然后当有任何客户端主动连接公网的80端口时，公网接收到连接请求之后马上把这连接请求通过先前建立好的隧道转发到内网主机，内网主机接收到来自隧道的数据包后再主动连接内网主机自身的80端口，连接成功之后将数据包原封不动地转发数据包给80端口，待HTTP服务器程序处理完这个数据包，生成了响应报文之后再原路转发回去，最终到达公网的80端口，然后返回给最开始请求公网服务器80端口的客户端。 大名鼎鼎的花生壳内网版以及nat123等内网穿透工具的原理基本就是如此。 https://juejin.im/entry/59f2ed94518825098951554c lanproxylanproxy是一个将局域网个人电脑、服务器代理到公网的内网穿透工具，目前仅支持tcp流量转发，可支持任何tcp上层协议（访问内网网站、本地支付接口调试、ssh访问、远程桌面…）。目前市面上提供类似服务的有花生壳、TeamViewer、GoToMyCloud等等，但要使用第三方的公网服务器就必须为第三方付费，并且这些服务都有各种各样的限制，此外，由于数据包会流经第三方，因此对数据安全也是一大隐患。 1，服务器配置服务器如腾讯云服务器、阿里云服务器，必须有独立IP。下载lanproxy-server-20171116.zip，解压后放到服务器上。server的配置文件放置在conf目录中，配置 config.properties123456789101112131415161718192021server.bind=0.0.0.0#与代理客户端通信端口server.port=4900#ssl相关配置server.ssl.enable=trueserver.ssl.bind=0.0.0.0server.ssl.port=4993server.ssl.jksPath=test.jksserver.ssl.keyStorePassword=123456server.ssl.keyManagerPassword=123456#这个配置可以忽略server.ssl.needsClientAuth=false#WEB在线配置管理相关信息config.server.bind=0.0.0.0config.server.port=8090config.admin.username=adminconfig.admin.password=admin 代理配置，打开地址 http://ip:8090 ，使用上面配置中配置的用户名密码登录 添加客户端: 配置客户端： 示例是将公网的80、443端口映射到内网主机的80、443端口。 2，Java客户端配置 Java client的配置文件放置在conf目录中，配置 config.properties123456789101112#与在proxy-server配置后台创建客户端时填写的秘钥保持一致；client.key=ssl.enable=truessl.jksPath=test.jksssl.keyStorePassword=123456#这里填写实际的proxy-server地址；没有服务器默认即可，自己有服务器的更换为自己的proxy-server（IP）地址server.host=lp.thingsglobal.org#proxy-server ssl默认端口4993，默认普通端口4900#ssl.enable=true时这里填写ssl端口，ssl.enable=false时这里填写普通端口server.port=4993 java客户端需要以下环境: 安装java1.7或以上环境 linux（mac）环境中运行bin目录下的 startup.sh windows环境中运行bin目录下的 startup.bat 3，其他平台客户端不用java客户端的可以使用下面提供的各个平台的客户端，省去安装java运行环境 源码地址https://github.com/ffay/lanproxy-go-client 发布包https://github.com/ffay/lanproxy-go-client/releases 普通端口连接12345678# mac 64位nohup ./client_darwin_amd64 -s SERVER_IP -p SERVER_PORT -k CLIENT_KEY &amp;# linux 64位nohup ./client_linux_amd64 -s SERVER_IP -p SERVER_PORT -k CLIENT_KEY &amp;# windows 64 位./client_windows_amd64.exe -s SERVER_IP -p SERVER_PORT -k CLIENT_KEY SSL端口连接12345678# mac 64位nohup ./client_darwin_amd64 -s SERVER_IP -p SERVER_SSL_PORT -k CLIENT_KEY -ssl true &amp;# linux 64位nohup ./client_linux_amd64 -s SERVER_IP -p SERVER_SSL_PORT -k CLIENT_KEY -ssl true &amp;# windows 64 位./client_windows_amd64.exe -s SERVER_IP -p SERVER_SSL_PORT -k CLIENT_KEY -ssl true 实测查看管理后台的数据统计，有访问流量，说明转发成功！","tags":[{"name":"内网映射","slug":"内网映射","permalink":"http://www.kekefund.com/tags/内网映射/"},{"name":"端口转发","slug":"端口转发","permalink":"http://www.kekefund.com/tags/端口转发/"}]},{"title":"seafile服务器配置","date":"2018-06-24T01:28:22.000Z","path":"2018/06/24/seafile-server/","text":"公司内部网盘分享方案。 目前开源的企业网盘方案有seafile，ownCloud。 Seafile 是一款开源的企业云盘，注重可靠性和性能。支持 Windows, Mac, Linux, iOS, Android 全平台。支持文件同步或者直接挂载到本地访问。 一、安装1、搭建seafile服务器seafile服务器主要支持Linux系统，包括Debian，Ubuntu，Centos等，也支持Windows系统（版本更新有滞后，人数超过25人会卡）。同时也提供 Docker安装方式，更加轻松的部署和更新Seafile服务。 本文采用Docker的部署方式。 配置文件：docker-compose.yml 1234567891011121314151617181920212223242526version: '3.4'services: seafile: image: seafileltd/seafile:latest volumes: - ./shared:/shared ports: - 80:80 - 8000:8000 environment: SEAFILE_SERVER_HOSTNAME: pan.xxx.com SEAFILE_ADMIN_EMAIL:xxx@163.com SEAFILE_ADMIN_PASSWORD: 123456 该配置文件将seafile的数据文件存储于当前目录的shared目录下，开放80和8000端口；环境变量设置了访问的主机IP或域名、设置了管理员账号和密码。 2、向Let’s encrypt申请SSL证书添加环境变量，并且添加443端口。 12SEAFILE_SERVER_LETSENCRYPT=true 这样完整的docker-compose.yml如下：123456789101112131415version: '3'services: seafile: image: seafileltd/seafile:latest volumes: - ./shared:/shared ports: - 80:80 - 443:443 - 8000:8000 environment: SEAFILE_SERVER_LETSENCRYPT: \"true\" SEAFILE_SERVER_HOSTNAME: pan1.cbbing.com SEAFILE_ADMIN_EMAIL: cbbing@163.com SEAFILE_ADMIN_PASSWORD: 12356789 3、支持在线编辑onlyoffice 的 documentserver 镜像是提供在线编辑服务的，镜像内已包含了 nginx 服务器，支持 80 端口和 443 端口，如果有合法证书，我们可以使用 https 连接。 123456789101112131415161718git clone https://github.com/neroxps/Docker-Only-Office-Chinese-font.git cd Docker-Only-Office-Chinese-font docker build -t onlyoffice/chinese .# httpdocker run -itd -p 780:80 --name office --restart=always onlyoffice/chinese# httpsdocker run -itd -p 7443:443 --name office --restart=always onlyoffice/chinese conf/seahub_settings.py添加 12345678910# Enable Only OfficeENABLE_ONLYOFFICE = TrueVERIFY_ONLYOFFICE_CERTIFICATE = FalseONLYOFFICE_APIJS_URL = 'http://pan.fofpower.com:780/web-apps/apps/api/documents/api.js'ONLYOFFICE_FILE_EXTENSION = ('doc', 'docx', 'ppt', 'pptx', 'xls', 'xlsx', 'odt', 'fodt', 'odp', 'fodp', 'ods', 'fods') https://bbs.seafile.com/t/topic/3430 二、使用1、添加/导入用户 下载模板，填入用户信息批量添加用户，适合从企业邮箱通讯录中批量导入用户 2、新建群组 群组共享的资料库对组内所有成员可见，方便部门内部文档沟通。 3，多终端使用seafile支持ios，android，同步盘，web端，实测体验不错。需要注意的是，如果seafile服务器端做了域名映射，需要修改系统配置，否则ios和android手机端文件上传和下载会失败。 将SERVICE_URL的8000去掉，直接用80端口1SERVICE_URL: http://pan.xxxx.com 参考 https://www.kancloud.cn/kancloud/seafile-manual/51487https://bbs.seafile.com/t/topic/3430","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.kekefund.com/tags/Docker/"},{"name":"seafile","slug":"seafile","permalink":"http://www.kekefund.com/tags/seafile/"}]},{"title":"Docker容器运行多条命令(supervisor)","date":"2018-03-30T01:45:32.000Z","path":"2018/03/30/supervisor/","text":"一, 简介Dockerfile 运行只支持一条命令，当在Docker里要运行多条命令，用supervisor来管理就比较合适了。Supervisor是一个 Python 开发的 client/server 系统，可以管理和监控类 UNIX 操作系统上面的进程。它可以同时启动，关闭多个进程，使用起来特别的方便。 组成部分supervisor 主要由两部分组成： supervisord(server 部分)：主要负责管理子进程，响应客户端命令以及日志的输出等； supervisorctl(client 部分)：命令行客户端，用户可以通过它与不同的 supervisord 进程联系，获取子进程的状态等。 二，存在的问题——日志不输出但是使用supervisor，Django运行的日志就不会在Docker里输出了，默认的输出如下：123456782018-03-28 06:48:20,292 CRIT Supervisor running as root (no user in config file)2018-03-28 06:48:20,308 INFO supervisord started with pid 12018-03-28 06:48:21,310 INFO spawned: 'celery_beat' with pid 72018-03-28 06:48:21,312 INFO spawned: 'celery_worker' with pid 82018-03-28 06:48:21,313 INFO spawned: 'django' with pid 92018-03-28 06:48:22,315 INFO success: celery_beat entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)2018-03-28 06:48:22,315 INFO success: celery_worker entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)2018-03-28 06:48:22,315 INFO success: django entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs) 上面的Docker中supervisor配置如下：1234567891011[supervisord]nodaemon=true [program:django]command=python manage.py runserver 0.0.0.0:8080 [program:celery_worker]command=python manage.py celery worker -c 4 -l info [program:celery_beat]command=python manage.py celery beat 这样的配置在容器中是同时运行Django，celery。 三，解决方案1，改进后的配置方案1234567891011121314151617181920212223242526[inet_http_server]port=9001username=abcpassword=123456 [supervisord]nodaemon=truelogfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log)pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid)childlogdir=/var/log/supervisor ; ('AUTO' child log dir, default $TEMP) [program:django]command=python manage.py runserver 0.0.0.0:8080#stdout_logfile = /var/log/supervisord/django_stdout.logloglevel=inforedirect_stderr=true [program:celery_worker]command=python manage.py celery worker -c 4 -l infologlevel=inforedirect_stderr=true [program:celery_beat]command=python manage.py celery beatloglevel=inforedirect_stderr=true 实测发现Django的日志输出会写到stderr.log文件中，所以在配置文件中将错误日志重定向到标准日志里;1redirect_stderr=true 容器中的生成的日志文件如下：12345root@a16bc77e96bc:/var/log/supervisor# lscelery_beat-stderr---supervisor-rSPQ7E.log django-stderr---supervisor-9LS_KA.logcelery_beat-stdout---supervisor-t5Q4UI.log django-stdout---supervisor-cTSBmq.logcelery_worker-stderr---supervisor-TRFzc7.log supervisord.logcelery_worker-stdout---supervisor-xNgeBU.log 2，查看日志运行容器时，将9001端口映射出去，通过ip:9001访问： 点击 Tail -f 查看各个进程的日志。 参考 http://debugo.com/docker-supervisord/ http://blog.csdn.net/zhousenshan/article/details/52988885 supervisor配置","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.kekefund.com/tags/Docker/"},{"name":"supervisor","slug":"supervisor","permalink":"http://www.kekefund.com/tags/supervisor/"}]},{"title":"scrapy+selenium爬取UC头条网站","date":"2017-12-06T05:49:26.000Z","path":"2017/12/06/scrapy-and-selenium/","text":"Scrapy是Python优秀的爬虫框架，selenium是非常好用的自动化WEB测试工具，两者结合可以非常容易对动态网页进行爬虫。本文的需求是抓取UC头条各个板块的内容。UC头条(https://news.uc.cn/ )网站没有提供搜索入口，只能每个板块的首页向下滚动鼠标加载更多。要对这样的网站进行检索，抓取其内容，采用一般的scrapy请求方式，每次只能获取最新的10条数据，分析其JS请求，发现参数过于复杂，没有规律。如果想获取更多数据，则需要采用模拟浏览器的方法，这时候selenium就派上用场了。 1，定义spider模拟从百度搜索进入，这个步骤可以省略，主要为了跳到parse函数1234567891011121314class UCTouTiaoSpider(VideoBaseSpider): name = \"uctoutiao_spider\" df_keys = ['人物', '百科', '乌镇'] def __init__(self, scrapy_task_id=None, *args, **kwargs): self.url_src = \"http://www.baidu.com\" def start_requests(self): requests = [] request = scrapy.Request(\"http://www.baidu.com\", callback=self.parse) requests.append(request) return requests 2，parse函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455def parse(self, response): self.log(response.url) urls = [\"https://news.uc.cn/\", \"https://news.uc.cn/c_redian/\", # \"https://news.uc.cn/c_shipin/\", # \"https://news.uc.cn/c_gaoxiao/\", \"https://news.uc.cn/c_shehui/\", \"https://news.uc.cn/c_yule/\", \"https://news.uc.cn/c_keji/\", \"https://news.uc.cn/c_tiyu/\", \"https://news.uc.cn/c_qiche/\", \"https://news.uc.cn/c_caijing/\", \"https://news.uc.cn/c_junshi/\", \"https://news.uc.cn/c_tansuo/\", \"https://news.uc.cn/c_lishi/\", \"https://news.uc.cn/c_youxi/\", \"https://news.uc.cn/c_lvyou/\", \"https://news.uc.cn/news/\", \"https://news.uc.cn/c_shishang/\", \"https://news.uc.cn/c_jiankang/\", \"https://news.uc.cn/c_guoji/\", \"https://news.uc.cn/c_yuer/\", \"https://news.uc.cn/c_meishi/\"] # 启动浏览器，这里用的火狐，如果在linux环境下可以用PhantomJS，稳定性稍微差点，有内存泄露的风险。 driver = webdriver.Firefox() for url in urls: try: print(url) driver.get(url) #模拟鼠标滚到底部(加载100条数据) for _ in range(10): driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\") driver.implicitly_wait(10) # 隐性等待,最长10秒 # print driver.page_source soup = bs(driver.page_source, 'lxml') articles = soup.find_all(href=re.compile(\"/a_\\w+?/\"), text=re.compile(\".+\")) for article in articles: for key in self.df_keys: item = VideoItem() #自定义的Item item['title'] = article.text item['href'] = article['href'] self.log(item) yield item except Exception as e: print e if driver == None: driver = webdriver.Firefox() if driver != None: driver.quit() 真正的实现部分比较简单，几句代码就搞定了。 附：selenium使用实例1，切换焦点至新窗口在页面上点击一个button, 然后打开了一个新的window, 将当前IWebDriver的focus切换到新window，使用IWebDriver.SwitchTo().Window(string windowName)。 例如， 我点击按钮以后弹出一个名字叫做”Content Display”的window， 要切换焦点到新窗口的方法是， 首先，获得新window的window name, 大家不要误以为page tile就是window name 哦， 如果你使用driver.SwitchTo().Window(“Content Display”)是找不到window name 叫做”Content Display”的窗口的， 其实Window Name 是一长串数字，类似“59790103-4e06-4433-97a9-b6e519a84fd0”。 要正确切换到”Content Display”的方法是： 获得当前所有的WindowHandles。 循环遍历到所有的window, 查找window.title与”Content Display”相符的window返回。 12345for handle in dr.window_handles: dr.switch_to.window(handle) print dr.title if len(dr.title) == '目标窗口标题': break 参考：Selenium - IWebDriver.SwitchTo() frame 和 Window 的用法 2 ，移至底部1driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\") 3，移动至指定元素某些按钮点击时必须可见，于是要把屏幕移动到按钮可见的区域12345678element = driver.find_element_by_xpath(\"//a[@class='p-next']\")element.location_once_scrolled_into_view #或者driver.set_window_size(800,800)element = driver.find_element_by_xpath(\"//a[@class='p-next']\")js = \"window.scrollTo(&#123;&#125;,&#123;&#125;);\".format(element.location['x'], element.location['y'] - 100)driver.execute_script(js) 参考：Python selenium —— 一定要会用selenium的等待，三种等待方式解读","tags":[{"name":"selenium","slug":"selenium","permalink":"http://www.kekefund.com/tags/selenium/"},{"name":"scrapy","slug":"scrapy","permalink":"http://www.kekefund.com/tags/scrapy/"}]},{"title":"nginx日志分析","date":"2017-10-27T10:57:09.000Z","path":"2017/10/27/nginx-log/","text":"1，日志定义在nginx.conf中定义的日志格式如下：12345678http &#123; ... log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status [$request_body] $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; ...&#125; 日志文件如下：12116.2.52.247 - - [26/Oct/2017:15:04:00 +0000] \"POST /api/v1/f1_static/ HTTP/1.1\" 200 [&#123;\\x22user_id\\x22:\\x229b999d46dd6149f49\\x22&#125;] 323 \"http://www.abc.com/ProductPerspective/detail/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\" \"-\"116.2.52.247 - - [26/Oct/2017:15:04:00 +0000] \"OPTIONS /api/v1/fund_info/ HTTP/1.1\" 200 [-] 31 \"http://www.abc.com/ProductPerspective/detail/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\" \"-\" 2，日志分割nginx没有命令直接将日志按天分割，我们写了一个shell脚本，每日0点定时执行。12345678910#nginx.log.sh#nginx日志切割脚本 #!/bin/bash#设置日志文件存放目录logs_path=\"/mydata/nginx/logs/\" #重命名日志文件mv $&#123;logs_path&#125;access-web.log $&#123;logs_path&#125;access-web-$(date -d \"yesterday\" +\"%Y%m%d\").logmv $&#123;logs_path&#125;access-api.log $&#123;logs_path&#125;access-api-$(date -d \"yesterday\" +\"%Y%m%d\").log cron：10 0 * * * /mydata/nginx/nginx.log.sh 3，日志搜集从nginx服务器将日志数据传输到日志服务器1234567891011[root@VM_231_116_centos ~]# scp -r /mydata/code/deploy/nginx/logs 10.115.82.34:/mydata/logsroot@10.105.83.34's password:access-power-20170929.log 100% 126KB 125.8KB/s 00:00access-web-20171016.log 100% 2616KB 2.6MB/s 00:00access-power-20170907.log 100% 1687KB 1.7MB/s 00:00access-api-20170911.log 100% 1209KB 1.2MB/s 00:00access-power-20170930.log 100% 1354KB 1.3MB/s 00:00access.log 100% 45MB 45.2MB/s 00:00access-api-20170907.log 100% 2960KB 2.9MB/s 00:00access-power-20170906.log 100% 669KB 669.1KB/s 00:01access-api-20170904.log 100% 9186KB 9.0MB/s 00:00 服务器之间文件（夹）复制1234567# 文件scp local_file remote_username@remote_ip:remote_folder 或者 scp local_file remote_username@remote_ip:remote_file # 目录scp -r local_folder remote_username@remote_ip:remote_folder 4，日志解析主要有几点： 逐行解析 正则匹配 日期的处理 批量写入数据库 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# -*- coding: utf-8 -*-import reimport timeimport osimport arrowimport pandas as pdimport jsonimport io_tosqlimport shutil from sqlalchemy import create_engineengine_user_info = create_engine( \"mysql+pymysql://&#123;&#125;:&#123;&#125;@&#123;&#125;:&#123;&#125;/&#123;&#125;\".format('usr', 'pwd', 'host','port', 'db'), connect_args=&#123;\"charset\": \"utf8\"&#125;) def parse(filename): month_abr = &#123;\"Jan\":\"01\", \"Feb\":\"02\", \"Mar\":\"03\", \"Apr\":\"04\", \"May\":\"05\", \"Jun\":\"06\", \"Jul\":\"07\", \"Aug\":\"08\", \"Sep\":\"09\", \"Oct\":\"10\", \"Nov\":\"11\", \"Dec\":\"12\"&#125; dfs = [] try: i = 0 file = open(filename) for line in file: pattern = \"(\\d+\\.\\d+\\.\\d+\\.\\d+).*?\\[(.*?)\\].*?(\\w+) (/.*?) .*?\\\" (\\d+) \\[(.*?)\\] (\\d+) \\\"(.*?)\\\" \\\"(.*?)\\\" \\\"(.*?)\\\"\" s = re.search(pattern, line) if s: remote_addr = s.group(1) local_time = s.group(2) request_method = s.group(3) request_url = s.group(4) status = s.group(5) request_body = s.group(6) body_bytes_sent = s.group(7) http_referer = s.group(8) http_user_agent = s.group(9) http_x_forwarded_for = s.group(10) # 30/Sep/2017:01:08:39 +0000 for mon in month_abr.keys(): if mon in local_time: local_time = local_time.replace(mon, month_abr[mon]) break lt = arrow.get(local_time, \"DD/MM/YYYY:HH:mm:ss\") lt = lt.shift(hours=8) local_time = str(lt.datetime) i = i+1 # print(\"line:&#123;&#125; &gt; &#123;&#125;\".format(i, local_time)) if request_body != '-': try: request_body = request_body.replace(r'\\x22', '\"').replace(\"null\", '\"\"') request_body_dict = json.loads(request_body) fund_id = request_body_dict.get('fund_id', None) user_id = request_body_dict.get('user_id', None) if user_id is None: user_id = request_body_dict.get('userId', None) except Exception as e: print(\"request_body:&#123;&#125;\".format(request_body)) print(e) fund_id = None user_id = None else: fund_id = None user_id = None if request_method not in (\"GET\", \"POST\"): # print(request_method) continue df = pd.DataFrame(&#123;\"remote_addr\": [remote_addr], \"request_method\": [request_method], \"local_time\": [local_time], \"request_url\": [request_url], \"status\": [status], \"request_body\": [request_body], \"body_bytes_sent\": [body_bytes_sent], \"http_referer\": [http_referer], \"http_user_agent\": [http_user_agent], \"http_x_forwarded_for\": [http_x_forwarded_for], \"fund_id\": [fund_id], \"user_id\": [user_id] &#125;) df['create_at'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time())) # print(df) dfs.append(df) #每100条写数据库 if len(dfs) &gt;= 100: df_all = pd.concat(dfs) df_all = df_all.drop_duplicates(subset=['remote_addr', 'request_url','local_time']) df_all.to_sql(\"log_table\", engine, if_exists=\"append\", index=False) print(\"写入长度为：\" + str(len(df_all))) dfs = [] df_all = pd.concat(dfs) df_all = df_all.drop_duplicates(subset=['remote_addr', 'request_url','local_time']) df_all.to_sql(\"log_table\", engine, if_exists=\"append\", index=False) except Exception as e: print(e) 5，日志展示日志结构化写入数据库后，到前端页面可以多维度展示，下面是展示页面示例： 统计每日活跃IP数 统计每日API请求次数 分类分析","tags":[{"name":"nginx","slug":"nginx","permalink":"http://www.kekefund.com/tags/nginx/"},{"name":"日志","slug":"日志","permalink":"http://www.kekefund.com/tags/日志/"}]},{"title":"nginx 缓存与优化","date":"2017-10-13T05:45:50.000Z","path":"2017/10/13/nginx-cache/","text":"在浏览器和应用服务器之间，存在多种“潜在”缓存，如： 客户端浏览器缓存 中间缓存 内容分发网络（CDN） 服务器上的负载均衡和反向代理 缓存，仅在反向代理和负载均衡的层面，就对性能提高有很大的帮助。 为什么要用缓存？ 网站的访问速度更快 减轻源服务器的负担 提高负载平衡器、反向代理和应用服务器前端web服务的性能 nginx代理模块缓存指令 指令 说明 proxy_cache 定义缓存的共享内存区域 proxy_cache_bypass 一个或者多个字符串变量，变量的值为非空或者非零将会导致响应从上游服务器获取而不是缓存 proxy_cache_key 用来区分缓存文件的key，作为缓存key的一个字符串，用于存储或者获取缓存值。默认值为$scheme$proxy_host$uri$is_args$args proxy_cache_lock 启用这个指令，当多个客户端请求一个缓存中不存在的文件（或称之为一个MISS），只有这些请求中的第一个被允许发送至服务器。其他请求在第一个请求得到满意结果之后在缓存中得到文件。 proxy_cache_lock_timeout 等待一个请求将出现在缓存或者proxy_cache_lock锁释放的时间长度 proxy_cache_min_uses 在一个响应被缓存为一个key之前需要请求的最小次数 proxy_cache_path 一个放置缓存响应和共享zone（keys_zone=name:size）的目录，用于存放key和响应的元数据。 proxy_cache_path:keys_zone 设置一个共享内存区，用于存储缓存键和元数据，有些类似计时器的用途。 proxy_cache_path:levels 冒号用于分隔在每个级别（1或2）的子目录名长度，最多三级深； proxy_cache_path:inactive 在一个不活动的响应被驱除出缓存之前待在缓存中的最大时间长度；例如设置如60m，则文件如果在60分钟之内没有被请求，则缓存管理会自动将其在内存中删除，不管文件是否过期。 proxy_cache_path:max_size 缓存的最大值，当大小超过这个值，缓存管理器溢出最近最少使用的缓存条目； proxy_cache_path:loader_files 缓存文件的最大数量，它们的元数据被每个缓存载入进程迭代载入； proxy_cache_path:loader_sleep 在每个迭代缓存载入进程之间停顿的毫秒数； proxy_cache_path:loader_threshold 缓存载入进程迭代花去时间的最大值 proxy_cache_use_stale 在访问上游服务器的时候发生错误，在这种情况下接受提供过期的缓存数据。参数updating告知NGINX在客户端请求的项目的更新正在原服务器中下载时发送旧内容，而不是向服务器转发重复的请求 proxy_cache_valid 缓存的有效期；指定对200、301或者302有效代码缓存的时间长度。特定参数any表示对任何响应都缓存一定时间长度。 proxy_cache_methods 缓存支持的方法，默认为GET，可以改为POST，OPTIONS等 缓存策略 首页缓存1分钟，因为它所包含的链接及文件列表经常更新 每篇文章都被缓存1天，因为一旦写完它们将不会改变，但我们又不希望缓存被填满，因此需要移除一些旧的缓存内容以便满足空间的需要。 尽量地缓存所有图像，因为从磁盘检索图像文件是一件比较“昂贵”的操作。 123456789101112131415161718192021http &#123; proxy_cache_path /var/spool/nginx/articles keys_zone=ARTICLES:16m levels=1:2 inactive=1d; proxy_cache_path /var/spool/nginx/images keys_zone=IMAGES:128m levels=1:2 inactive=30d; proxy_temp_path /var/spool/nginx; server &#123; location / &#123; # 首页 proxy_cache_valid 1m; &#125; location /articles &#123; proxy_cache_valid 1d; &#125; location /img &#123; proxy_cache_valid 10y; &#125; &#125;&#125; 示例下面的配置设计缓存所有的响应6个小时，缓存大小为1GB。任何条目保存刷新，就是说，在6个小时内被调用为超时，有效期为1天。在此时间后，上游服务器将再次调用提供响应。如果上游服务器由于错误、超时、无效头或者是由于缓存条目被升级，那么过期的条目就会被使用。共享内存区、CACHE被定义为10MB，并且在location中使用，在这里设置缓存key，并且也可以从这里查询。123456789101112131415161718192021http &#123; proxy_temp_path /var/spool/nginx; proxy_cache_path /var/spool/nginx keys_zone=CACHE:10m levels=1:2 inactive=6h max_size=1g; server &#123; location / &#123; # using include to bring in a file with commonly-used settings include proxy.conf; proxy_cache CACHE; proxy_cache_valid any 1d; proxy_cache_use_stale error timeout invalid_header updating http_500 http_502 http_503 http_504; proxy_pass http://upstream; &#125; &#125;&#125; 实践1，检测缓存状态通过添加如下代码实现1234567891011121314server &#123; listen 80; server_name xx.fofpower.com; location / &#123; proxy_cache api_cache; proxy_cache_valid 200 206 304 301 302 1d; proxy_ignore_headers Cache-Control; add_header X-Cache-Status $upstream_cache_status; #添加此行 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://myapi; &#125;&#125; 浏览器上看到的状态可能有： $upstream_cache_status可能值：1，MISS——响应在缓存中找不到，所以需要在服务器中取得。2，HIT——响应包含来自缓存的最新有效的内容3，EXPIRED——缓存中的某一项过期了，来自原始服务器的响应包含最新的内容4，STALE——内容陈旧是因为原始服务器不能正确响应。需要配置proxy_cache_use_stale5，UPDATING——内容过期了，因为相对于之前的请求，响应的入口（entry）已经更新，并且proxy_cache_use_stale的updating已被设置6，REVALIDATED——proxy_cache_revalidate命令被启用，NGINX检测得知当前的缓存内容依然有效(If-Modified-Since或者If-None-Match) 2，缓存POST请求NGINX默认支持GET请求的缓存，要增加POST，需要设置proxy_cache_methods。对于POST请求，url相同，body内容不同，如果使用默认的proxy_cache_key，会造成不同的post请求，用了一个缓存键，返回给前端的数据会错乱。解决方案是将post的请求参数也作为key的一部分。1234567891011121314151617server &#123; listen 80; server_name www.fofeasy.com; add_header X-Cache-Status $upstream_cache_status; location / &#123; proxy_cache web_cache; proxy_cache_valid 200 206 304 301 302 10d; proxy_cache_key $uri$request_body; #增加此行 proxy_cache_methods GET POST; #增加此行 proxy_ignore_headers Cache-Control; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://myweb; &#125;&#125; 参考 NGINX缓存使用官方指南 使用 Nginx 反代并缓存动态内容","tags":[{"name":"nginx","slug":"nginx","permalink":"http://www.kekefund.com/tags/nginx/"},{"name":"缓存","slug":"缓存","permalink":"http://www.kekefund.com/tags/缓存/"}]},{"title":"docker命令手册","date":"2017-08-09T08:06:45.000Z","path":"2017/08/09/docker-command/","text":"Docker 命令操作手册，方便查询使用。 运行docker run –name 指定容器名-p 指定端口映射-v 挂载数据卷或者本地目录映射 :ro 挂载为只读-d 后台持续运行-i 交互式操作-t 终端-rm 容器退出后随之将其删除(与-d 冲突) eg: 1234docker run --name ghost1 -p 80:2368 -v /c/Dev/server/blogtest2:/var/lib/ghost ghostdocker run -it --rm ubuntu:14.04 bashdocker run ubuntu:14.04 /bin/echo 'Hello world'docker run --name webserver -d -p 80:80 nginx 管理容器1234567891011docker ps 列出正在运行的容器 docker kill $(docker ps -q) 停止所有正在运行的容器 docker ps -a 查看所有容器，包括运行和停止的docker start 启动一个已有容器 docker stop 终止一个运行中的容器 docker restart 重启某个容器 docker rm xxxx 删除容器 -f 删除运行中的 docker rm $(docker ps -a -q) 删除所有终止的容器 docker logs [container id or names] 获取输出log -f 实时打印日志docker diff 容器名 查看我们定制以及修改 docker volume ls 列出所有本机的数据卷 管理镜像123456789docker pull [option] [url] 获取镜像, 例如: docker pull ubuntu:14.04 docker images 列出本地镜像 docker build -t nginx:v3 . 在当前目录构建镜像,-t 是指定镜像名称 tagdocker rmi xxxxxx 删除本地镜像docker commit 选项 容器名/id 仓库名 tag ：可以把修改定制过的容器保存为镜像 docker images -f dangling=true 列出所有虚悬镜像(dangling image)docker rmi $(docker images -q -f dangling=true) 删除所有虚悬镜像docker histroy 镜像名:标签 查看镜像修改的历史纪录 查询单个容器详细信息可以看到容器的完整ID、运行状态、网络设置、镜像等信息。123456789101112131415161718192021222324252627282930[root@VM_25_5_centos ~]# docker inspect splash[ &#123; \"Id\": \"b5a387e5f9064113e48c06384be045675e12047c7ef5564f76ae8bf0c7f95304\", \"Created\": \"2017-04-05T04:49:40.025249222Z\", \"Path\": \"python3\", .... \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"b3063867b30c820bb92ee198edad8d5cb8974135d0490e956d3646364ccca711\", \"EndpointID\": \"979a8d0bbfde532c45dfbf97bab3c2d874100b4ca448a460b81904709260eb3b\", \"Gateway\": \"172.17.0.1\", \"IPAddress\": \"172.17.0.3\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:11:00:03\" &#125; &#125; &#125; &#125;] 查询日志12[root@VM_25_5_centos ~]# docker logs splash2017-04-10 12:32:49.050352 [-] \"101.226.66.173\" - - [10/Apr/2017:12:32:48 +0000] \"GET /4e5e5d7364f443e28fbf0d3ae744a59a HTTP/1.1\" 404 153 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36\" 实时打印日志加上-f参数12[root@VM_25_5_centos ~]# docker logs -f splash2017-04-10 12:32:49.050352 [-] \"101.226.66.173\" - - [10/Apr/2017:12:32:48 +0000] \"GET /4e5e5d7364f443e28fbf0d3ae744a59a HTTP/1.1\" 404 153 \"-\" \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36\" 查看容器所占用的系统资源如CPU使用率、内存、网络和磁盘开销。1234[root@VM_25_5_centos ~]# docker stats splashCONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDSsplash 0.04% 230.5 MiB / 7.64 GiB 2.95% 50.7 MB / 33.77 MB 547.9 MB / 131.1 kB 7 查看容器使用了哪些进程12345[root@VM_25_5_centos ~]# docker exec api1 ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.3 125480 25136 ? Ss+ Mar31 0:00 python ./manage.py runserver 0.0.0.0:8000root 6 0.9 0.8 747960 64664 ? Sl+ Mar31 139:37 /usr/local/bin/python ./manage.py runserver 0.0.0.0:8000root 886 0.0 0.0 19180 1300 ? Rs 16:56 0:00 ps aux 转移Docker的数据目录到大的磁盘分区上12345service docker stopmkdir /data/dockerData/mv /var/lib/docker /data/dockerData/ln -s /data/dockerData/docker /var/lib/dockerservice docker start centos7 安装docker12rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpmyum -y install docker-io 可通过以下命令启动 Docker 服务：12service docker startchkconfig docker on # 设置开机启动 可使用以下命令，查看 Docker 是否安装成功：123456789101112131415161718[root@localhost ~]# docker versionClient: Version: 1.12.6 API version: 1.24 Package version: docker-1.12.6-32.git88a4867.el7.centos.x86_64 Go version: go1.7.4 Git commit: 88a4867/1.12.6 Built: Mon Jul 3 16:02:02 2017 OS/Arch: linux/amd64Server: Version: 1.12.6 API version: 1.24 Package version: docker-1.12.6-32.git88a4867.el7.centos.x86_64 Go version: go1.7.4 Git commit: 88a4867/1.12.6 Built: Mon Jul 3 16:02:02 2017 OS/Arch: linux/amd64 centos7 卸载docker123456789[root@localhost ~]# yum list installed | grep dockerdocker.x86_64 2:1.12.6-28.git1398f24.el7.centosdocker.x86_64 2:1.12.6-32.git88a4867.el7.centosdocker-client.x86_64 2:1.12.6-28.git1398f24.el7.centosdocker-client.x86_64 2:1.12.6-32.git88a4867.el7.centosdocker-common.x86_64 2:1.12.6-28.git1398f24.el7.centosdocker-common.x86_64 2:1.12.6-32.git88a4867.el7.centos [root@localhost ~]# yum -y remove docker.x86_64[root@localhost ~]# yum -y remove docker-common.x86_64 docker网络命令123456docker network createdocker network connectdocker network lsdocker network rmdocker network disconnectdocker network inspect 参考：Docker network命令详解","tags":[{"name":"docker","slug":"docker","permalink":"http://www.kekefund.com/tags/docker/"}]},{"title":"Docker镜像自动构建","date":"2017-08-08T10:09:51.000Z","path":"2017/08/08/docker-image-auo-build/","text":"基于Git的代码版本控制，能保证每个开发人员的开发成果得到有效的保护，不会发生因为其他人的“误操作”操作将自己辛辛苦苦写的代码删除，而且能够跟踪每一次的改动，能快速回滚到之前任一版本。基于Docker镜像的版本控制，能保证应用服务的正常迭代更新，某一版本出问题了可以快速切换至其他镜像版本。 1，镜像仓库镜像的版本控制，主要是通过给image加tag来区分。首先，我们需要一个镜像仓库，最好是私有仓库。目前国内阿里云和腾讯云都提供免费的私有镜像服务，当然，我们还可以自己搭建私有镜像仓库。 1.1，腾讯云镜像仓库：（https://console.qcloud.com/ccs） 1.2，阿里云镜像仓库：（https://cr.console.aliyun.com） 1.3，自建私有仓库参考本人博客: docker私有仓库搭建 2, 上传镜像以腾讯云镜像仓库为例。先开通镜像仓库，然后在命令行登录：12$ docker login --username=yourname -p=yourpassword ccr.ccs.tencentyun.comLogin Succeeded username：腾讯云账号，p为密码 本地镜像上传12$ sudo docker tag [ImageId] ccr.ccs.tencentyun.com/[namespace]/[ImageName]:[镜像版本号]$ sudo docker push ccr.ccs.tencentyun.com/[namespace]/[ImageName]:[镜像版本号] 例如：12$ docker tag fof_api:v20170808 ccr.ccs.tencentyun.com/sk/api:v20170808$ docker push ccr.ccs.tencentyun.com/sk/api:v20170808 3, 下载镜像12$ docker login --username=yourname -p=yourpassword ccr.ccs.tencentyun.com$ docker pull ccr.ccs.tencentyun.com/sk/api:v20170808 4，镜像制作4.1 API镜像自动构建API是基于python的Django框架编写的。在项目的根目录定义好Dockerfile和requirements.txt，具体参考docker版Django。然后定义Shell脚本(make.sh)：1234567891011# make.sh#!/bin/shgit pullecho \"开始制作镜像...\"image_tag=`date +%Y%m%d_%H%M`echo \"当前时间：$image_tag\"docker build -t ccr.ccs.tencentyun.com/sk/api:$&#123;image_tag&#125; .echo \"制作镜像成功!\"docker login --username=yourname -p=yourpassword ccr.ccs.tencentyun.comdocker push ccr.ccs.tencentyun.com/sk/api:$&#123;image_tag&#125; 制作流程： 更新最新代码 制作镜像 镜像添加tag，这里tag是当前日期 上传镜像只腾讯云docker仓库 4.2 Web应用镜像自动构建Web应用是用maven搭建的SpringMVC+MyBatis项目。打包生成war包可以通过maven命令行在服务器上执行。这里我们采用的服务器是centos7.3，java版本是1.8，maven版本是3.3.9。1234567$ mvn -vApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: /usr/local/apache-maven-3.3.9Java version: 1.8.0_111, vendor: Oracle CorporationJava home: /usr/local/jdk1.8.0_111/jreDefault locale: en_US, platform encoding: ANSI_X3.4-1968OS name: \"linux\", version: \"3.10.0-514.21.2.el7.x86_64\", arch: \"amd64\", family: \"unix\" 在服务器上安装好java，maven后，配置maven repository路径:apache-maven-3.3.9/conf/settings.xml123456&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; &lt;localRepository&gt;/your/path/apache-maven-3.3.9/repository&lt;/localRepository&gt; ...&lt;/settings&gt; 4.2.1 war包生成脚本1234567891011121314151617181920212223242526272829303132#mvn_on_centos.sh#!/bin/shAPP_PATH=\"/mydata/code/build/fofweb\"cd $&#123;APP_PATH&#125;/dzmsoft-ccsmvn cleanmvn install -DskipTestscd $&#123;APP_PATH&#125;/dzmsoft-ucsmvn cleanmvn install -DskipTestscd $&#123;APP_PATH&#125;/fofeasy-basemvn cleanmvn install -DskipTestscd $&#123;APP_PATH&#125;/fofeasy-backstagemvn cleanmvn install -DskipTestscd $&#123;APP_PATH&#125;/fofeay-reportmvn cleanmvn install -DskipTestscd $&#123;APP_PATH&#125;/fofeasymvn cleanmvn install -DskipTestscd /mydata/code/deploy/web_warmkdir ./ROOTcd ROOTcp /mydata/code/build/fofweb/fofeasy/target/fofeasy.war .jar xvf fofeasy.warrm fofeasy.warcd ..rm ROOT.tar.gztar czf ROOT.tar.gz ./ROOTrm -rf ROOT/ 这里，我们先生成fofeasy.war包，然后将war包移到脚本所在目录解压，打包成tar.gz。 4.2.2 镜像制作及上传12345678910111213141516171819202122#make_power_image_on_centos.sh#!/bin/shMVN_PATH=\"/mydata/code/deploy/web_war\"echo \"MVN_PATH:$&#123;MVN_PATH&#125;\"echo \"开始生成WAR包...\"$&#123;MVN_PATH&#125;/mvn_on_centos.shecho \"WAR包生成成功！\"echo \"复制War包到source目录\"cp $&#123;MVN_PATH&#125;/ROOT.tar.gz source/echo \"复制完成\"echo \"开始制作镜像...\"image_tag=`date +%Y%m%d` #_%H%Mecho \"当前时间：$image_tag\"docker build -t ccr.ccs.tencentyun.com/sk/power:v$&#123;image_tag&#125; .echo \"制作镜像成功!\"docker login --username=yourname -p=yourpassword ccr.ccs.tencentyun.comdocker push ccr.ccs.tencentyun.com/sk/power:v$&#123;image_tag&#125;echo \"上传镜像成功!\" 4.2.3 镜像自动制作-定时任务编译一个调用shell脚本build.sh来管理以上两个构建镜像的sh文件，123456789101112131415161718#!/bin/shPOWER_PATH=\"/mydata/code/deploy/fofpower/web\"API_PATH=\"/mydata/code/build/fof_api\"GIT_WEB_PATH=\"/mydata/code/build/fofweb\" echo \"fofweb:git pull...\"cd $&#123;GIT_WEB_PATH&#125;git pull echo \"Build fofpower...\"cd $&#123;POWER_PATH&#125;$&#123;POWER_PATH&#125;/make_power_image_on_centos.sh echo \"Build fof_api...\"cd $&#123;API_PATH&#125;$&#123;API_PATH&#125;/make.sh echo `date +%Y-%m-%d` `date +%X` \"制作完成\" &gt;&gt; /mydata/code/build/console.txt 在cron中添加：1$ crontab -e 在行尾添加:122 11 * * * cd /mydata/code/build/ &amp;&amp; ./build.sh 定时任务每天11：22执行。 遇到在Shell sh下执行成功，在cron中确执行不成功的问题？这是因为cron使用的环境变量不是系统的环境变量，而是自己的环境变量。解决方法：在sh脚本添加如下代码：12#!bin/bashsource /etc/profile 5，镜像使用本地push镜像（需先登录）1$ docker push ccr.ccs.tencentyun.com/sk/api:[tag] 上传成果的镜像在镜像仓库能看到： 还可以在后台自动构建，关联github工程，指定Dockerfile，commit触发自动构建。 PS: commit触发自动构建，在阿里云和腾讯云上测试没成果，需手动构建。 参考 使用阿里云免费构建docker私有仓库Centos 下maven的安装与配置～","tags":[{"name":"镜像","slug":"镜像","permalink":"http://www.kekefund.com/tags/镜像/"},{"name":"腾讯云","slug":"腾讯云","permalink":"http://www.kekefund.com/tags/腾讯云/"},{"name":"阿里云","slug":"阿里云","permalink":"http://www.kekefund.com/tags/阿里云/"},{"name":"build","slug":"build","permalink":"http://www.kekefund.com/tags/build/"}]},{"title":"SpringMVC工程实现单人登录(踢人)","date":"2017-08-03T07:18:50.000Z","path":"2017/08/03/springmvc-login-single/","text":"网站不允许多个用户使用一个账号登录，即最后登录的用户会把之前登录在线的用户踢下线。在SpringMVC中配置如下： 1，spring-shiro.xml配置123456789101112131415161718192021222324252627282930313233343536373839404142 ... &lt;!-- 并发登录控制人数 --&gt;&lt;bean id=\"kickoutSessionControlFilter\" class=\"com.dzmsoft.ucs.base.shiro.KickoutSessionControlFilter\"&gt; &lt;property name=\"cacheManager\" ref=\"cacheManager\"/&gt; &lt;property name=\"sessionManager\" ref=\"sessionManager\"/&gt; &lt;property name=\"kickoutAfter\" value=\"false\"/&gt; &lt;property name=\"maxSession\" value=\"1\"/&gt; &lt;property name=\"kickoutUrl\" value=\"/login?kickout=true\"/&gt; &lt;/bean&gt; &lt;bean id=\"shiroFilter\" class=\"org.apache.shiro.spring.web.ShiroFilterFactoryBean\"&gt; &lt;property name=\"securityManager\" ref=\"securityManager\" /&gt; &lt;property name=\"loginUrl\" value=\"/login\" /&gt; &lt;property name=\"successUrl\" value=\"/index\" /&gt; &lt;property name=\"filters\"&gt; &lt;util:map&gt; &lt;entry key=\"authc\" value-ref=\"authcFilter\"/&gt; &lt;entry key=\"kickout\" value-ref=\"kickoutSessionControlFilter\"/&gt; #新增 &lt;/util:map&gt; &lt;/property&gt; &lt;!-- 配置哪些页面需要受保护. 以及访问这些页面需要的权限. 1). anon 可以被匿名访问 2). authc 必须认证(即登录)后才可能访问的页面. 3). logout 登出. 4). roles 角色过滤器--&gt; &lt;property name=\"filterChainDefinitions\"&gt; &lt;value&gt; /resources/** = anon /register/** = anon /ucsUser/** = anon /login = anon /rest/** =anon /logout = logout /userManager = roles /** = kickout,authc #新增 &lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; ... 2，添加KickoutSessionControlFilter.java文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118package com.dzmsoft.ucs.base.shiro;import org.apache.shiro.cache.Cache;import org.apache.shiro.cache.CacheManager;import org.apache.shiro.session.Session;import org.apache.shiro.session.mgt.DefaultSessionKey;import org.apache.shiro.session.mgt.SessionManager;import org.apache.shiro.subject.Subject;import org.apache.shiro.web.filter.AccessControlFilter;import org.apache.shiro.web.util.WebUtils;import com.dzmsoft.framework.base.web.mvc.pojo.ShiroUser;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;import java.io.Serializable;import java.util.Deque;import java.util.LinkedList;/** * &lt;p&gt;User: Zhang Kaitao * &lt;p&gt;Date: 14-2-18 * &lt;p&gt;Version: 1.0 */public class KickoutSessionControlFilter extends AccessControlFilter &#123; private String kickoutUrl; //踢出后到的地址 private boolean kickoutAfter = false; //踢出之前登录的/之后登录的用户 默认踢出之前登录的用户 private int maxSession = 1; //同一个帐号最大会话数 默认1 private SessionManager sessionManager; private Cache&lt;String, Deque&lt;Serializable&gt;&gt; cache; public void setKickoutUrl(String kickoutUrl) &#123; this.kickoutUrl = kickoutUrl; &#125; public void setKickoutAfter(boolean kickoutAfter) &#123; this.kickoutAfter = kickoutAfter; &#125; public void setMaxSession(int maxSession) &#123; this.maxSession = maxSession; &#125; public void setSessionManager(SessionManager sessionManager) &#123; this.sessionManager = sessionManager; &#125; public void setCacheManager(CacheManager cacheManager) &#123; this.cache = cacheManager.getCache(\"shiro-kickout-session\"); &#125; @Override protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) throws Exception &#123; return false; &#125; @Override protected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception &#123; Subject subject = getSubject(request, response); if(!subject.isAuthenticated() &amp;&amp; !subject.isRemembered()) &#123; //如果没有登录，直接进行之后的流程 return true; &#125; Session session = subject.getSession(); ShiroUser user = (ShiroUser)subject.getPrincipal(); String username = user.getName();// String username = (String) subject.getPrincipal(); Serializable sessionId = session.getId(); //TODO 同步控制 Deque&lt;Serializable&gt; deque = cache.get(username); if(deque == null) &#123; deque = new LinkedList&lt;Serializable&gt;(); cache.put(username, deque); &#125; //如果队列里没有此sessionId，且用户没有被踢出；放入队列 if(!deque.contains(sessionId) &amp;&amp; session.getAttribute(\"kickout\") == null) &#123; deque.push(sessionId); &#125; //如果队列里的sessionId数超出最大会话数，开始踢人 while(deque.size() &gt; maxSession) &#123; Serializable kickoutSessionId = null; if(kickoutAfter) &#123; //如果踢出后者 kickoutSessionId = deque.removeFirst(); &#125; else &#123; //否则踢出前者 kickoutSessionId = deque.removeLast(); &#125; try &#123; Session kickoutSession = sessionManager.getSession(new DefaultSessionKey(kickoutSessionId)); if(kickoutSession != null) &#123; //设置会话的kickout属性表示踢出了 kickoutSession.setAttribute(\"kickout\", true); &#125; &#125; catch (Exception e) &#123;//ignore exception &#125; &#125; //如果被踢出了，直接退出，重定向到踢出后的地址 if (session.getAttribute(\"kickout\") != null) &#123; //会话被踢出了 try &#123; subject.logout(); &#125; catch (Exception e) &#123; //ignore &#125; saveRequest(request); WebUtils.issueRedirect(request, response, kickoutUrl); return false; &#125; return true; &#125;&#125; 3，login.jsp添加提示提示是基于url中的参数kickout来判断的123...&lt;c:if test=\"$&#123;not empty param.kickout&#125;\"&gt;&lt;font color='red'&gt;您被踢出登录了。&lt;/font&gt;&lt;/c:if&gt; ... 4，ehcache.xml配置添加12345678&lt;cache name=\"shiro-kickout-session\" maxEntriesLocalHeap=\"2000\" eternal=\"false\" timeToIdleSeconds=\"3600\" timeToLiveSeconds=\"0\" overflowToDisk=\"false\" statistics=\"true\"&gt; &lt;/cache&gt; 5，实测通过","tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.kekefund.com/tags/SpringMVC/"},{"name":"Login","slug":"Login","permalink":"http://www.kekefund.com/tags/Login/"}]},{"title":"2017腾讯云+未来峰会","date":"2017-06-23T07:19:59.000Z","path":"2017/06/23/qcloud-summit/","text":"云时代的新趋势 马化腾 电子发票，扫描填写，云端上传，财务报销，钱款到账 人工智能，为何是现在？ 张首晟 斯坦福大学物理教授，美国国家科学院院士 摩尔定律所描述的计算能力的指数增长 互联网和物联网的爆发性增长所产生的海量数据 智能算法的快速发展 仅基于摄像头的自动驾驶技术不需要激光雷达，也不需要高清三维地图 =&gt;Autox技术的可扩展性 大数据金融、教育、健康……，我们需要更多的结构化数据。 数据市场分享个人数据而保护个人隐私 =&gt; 形成数据市场Homomorphic encryption（同态加密）技术 AI即服务 金融科技 李亮，平安科技首席战略官 1.0：金融IT，科技支持金融 2.0：互联网金融，互联网+金融 3.0：金融科技，科技引领金融 AI、云、数字化O2O和安全是推动金融科技发展的四大关键支柱 金融·科技 or 科技·金融 金融应用场景 投资类 银行类 保险类 无缝的数据访问是推动AI商业化应用的关键 田民，顺丰集团CTO，顺丰科技CEO AI的4大核心要素 数据 场景 技术 算法 AI转型的5大关键 成功用例 数据生态 技术工具 无缝介入工作流程 开放的文化和组织 AI技术采用率成熟度的6大标准 数字化成熟度 商业规模化应用 核心业务应用 技术多元化 投资回报 高管态度 互联网专场摩拜高速增长背后的技术支撑（六脉神剑） 摩拜首席架构师 范同祥 物联网技术 云计算平台 微服务架构 DevOps流水线 腾讯专家团 摩拜技术文化 领先同行的物联网技术 智能锁 OTA在线升级 软硬件快速迭代 近100项专利或专利申请 GPS定位 GSM通信 蓝牙开锁 NB-IoT 拥抱主流技术栈 Spring Cloud Netflix OSS Consul ZipKin 服务分层，保障核心系统稳定可用 数据分层 核心业务系统 运营支撑系统 实用、高效的工具链，步入DevOps快车道 代码模板化 服务容器化 环境标准化 自动化测试 自动化部署 多分支并行迭代 借力腾讯云 技术文化 金融专场云上“智”造新金融 朱立强，腾讯FiT副总裁,腾讯云副总裁 AI与金融结合的方向 “云+AI”金融应用 智能投顾 智能营销 智能审批 反欺诈 链接2017腾讯云+未来峰会","tags":[{"name":"AI","slug":"AI","permalink":"http://www.kekefund.com/tags/AI/"},{"name":"云计算","slug":"云计算","permalink":"http://www.kekefund.com/tags/云计算/"},{"name":"金融","slug":"金融","permalink":"http://www.kekefund.com/tags/金融/"}]},{"title":"采用Docker集成jquery-file-upload组件到WEB应用","date":"2017-06-15T09:13:19.000Z","path":"2017/06/15/jquery-file-upload-docker/","text":"1，Docker镜像jQuery-File-Upload 组件是一个非常好用的文件上传组件，有很多友好的特性： 支持文件多选 拖拽上传 上传进度条 取消上传 图片、音视频预览 纯JS和HTML5代码，不需额外安装插件 服务器端提供了三种部署方式: gae-go、gae-python和php，前两种基于gae，在国内基本被墙了，肯定用不了。php的部署用官方提供的部署方式运行不起来，从dockerhub上找到了一个可用的docker镜像:yaasita/docker-jquery-file-upload，日文？&amp;!OMG。 运行起来：1$ docker run -d -p 22 -p 8033:80 yaasita/docker-jquery-file-upload /usr/bin/supervisord 跟官方给出的Demo是一样的，不过我们需要做下汉化。 2，集成效果如下： 每个Tab标签对应的是一个地址。 3，WEB前端html调用modal，modal部分如下，通过3个iframe，请求到服务器端的文件上传接口。1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!-- 导入modal --&gt;&lt;div class=\"modal fade\" id=\"importModal\" tabindex=\"-1\" role=\"dialog\" aria-labelledby=\"myModalLabel\" aria-hidden=\"true\"&gt;&lt;div style=\"margin-top:10%;\"&gt;&lt;div class=\"modal-dialog\" style=\"min-width:600px;width:60%;\"&gt;&lt;div class=\"modal-content\"&gt;&lt;div class=\"modal-header\"&gt;&lt;button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\"&gt;&amp;times;&lt;/button&gt;&lt;h2 class=\"modal-title\" id=\"myModalLabel\" style=\"font-size:18px;\"&gt;数据导入&lt;/h2&gt;&lt;/div&gt;&lt;div class=\"modal-body\" id=\"upLoad\"&gt;&lt;ul id=\"labUl\" class=\"labelUl\" style=\"float:none;\"&gt;&lt;li id=\"basicInfotab\" class=\"Active2\"&gt;&lt;span&gt;基本信息&lt;/span&gt;&lt;/li&gt;&lt;li id=\"netDatatab\"&gt;&lt;span&gt;净值数据&lt;/span&gt;&lt;/li&gt;&lt;li id=\"positionDatatab\"&gt;&lt;span&gt;持仓数据&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div id=\"basicInfoDiv\" class=\"uploadDiv\"&gt;&lt;iframe scrolling=\"yes\" src=\"http://localhost:8010/\" class=\"fileUpload\"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;div id=\"netDataDiv\" class=\"uploadDiv\" style=\"display:none;\"&gt;&lt;iframe scrolling=\"yes\" src=\"http://localhost:8011/\" class=\"fileUpload\"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;div id=\"positionDataDiv\" class=\"uploadDiv\" style=\"display:none;\"&gt;&lt;iframe scrolling=\"yes\" src=\"http://localhost:8012/\" class=\"fileUpload\"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=\"modal-footer\"&gt;&lt;button type=\"button\" class=\"easy1Btn\" id=\"buttonImportAndCalc\"&gt;导入&amp;计算&lt;/button&gt;&lt;!-- &lt;button type=\"button\" id=\"fileUpload\" class=\"easy2Btn\"&gt;上传&lt;/button&gt; --&gt;&lt;button type=\"button\" class=\"easy1Btn\" data-dismiss=\"modal\"&gt;关闭&lt;/button&gt;&lt;/div&gt;&lt;div id=\"layer\"&gt;&lt;/div&gt;&lt;div id=\"onLoad\"&gt;&lt;/div&gt;&lt;/div&gt;&lt;!-- /.modal-content --&gt;&lt;/div&gt;&lt;!-- /.modal --&gt; 4，服务器端配置4.1，Dockerfile文件位置: ./FileUpload/Dockerfile12345678910111213# Version 0.1# 基础镜像FROM yaasita/docker-jquery-file-upload# 维护者信息MAINTAINER cbbing@163.com# 镜像命令COPY index.html /var/www/upload/index.htmlCMD [\"/usr/bin/supervisord\"] 其中，Dockerfile中的index.html文件，是为了汉化docker镜像中的index文件。 4.2，docker-compose.ymldocker-compose中配置了3个容器，对外提供文件上传接口，分别对应服务器的info, nav, pos目录。1234567891011121314151617181920212223242526272829version: '2'services: fileupload1: build: ./FileUpload ports: - 8010:80 - 22 volumes: - /usr/local/upload/info:/var/www/upload/server/php/files restart: \"always\" fileupload2: build: ./FileUpload ports: - 8011:80 - 22 volumes: - /usr/local/upload/nav:/var/www/upload/server/php/files restart: \"always\" fileupload3: build: ./FileUpload ports: - 8012:80 - 22 volumes: - /usr/local/upload/pos:/var/www/upload/server/php/files restart: \"always\" 目录结构：1234- docker-compose.yml- FileUpload/---- Dockerfile---- index.html 4.3 运行1$ docker-compose up --build 参考 https://github.com/blueimp/jQuery-File-Upload","tags":[{"name":"Dockerfile","slug":"Dockerfile","permalink":"http://www.kekefund.com/tags/Dockerfile/"},{"name":"Docker","slug":"Docker","permalink":"http://www.kekefund.com/tags/Docker/"},{"name":"jquery-file-upload","slug":"jquery-file-upload","permalink":"http://www.kekefund.com/tags/jquery-file-upload/"},{"name":"docker-compose","slug":"docker-compose","permalink":"http://www.kekefund.com/tags/docker-compose/"}]},{"title":"docker私有仓库搭建","date":"2017-06-07T08:08:33.000Z","path":"2017/06/07/doker-registry/","text":"Docker提供了开放的中央仓库dockerhub，同时也允许我们使用registry搭建本地私有仓库。搭建私有仓库有如下的优点： 节省网络带宽，提升Docker部署速度，不用每个镜像从DockerHub上去下载，只需从私有仓库下载就可； 私有镜像，包含公司敏感信息，不方便公开对外，只在公司内部使用。 1，搭建私有仓库1.1 下载镜像registry1$ docker pull registry 1.2 启动容器1$ docker run -d -p 5000:5000 --restart=always --name=registry-srv -v /mydata/dockerRegistry:/var/lib/registry registry 解释一下:12345-d：后台运行-p：将容器的5000端口映射到宿主机的5000端口--restart：docker服务重启后总是重启此容器--name：容器的名称-v：将容器内的/var/lib/registry映射到宿主机的/mydata/dockerRegistry目录 2，搭建WEB服务私有仓库搭建好了，怎么查看仓库里的镜像，搭建一个web服务，查看修改image比较方便。 2.1 下载镜像1$ docker pull hyper/docker-registry-web 2.2 启动容器1docker run -it -p 8080:8080 --restart=always --name registry-web --link registry-srv -e REGISTRY_URL=http://registry-srv:5000/v2 -e REGISTRY_NAME=localhost:5000 hyper/docker-registry-web 解释一下：123-it: 以交互模式运行--link：链接其它容器(registry-srv)，在此容器中，使用registry-srv等同于registry-srv容器的局域网地址-e：设置环境变量 3，上传本地镜像到私有仓库例如，将本地的hcharts上传到仓库12$ docker images | grep hchartscbbing/hcharts latest 64164ca3dafe 3 weeks ago 550 MB 3.1 修改镜像tag1$ docker tag cbbing/hcharts 192.168.1.87:5000/cbbing/hcharts 3.2 上传tag镜像到仓库123456docker push 192.168.1.87:5000/cbbing/hchartsThe push refers to a repository [192.168.1.87:5000/hcharts]96b641920005: Pushing [===&gt; ] 8.852 MB/125.1 MBee2881ef910e: Pushing [==&gt; ] 11.69 MB/232.7 MB36018b5e9787: Pushing [===&gt; ] 11.99 MB/192.5 MB 3.3 会出现的push失败123$ docker push 192.168.1.87:5000/hchartsThe push refers to a repository [192.168.1.87:5000/hcharts]Get https://192.168.1.87:5000/v1/_ping: http: server gave HTTP response to HTTPS client 这是因为Docker在1.3.x之后默认docker registry使用的是https，为了解决这个问题，修改本地主机的docker启动配置文件，添加1--insecure-registry 192.168.1.87:5000 我的macbook在图形化终端上修改如下，填写后“Apply&amp;Restart”。 3.4 下载镜像在另外的客户机上pull镜像，跟拉取DockerHub上的镜像是一样的流程。1$ docker pull 192.168.1.87:5000/hcharts 4，wen端查看私有仓库镜像访问:http://192.168.1.87:8080/，网页上呈现： 参考： 1, https://store.docker.com/images/registry2, https://store.docker.com/community/images/hyper/docker-registry-web","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.kekefund.com/tags/Docker/"},{"name":"Registry","slug":"Registry","permalink":"http://www.kekefund.com/tags/Registry/"}]},{"title":"scrapy+splash 爬取动态网站(JS)","date":"2017-05-25T09:05:06.000Z","path":"2017/05/25/scrapy-splash/","text":"scrapy只支持静态网页的抓取，通过scrapy-splash，能实现对JavaScript的解析。 一、搭建Docker服务器 Docker的相关知识参考本站的Docker相关文章。 Scrapy-Splash采用Splash HTTP API，需要搭建一个Splash实例，用Docker搭建最方便： 123$ docker run -d -p 8050:8050 --restart=always --name=splash -m 200m scrapinghub/splash # -m 200m 内存限制使用200兆 在服务器运行好Docker后，就可以通过IP+端口(例如：http://123.206.211.100:8050 )访问了。 二、Scrapy项目1，安装scrapy-splash1$ pip install scrapy-splash 2，配置(setting.py)增加Splash服务器地址1SPLASH_URL = 'http://123.206.211.100:8050' 开启Splash中间件123456DOWNLOADER_MIDDLEWARES = &#123; 'scrapy_splash.SplashCookiesMiddleware': 723, 'scrapy_splash.SplashMiddleware': 725, 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810, ...&#125; 其它设置123DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'SPLASH_COOKIES_DEBUG = True 3，spider.py使用SplashRequest12345678910111213141516171819202122import scrapyfrom scrapy_splash import SplashRequestclass SpiderS1(scrapy.Spider): name = \"s1_spider\" def start_requests(self): urls = ['http://sports.sina.com.cn/g/seriea/2017-05-23/doc-ifyfkqiv6736172.shtml', 'http://sports.sina.com.cn/basketball/nba/2017-05-23/doc-ifyfkqiv6683532.shtml'] requests = [] for url in urls: url = url.strip() request = SplashRequest(url, callback=self.parse, args=&#123;'wait':3&#125;) requests.append(request) return requests def parse(self, response): self.log(response.url) ... 使用非常简单，具体请求参数参考 Scrapy&amp;JavaScript integration through Splash。","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://www.kekefund.com/tags/Scrapy/"},{"name":"Docker","slug":"Docker","permalink":"http://www.kekefund.com/tags/Docker/"},{"name":"Splash","slug":"Splash","permalink":"http://www.kekefund.com/tags/Splash/"},{"name":"JavaScript","slug":"JavaScript","permalink":"http://www.kekefund.com/tags/JavaScript/"}]},{"title":"iReport 选择MySQL数据源生成报告","date":"2017-05-15T07:40:53.000Z","path":"2017/05/15/ireport-use-db/","text":"ireport是java项目中比较好用的pdf报告生成工具，可以通过JRBeanCollectionDataSource传递java对象至ireport。在项目比较小的时候是比较快捷的。当项目需求复杂了，人员增多了，这种方式不适合任务细分，前后流程衔接太紧密，而且不好调试。采用Mysql作为中间数据交换，能直接在ireport中预览数据展示结果。而且前后端解耦，可以将任务分派给不同的人来做。 一、配置数据库点击下图的红框： 新建一个Database JDBC connection:配置好MySQL的信息，Test成功！ 二、使用 JDBC连接1，SQL与Java Object的对应关系 2，SQL Qurey点击下图的红框：编写SQL语句，点击“Read Fields”，会显示查询的字段，点击OK，字段会在左侧的Fields中显示出来。 Fields会根据sql查询的字段自动生成：在Detail报表中调用Fields：Preview预览如下： 三、在Java项目中调用需要先在jrxml中设置语言为java，否则会报如下的错误：。1java.lang.NoClassDefFoundError: org/codehaus/groovy/control/CompilationFailedException java文件:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import net.sf.jasperreports.engine.JRExporterParameter;import net.sf.jasperreports.engine.JasperExportManager;import net.sf.jasperreports.engine.JasperFillManager;import net.sf.jasperreports.engine.JasperPrint;import net.sf.jasperreports.engine.JasperReport;import net.sf.jasperreports.engine.export.JRRtfExporter;import net.sf.jasperreports.engine.util.JRLoader;/** * 报告 * @author dzm * */@Controller@RequestMapping(\"productReport\")public class ProductRepotController extends BaseController&#123; /** * 一建生成报告 * parmas: fileFormat: pdf or doc */ @RequestMapping(value = \"exportReport\", method = RequestMethod.GET) public void exportReport(String fundId, String fundName, String fileFormat, HttpServletResponse response)&#123; Date curDate = new Date(); String dateStr = DateUtil.formatDate(curDate, \"yyyy-MM-dd\"); // 文件路径 StringBuilder filePath = new StringBuilder(\"\"); filePath.append(upload_path).append(dateStr).append(\"/\").append(fundId); FileUtil.createDictionary(filePath.toString()); // 文件名 StringBuilder fileName = new StringBuilder(\"\"); fileName.append(fundName).append(\"报告_\").append(dateStr).append(\".\").append(fileFormat); // 文件全路径 StringBuilder fullName = new StringBuilder(\"\"); fullName.append(filePath).append(\"/\").append(fileName); // 加载报表对象 InputStream is = null; JasperReport jasperReport = null; JasperPrint jasperPrint = null; is = ProductDetailController.class.getClassLoader().getResourceAsStream(analysis_jasper_path); try&#123; // 响应头部信息设置 response.setContentType(\"text/plain\"); response.setHeader(\"Content-Disposition\", \"attachment; filename=\"+FileUtil.getAttachName(fileName.toString())); // jasperReport = (JasperReport) JRLoader.loadObject(is); Map&lt;String,Object&gt; params = new HashMap&lt;String,Object&gt;(); params.put(\"SUBREPORT_DIR\", getClassPath(subreport_dir)); params.put(\"IMAGE_DIR\", \"file:\"+filePath+\"/\"); // Class.forName(\"com.mysql.jdbc.Driver\"); String url = \"jdbc:mysql://localhost:3306/test?\" + \"user=s1&amp;password=s1&amp;useUnicode=true&amp;characterEncoding=UTF8\"; Connection conn = (Connection) DriverManager.getConnection(url); jasperPrint = JasperFillManager.fillReport(jasperReport, params, conn); if (fileFormat.equals(\"pdf\")) &#123; JasperExportManager.exportReportToPdfFile(jasperPrint, fullName.toString()); &#125;else&#123; JRRtfExporter docReport = new JRRtfExporter(); docReport.setParameter(JRExporterParameter.OUTPUT_FILE_NAME,fullName.toString()); docReport.setParameter(JRExporterParameter.JASPER_PRINT, jasperPrint); docReport.exportReport(); &#125; FileUtil.download(fullName.toString(), response.getOutputStream()); &#125; catch(Exception e)&#123; e.printStackTrace(); logger.error(\"生成报告失败，失败原因:&#123;&#125;\",e.getMessage()); &#125; finally&#123; try&#123; if (is!=null) is.close(); &#125;catch(Exception e)&#123; logger.error(\"关闭文件输入流异常:&#123;&#125;\",e.getMessage()); &#125; &#125; &#125;&#125; 参考 https://my.oschina.net/jiangli0502/blog/122885http://stackoverflow.com/questions/8935925/jasperreports-compilation-error","tags":[{"name":"ireport","slug":"ireport","permalink":"http://www.kekefund.com/tags/ireport/"},{"name":"报告","slug":"报告","permalink":"http://www.kekefund.com/tags/报告/"},{"name":"mysql","slug":"mysql","permalink":"http://www.kekefund.com/tags/mysql/"}]},{"title":"Docker版highcharts中文导出服务器","date":"2017-05-12T08:57:28.000Z","path":"2017/05/12/docker-highcharts-server/","text":"highcharts的导出服务器使用固然方便，但需要把数据上传到它的服务器，对于数据安全性的考虑和外网访问限制的场景，搭建自己的导出服务器是比较可靠的。highcharts提供了一套搭建导出服务器的方法，本文在这个基础上将导出服务器封装成一个Docker容器，并且实现了图片中文字体的支持。 1，基于centos官方镜像1docker pull centos 2，启动容器并进入1docker run -it --name=hcharts -p 3002:80 -v /Users/Download:/home centos bash 3，安装phantomjs由于phantomjs的tar.bz2在centos容器中解压失败，我们采用在宿主机中下载并解压好，通过-v链接到容器中。在宿主机中下载最新版的phamtomjs，解压到/Users/Download/。在容器的/home目录下就能看到宿主机的解压文件。 设置PATH路径：12[root@bfdb9f8adaf6 my_fonts]# export PHANTOMJS_HOME = /PATH/phantomjs-2.1.1[root@bfdb9f8adaf6 my_fonts]# export PATH = $PATH:$PHANTOMJS_HOME/bin 请在下面相对应的linux版本 运行以下命令 12345# ubuntusudo apt-get install libfontconfig # centosyum install libXext libXrender fontconfig libfontconfig.so.1 输出下面的版本信息即安装成功 12# phantomjs --version2.1.1 4，安装node.js12curl --silent --location https://rpm.nodesource.com/setup_6.x | bash -yum -y install nodejs 参考:https://nodejs.org/en/download/package-manager/ 5，安装highcharts导出模块123456# 淘宝[npm镜像](http://npm.taobao.org/)：npm config set registry https://registry.npm.taobao.org // 全局安装导出模块npm install highcharts-export-server -g// 安装完毕后检查是否安装成功highcharts-export-server 6，中文字体的支持a，复制中文字体到容器将windows的fonts目录下拷贝simkai.ttf、simsun.ttc、simhei.ttf，通过-v上传到容器/home目录下。在容器的usr/share/fonts目录下新建my_fonts，12345$ cd /usr/share/fonts$ mkdir myfonts$ mv /home/simkai.ttf ./myfonts$ mv /home/simsun.ttc ./myfonts$ mv /home/simhei.ttf ./myfonts b，生成字体索引在myfonts目录下执行 mkfontscale，执行成功后执行下面命令：fc-list :lang=zh，输出有信息即安装成功！12345[root@bfdb9f8adaf6 my_fonts]# fc-list :lang=zh/usr/share/fonts/my_fonts/simsun.ttc: NSimSun,新宋体:style=Regular/usr/share/fonts/my_fonts/simhei.ttf: SimHei,黑体:style=Regular,Normal,obyčejné,Standard,Κανονικά,Normaali,Normál,Normale,Standaard,Normalny,Обычный,Normálne,Navadno,Arrunta/usr/share/fonts/my_fonts/simkai.ttf: KaiTi,楷体:style=Regular,Normal,obyčejné,Standard,Κανονικά,Normaali,Normál,Normale,Standaard,Normalny,Обычный,Normálne,Navadno,Arrunta/usr/share/fonts/my_fonts/simsun.ttc: SimSun,宋体:style=Regular 7，测试1curl -H \"Content-Type: application/json\" -X POST -d '&#123;\"infile\":&#123;\"title\": &#123;\"text\": \"中文测试\"&#125;, \"xAxis\": &#123;\"categories\": [\"Jan\", \"Feb\", \"Mar\"]&#125;, \"series\": [&#123;\"data\": [29.9, 71.5, 106.4]&#125;]&#125;&#125;' 127.0.0.1:3002 -o mychart.png 得到图片： 8，从Docker仓库获取本教程中的镜像已上传至docker官方仓库, 链接，按照说明pull即可。1docker run -d --name=hcharts0.2 --restart=always -p 3003:3000 cbbing/hcharts highcharts-export-server --enableServer 1 --port 3000 参考： 1，Linux下导出服务器缺少字体中文乱码问题解决办法 2，搭建导出服务器","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.kekefund.com/tags/Docker/"},{"name":"highcharts","slug":"highcharts","permalink":"http://www.kekefund.com/tags/highcharts/"}]},{"title":"docker版私人网盘ownCloud","date":"2017-04-19T04:37:04.000Z","path":"2017/04/19/docker-owncloud/","text":"ownCloud是一个自由且开源的个人云存储解决方案。ownCloud在客户端可通过网页界面，或者安装专用的客户端软件来使用。网页界面当然就是任何能开网页的平台都支持，而客户端软件也支持相当多平台，Windows、Linux、iOS、Android皆有。除了云存储之外，ownCloud也可用于同步日历、电子邮件联系人、网页浏览器的书签；此外还有多人在线文件同步协作的功能（类似google documents或Duddle等等）。ownCloud官方提供了Docker版的ownCloud，部署安装能一步到位。 如何使用Docker开始使用直接运行：1$ docker run -d -p 80:80 owncloud:8.1 然后进入 http://localhost/，根据向导配置。默认情况下使用SQLite作为数据储存。对于MySQL数据库，可以通过容器连接，例如:–link my-mysql:mysql。 数据持久化所有的数据在数据库中管理，数据保存在/var/www/html。可以通过以下命令对容器的数据卷和宿主机的数据卷映射。1-v /&lt;mydatalocation&gt;:/var/www/html 对于更细粒度的数据持久，设置如下的命令：123-v /&lt;mydatalocation&gt;/apps:/var/www/html/apps installed / modified apps-v /&lt;mydatalocation&gt;/config:/var/www/html/config local configuration-v /&lt;mydatalocation&gt;/data:/var/www/html/data the actual data of your ownCloud 通过docker-composeownCloud的docker-compose.yml示例如下：123456789101112131415161718192021222324252627282930313233# ownCloud with MariaDB/MySQL## Access via \"http://localhost:8080\" (or \"http://$(docker-machine ip):8080\" if using docker-machine)## During initial ownCloud setup, select \"Storage &amp; database\" --&gt; \"Configure the database\" --&gt; \"MySQL/MariaDB\"# Database user: root# Database password: example# Database name: pick any name# Database host: replace \"localhost\" with \"mysql\"version: '2'services: owncloud: image: owncloud volumes: - \"/mydata/code/ownCloud/ownData:/var/www/html\" ports: - 8021:80 mysql: image: mysql:5.6 volumes: - \"/mydata/code/ownCloud/mysqldata:/var/lib/mysql\" ports: - 3308:3306 environment: MYSQL_ROOT_PASSWORD: 123456 MYSQL_DATABASE: ownCloud MYSQL_USER: abc MYSQL_PASSWORD: 123456 创建1$ docker-compose up 查看状态12345678[root@VM_25_5_centos ownCloud]# docker-compose ps Name Command State Ports-------------------------------------------------------------------------owncloud_mysql_1 docker- Up 3306/tcp entrypoint.sh mysqldowncloud_ownclou /entrypoint.sh Up 0.0.0.0:8021-&gt;80d_1 apache2-for ... /tcp 删除123456[root@VM_25_5_centos ownCloud]# docker-compose downStopping owncloud_owncloud_1 ... doneStopping owncloud_mysql_1 ... doneRemoving owncloud_owncloud_1 ... doneRemoving owncloud_mysql_1 ... doneRemoving network owncloud_default ownCloud配置进入 http://localhost:8021/ , 出现页面：添加用户和数据库信息： 注意：红框内的数据库地址为docker-compose.yml中mysql的名称。点击“安装完成”！网页版登录：ownCloud支持windows，mac桌面端，ios/android手机端。基本可以替代在线网盘如百度网盘等。","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.kekefund.com/tags/Docker/"},{"name":"ownCloud","slug":"ownCloud","permalink":"http://www.kekefund.com/tags/ownCloud/"},{"name":"网盘","slug":"网盘","permalink":"http://www.kekefund.com/tags/网盘/"}]},{"title":"docker版Django","date":"2017-03-30T09:14:05.000Z","path":"2017/03/30/docker-django/","text":"Django的运行是基于python的环境，加上django包。在docker中运行django，实现方式是从docker下载python镜像，然后安装django运行所依赖的包。 在https://store.docker.com/images/python?tab=description 中介绍pull镜像方式有一种叫python:onbuild。这种镜像创建方式根据项目中提供的requirements.txt文件自动pip安装依赖包。大多数情况，通过python:onbuild能创建一个满足工程所需的独立镜像。 一、编写requirements.txt下述的目录结构是一个Django Rest Framework例子，其中项目名称为restful，app名称为api。 首先我们需要把项目所依赖的包放到requirements.txt中：12345678Django==1.8django-bootstrap-toolkit==2.15.0django-filter==1.0.1djangorestframework==3.5.4djangorestframework-jwt==1.10.0pandas==0.19.2SQLAlchemy==1.1.4MySQL-python==1.2.5 二、编写Dockerfile本文是基于python2.7制作的，Dockerfile文件如下：12FROM python:2-onbuildCMD [ \"python\", \"./manage.py\", \"runserver\", \"0.0.0.0:8000\"] CMD命令执行Django启动程序，0.0.0.0是对所有IP开放，监听端口8000。需要说明的是CMD中的每个参数得单独分开，像这样”runserver 0.0.0.0:8000”是运行不成功的。 2.1 pip 国内镜像先创建pip.conf文件，使用阿里云作为镜像：12345[global]index-url = http://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com 在创建Dockerfile:12345678910111213FROM python:3.5RUN mkdir -p /usr/src/appWORKDIR /usr/src/appCOPY pip.conf /root/.pip/pip.confCOPY requirements.txt /usr/src/app/RUN pip install -r /usr/src/app/requirements.txtRUN rm -rf /usr/src/appCOPY . /usr/src/appCMD [ \"python\", \"./manage.py\", \"runserver\", \"0.0.0.0:8091\"] 三、构建镜像 $ docker build -t my-python-app .123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[cbb@number_api]$ docker build -t number_api_django:0.3 .Sending build context to Docker daemon 655.9 kBStep 1/2 : FROM python:2-onbuild# Executing 3 build triggers...Step 1/1 : COPY requirements.txt /usr/src/app/Step 1/1 : RUN pip install --no-cache-dir -r requirements.txt ---&gt; Running in 4711187b3011Collecting Django==1.8 (from -r requirements.txt (line 2)) Downloading Django-1.8-py2.py3-none-any.whl (6.2MB)Collecting django-bootstrap-toolkit==2.15.0 (from -r requirements.txt (line 3)) Downloading django-bootstrap-toolkit-2.15.0.tar.gzCollecting django-filter==1.0.1 (from -r requirements.txt (line 4)) Downloading django_filter-1.0.1-py2.py3-none-any.whl (54kB)Collecting djangorestframework==3.5.4 (from -r requirements.txt (line 5)) Downloading djangorestframework-3.5.4-py2.py3-none-any.whl (709kB)Collecting djangorestframework-jwt==1.10.0 (from -r requirements.txt (line 6)) Downloading djangorestframework_jwt-1.10.0-py2.py3-none-any.whlCollecting pandas==0.19.2 (from -r requirements.txt (line 7)) Downloading pandas-0.19.2-cp27-cp27mu-manylinux1_x86_64.whl (17.2MB)Collecting SQLAlchemy==1.1.4 (from -r requirements.txt (line 8)) Downloading SQLAlchemy-1.1.4.tar.gz (5.1MB)Collecting MySQL-python==1.2.5 (from -r requirements.txt (line 9)) Downloading MySQL-python-1.2.5.zip (108kB)Collecting PyJWT&lt;2.0.0,&gt;=1.4.0 (from djangorestframework-jwt==1.10.0-&gt;-r requirements.txt (line 6)) Downloading PyJWT-1.4.2-py2.py3-none-any.whlCollecting pytz&gt;=2011k (from pandas==0.19.2-&gt;-r requirements.txt (line 7)) Downloading pytz-2016.10-py2.py3-none-any.whl (483kB)Collecting numpy&gt;=1.7.0 (from pandas==0.19.2-&gt;-r requirements.txt (line 7)) Downloading numpy-1.12.1-cp27-cp27mu-manylinux1_x86_64.whl (16.5MB)Collecting python-dateutil (from pandas==0.19.2-&gt;-r requirements.txt (line 7)) Downloading python_dateutil-2.6.0-py2.py3-none-any.whl (194kB)Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python2.7/site-packages (from python-dateutil-&gt;pandas==0.19.2-&gt;-r requirements.txt (line 7))Installing collected packages: Django, django-bootstrap-toolkit, django-filter, djangorestframework, PyJWT, djangorestframework-jwt, pytz, numpy, python-dateutil, pandas, SQLAlchemy, MySQL-python Running setup.py install for django-bootstrap-toolkit: started Running setup.py install for django-bootstrap-toolkit: finished with status 'done' Running setup.py install for SQLAlchemy: started Running setup.py install for SQLAlchemy: finished with status 'done' Running setup.py install for MySQL-python: started Running setup.py install for MySQL-python: finished with status 'done'Successfully installed Django-1.8 MySQL-python-1.2.5 PyJWT-1.4.2 SQLAlchemy-1.1.4 django-bootstrap-toolkit-2.15.0 django-filter-1.0.1 djangorestframework-3.5.4 djangorestframework-jwt-1.10.0 numpy-1.12.1 pandas-0.19.2 python-dateutil-2.6.0 pytz-2016.10Step 1/1 : COPY . /usr/src/app ---&gt; 712a54b6b923Removing intermediate container df33c056f7c0Removing intermediate container 4711187b3011Removing intermediate container 6220af43bf96Step 2/2 : CMD python ./manage.py runserver 0.0.0.0:8000 ---&gt; Running in 53c0cf32d840 ---&gt; 17c97bc704d9Removing intermediate container 53c0cf32d840Successfully built 17c97bc704d9[cbb@number_api]$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnumber_api_django 0.3 17c97bc704d9 23 seconds ago 868 MB 这样就成功创建了镜像number_api_django:0.3 四、运行容器 docker run12345678[cbb@number_api]$ docker run -it --rm -p 8080:8000 --name api1 number_api_django:0.3Performing system checks...System check identified no issues (0 silenced).March 30, 2017 - 07:34:03Django version 1.8, using settings 'restful.settings'Starting development server at http://0.0.0.0:8000/Quit the server with CONTROL-C. 这样就启动了django程序。","tags":[{"name":"Django","slug":"Django","permalink":"http://www.kekefund.com/tags/Django/"},{"name":"docker","slug":"docker","permalink":"http://www.kekefund.com/tags/docker/"},{"name":"Dockerfile","slug":"Dockerfile","permalink":"http://www.kekefund.com/tags/Dockerfile/"}]},{"title":"Django Rest Framework 通过token访问","date":"2017-03-30T03:47:57.000Z","path":"2017/03/30/django-rest-framework-jwt/","text":"在web apps上实现身份验证时，首先考虑到的解决方案就是Cookie。基于Cookie的身份验证使用服务器端Cookie来对每个请求进行身份验证，这意味着您需要在数据库中（如Redis）保留一个会话存储。 基于token令牌的身份验证是一个最近比较流行的解决方案，它依赖于每个请求发送到服务器的签名令牌，对于移动端和网页端都比较适用。 一、安装需先安装django rest framework 1pip install djangorestframework-jwt 二、使用setting.py12345678910REST_FRAMEWORK = &#123; 'DEFAULT_PERMISSION_CLASSES': ( 'rest_framework.permissions.IsAuthenticated', ), 'DEFAULT_AUTHENTICATION_CLASSES': ( 'rest_framework_jwt.authentication.JSONWebTokenAuthentication', 'rest_framework.authentication.SessionAuthentication', 'rest_framework.authentication.BasicAuthentication', ),&#125; 工程的urls.py123456789from rest_framework_jwt.views import obtain_jwt_token#...urlpatterns = [ '', # ... url(r'^api-token-auth/', obtain_jwt_token),] 三、python 访问1, 获得令牌123456n [41]: import requestsIn [42]: r = requests.post(\"http://localhost:8000/api-token-auth/\", data=&#123;'username':'cbb','password':'cbb'&#125;)In [43]: print r.text&#123;\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImNiYiIsInVzZXJfaWQiOjEsImVtYWlsIjoiY2JiaW5nQDE2My5jb20iLCJleHAiOjE0OTA4MjI5MDF9.k5fznq2RoEnsIIFYvc-afHLKXiEyfZyHjRwV8-db5FM\"&#125; 注意，post请求需要网址最后带“/”，django默认自动补全是关闭的。 网址不带/的出错提示： You called this URL via POST, but the URL doesn’t end in a slash and you have APPEND_SLASH set. Django can’t redirect to the slash URL while maintaining POST data. Change your form to point to 127.0.0.1:8000/api/users/ (note the trailing slash), or set APPEND_SLASH=False in your Django settings. 123456In [44]: import jsonIn [45]: jsonData = json.loads(r.text)In [46]: jsonData[u'token']Out[46]: u'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImNiYiIsInVzZXJfaWQiOjEsImVtYWlsIjoiY2JiaW5nQDE2My5jb20iLCJleHAiOjE0OTA4MjI5MDF9.k5fznq2RoEnsIIFYvc-afHLKXiEyfZyHjRwV8-db5FM' 2，通过令牌访问 构造headers 1234In [48]: headers = &#123;'Authorization': 'JWT &#123;&#125;'.format(jsonData[u'token'])&#125;In [49]: headersOut[49]: &#123;'Authorization': 'JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImNiYiIsInVzZXJfaWQiOjEsImVtYWlsIjoiY2JiaW5nQDE2My5jb20iLCJleHAiOjE0OTA4MjI5MDF9.k5fznq2RoEnsIIFYvc-afHLKXiEyfZyHjRwV8-db5FM'&#125; get请求 1234In [50]: r = requests.get(\"http://127.0.0.1:8000/api/users?format=json\", headers=headers)In [51]: print r.text[&#123;\"name\":\"cbb\",\"birthday\":\"2017-03-07\",\"gender\":1&#125;,&#123;\"name\":\"xx\",\"birthday\":\"2017-03-01\",\"gender\":2&#125;,&#123;\"name\":\"keke\",\"birthday\":\"2016-03-01\",\"gender\":2&#125;,&#123;\"name\":\"小小\",\"birthday\":\"2017-03-07\",\"gender\":2&#125;,&#123;\"name\":\"小小1\",\"birthday\":\"2017-03-07\",\"gender\":2&#125;,&#123;\"name\":\"小小1\",\"birthday\":\"2017-03-07\",\"gender\":2&#125;,&#123;\"name\":\"xx\",\"birthday\":\"2017-03-07\",\"gender\":1&#125;,&#123;\"name\":\"ckk\",\"birthday\":\"2017-03-22\",\"gender\":2&#125;,&#123;\"name\":\"ckk\",\"birthday\":\"2017-03-22\",\"gender\":2&#125;,&#123;\"name\":\"ckkk\",\"birthday\":\"2017-03-09\",\"gender\":2&#125;,&#123;\"name\":\"cbb\",\"birthday\":\"2017-03-07\",\"gender\":1&#125;,&#123;\"name\":\"cbb\",\"birthday\":\"2017-03-07\",\"gender\":1&#125;] post请求 123456In [54]: data = &#123;\"name\":\"xx\",\"birthday\":\"2017-03-01\",\"gender\":2&#125;In [55]: r = requests.post(\"http://127.0.0.1:8000/api/users/\",data=data, headers=headers)In [56]: print r.text\"&#123;\\\"birthdayNumberCount\\\": &#123;\\\"1\\\": 4, \\\"2\\\": 1, \\\"3\\\": 1, \\\"4\\\": 2, \\\"5\\\": 3, \\\"6\\\": 0, \\\"7\\\": 1, \\\"8\\\": 0, \\\"9\\\": 0&#125;, \\\"suitableJob\\\": \\\"公众人物、开发商、投机者、设计师、新闻工作(媒体)、表演者、变革推动者、广告创意人才、探险家、心灵导师、作家、自由职业\\\", \\\"destinyNumber\\\": 5, \\\"destinyMean\\\": \\\"很注重感观享受，喜欢冒险、自由，个性开朗，人缘好；有口才，社交能力强，拥有演说和促销的天才。不容易离婚，爱美。\\\", \\\"destinyDetailMean\\\": \\\"因4的能力充斥内在，更需要职业的稳定来协助创造力与变动，不然就会形成外强中干，而无法让自己身心自由。\\\", \\\"birthdayNumbers\\\": [2, 0, 1, 7, 0, 3, 0, 1, 1, 4, 5], \\\"toLearn\\\": \\\"节制自由，学习承诺与勇气。\\\", \\\"userInfo\\\": &#123;\\\"username\\\": \\\"xx\\\", \\\"gender\\\": 2, \\\"birthday\\\": \\\"2017-03-01\\\"&#125;, \\\"talentNumbers\\\": [14]&#125;\" 四、进阶自定义令牌有效期，如设置有效期为5小时，在setting.py中添加123JWT_AUTH = &#123; 'JWT_EXPIRATION_DELTA': datetime.timedelta(hours=5), #seconds=300&#125; 官方文档中所有能自定义的参数如下：1234567891011121314151617181920212223242526272829303132333435JWT_AUTH = &#123; 'JWT_ENCODE_HANDLER': 'rest_framework_jwt.utils.jwt_encode_handler', 'JWT_DECODE_HANDLER': 'rest_framework_jwt.utils.jwt_decode_handler', 'JWT_PAYLOAD_HANDLER': 'rest_framework_jwt.utils.jwt_payload_handler', 'JWT_PAYLOAD_GET_USER_ID_HANDLER': 'rest_framework_jwt.utils.jwt_get_user_id_from_payload_handler', 'JWT_RESPONSE_PAYLOAD_HANDLER': 'rest_framework_jwt.utils.jwt_response_payload_handler', 'JWT_SECRET_KEY': settings.SECRET_KEY, 'JWT_GET_USER_SECRET_KEY': None, 'JWT_PUBLIC_KEY': None, 'JWT_PRIVATE_KEY': None, 'JWT_ALGORITHM': 'HS256', 'JWT_VERIFY': True, 'JWT_VERIFY_EXPIRATION': True, 'JWT_LEEWAY': 0, 'JWT_EXPIRATION_DELTA': datetime.timedelta(seconds=300), 'JWT_AUDIENCE': None, 'JWT_ISSUER': None, 'JWT_ALLOW_REFRESH': False, 'JWT_REFRESH_EXPIRATION_DELTA': datetime.timedelta(days=7), 'JWT_AUTH_HEADER_PREFIX': 'JWT', 'JWT_AUTH_COOKIE': None,&#125; 参考 https://github.com/GetBlimp/django-rest-framework-jwthttp://getblimp.github.io/django-rest-framework-jwt/#requirementshttp://stackoverflow.com/questions/21317899/how-do-i-create-a-login-api-using-django-rest-framework","tags":[{"name":"Django","slug":"Django","permalink":"http://www.kekefund.com/tags/Django/"},{"name":"DjangoRestFramework","slug":"DjangoRestFramework","permalink":"http://www.kekefund.com/tags/DjangoRestFramework/"},{"name":"API","slug":"API","permalink":"http://www.kekefund.com/tags/API/"}]},{"title":"Django Rest Framwork实现RESTful API","date":"2017-03-30T02:46:25.000Z","path":"2017/03/30/django-restful-framework/","text":"安装123pip install djangorestframeworkpip install markdown # Markdown为可视化 API 提供了支持pip install django-filter 创建工程 工程名：restfulapp名：apiIDE：PyCharm 配置rest_framework1234567891011121314151617181920212223242526272829\"setting.py\"...# Application definitionINSTALLED_APPS = ( 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', # 新增 'api', 'rest_framework',)# 新增REST_FRAMEWORK = &#123; 'DEFAULT_PERMISSION_CLASSES': ('rest_framework.permissions.IsAdminUser',), 'PAGINATE_BY': 10&#125;... 配置数据库数据库采用mysql 123456789101112131415\"setting.py\"DATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'restful', 'USER': 'admin', 'PASSWORD': '123', 'HOST': '127.0.0.1', 'PORT': 3306, &#125;&#125; 建立模型1234567891011121314151617181920\"models.py\"#coding:utf-8from django.db import models# Create your models here.class User(models.Model): GENDER_CHOICES = ( (1, \"Male\"), (2, \"Female\") ) name = models.CharField(max_length=60, blank=False, verbose_name='姓名') birthday = models.DateField(blank=False) gender = models.IntegerField(choices=GENDER_CHOICES) def __unicode__(self): return self.name + \" ( \" + str(self.birthday) + \")\" 同步数据库12python manage.py makemigrationspython manage.py migrate 序列化在api下新建serializers.py 123456789101112131415\"serializers.py\"#coding:utf-8from rest_framework import serializersfrom models import User, NumberologyInfo, OtherInfoclass UserSerializer(serializers.ModelSerializer): class Meta: model = User fields = ('name', 'birthday', 'gender') 添加视图12345678910111213141516171819202122232425\"views.py\"#coding:utf-8from django.shortcuts import render# Create your views here.from django.shortcuts import render# Create your views here.from rest_framework import viewsetsfrom rest_framework.views import APIViewfrom rest_framework.response import Responsefrom rest_framework.serializers import Serializerfrom rest_framework import generics, permissionsfrom .models import User, NumberologyInfofrom .serializers import UserSerializer, NumberologyInfoSerializerclass UserViewSet(viewsets.ModelViewSet): \"\"\" 允许查看和编辑user 的 API endpoint \"\"\" queryset = User.objects.all() serializer_class = UserSerializer 创建视图的三种方式1234567891011121314151617181920212223242526272829303132333435363738394041\"views.py\"# 第一种方式：APIViewclass TaskList(APIView): def get(self, request, format=None): users = User.objects.all() serializer = UserSerializer(users, many=True) return Response(serializer.data) def post(self, request, format=None): serializer = UserSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) else: return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)# 第二种方式：通用视图 ListCreateAPIViewclass TaskListCreate(generics.ListCreateAPIView): queryset = User.objects.all() serializer_class = UserSerializer# 第三种方式：装饰器 api_view@api_view(['GET', 'POST'])def task_list(request): ''' List all tasks, or create a new task. ''' if request.method == 'GET': tasks = User.objects.all() serializer = UserSerializer(tasks, many=True) return Response(serializer.data) elif request.method == 'POST': serializer = UserSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) else: return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) 设置url123456789101112131415161718192021\"urls.py\"#coding:utf-8from django.conf.urls import patterns, url, includefrom rest_framework import routersfrom api import viewsrouter = routers.DefaultRouter()router.register(r'users', views.UserViewSet)# Wire up our API using automatic URL routing.# Additionally, we include login URLs for the browseable API.urlpatterns = [ url(r'^', include(router.urls)), #验证登录使用 url(r'auth',include('rest_framework.urls')), ] 启动服务 参考 1，django-rest-framework 系列教程（一）- Start Your API 2，Django RESTful API 设计指南 3，利用 Django REST framework 编写 RESTful API 4，用Django Rest Framework和AngularJS开始你的项目 5，Django Rest Framework 入门指南 6，django-rest-framework里的api请求频率控制 7， 验证和授权","tags":[{"name":"Django","slug":"Django","permalink":"http://www.kekefund.com/tags/Django/"},{"name":"DjangoRestFramework","slug":"DjangoRestFramework","permalink":"http://www.kekefund.com/tags/DjangoRestFramework/"},{"name":"API","slug":"API","permalink":"http://www.kekefund.com/tags/API/"}]},{"title":"centos7安装VNC服务器","date":"2017-03-24T02:51:59.000Z","path":"2017/03/24/centos7-install-gnome/","text":"centos7系统下的VNC服务器的中文安装教程多如牛毛，有些安装流程复杂但到最后却不成功，本人试验了不下10个教程，装的快要吐血😓。谷歌到这篇英文教程How to install VNC server on Centos 7，发现是良心之作，操作简单可行。于是翻译之以饷读者。 VNC服务器用于从远程客户端连接到服务器的桌面环境。远程计算机上使用VNC客户端连接服务器。在本文我们可以了解如何在centos 7上安装VNC服务器，将采用centos yum库中提供的默认包来安装。 安装 VNC服务器如果你不曾安装过桌面环境（X windows），就按照以下命令来安装软件，重启后，你就会具有centos7的桌面。 123456[root@krizna ~]# yum check-update[root@krizna ~]# yum groupinstall \"X Window System\"[root@krizna ~]# yum install gnome-classic-session gnome-terminal nautilus-open-terminal control-center liberation-mono-fonts[root@krizna ~]# unlink /etc/systemd/system/default.target[root@krizna ~]# ln -sf /lib/systemd/system/graphical.target /etc/systemd/system/default.target[root@krizna ~]# reboot 现在开始安装VCN包步骤1：执行下面的命令安装VNC包1[root@krizna ~]# yum install tigervnc-server -y 步骤2：将/lib/systemd/system/vncserver@.service拷贝至/etc/systemd/system/，并重命名为vncserver@:1.service。1[root@krizna ~]# cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver@:1.service 步骤3：打开/etc/systemd/system/下的vncserver@:1.service，将替换为你的用户名。找到这两行：12ExecStart=/sbin/runuser -l &lt;USER&gt; -c \"/usr/bin/vncserver %i\"PIDFile=/home/&lt;USER&gt;/.vnc/%H%i.pid 替换为（假定用户名为john）：12ExecStart=/sbin/runuser -l john -c \"/usr/bin/vncserver %i\"PIDFile=/home/john/.vnc/%H%i.pid 如果你是root用户，就这样替换：12ExecStart=/sbin/runuser -l root -c \"/usr/bin/vncserver %i\"PIDFile=/root/.vnc/%H%i.pid 步骤4：重新加载systemd进行更改1[root@krizna ~]# systemctl daemon-reload 步骤5：创建VNC密码1[root@krizna ~]# vncpasswd 步骤6：启动服务，并设置开机自动运行12[root@krizna ~]# systemctl enable vncserver@:1.service[root@krizna ~]# systemctl start vncserver@:1.service 步骤7：防火墙允许VNC访问 12[root@krizna ~]# firewall-cmd --permanent --add-service vnc-server[root@krizna ~]# systemctl restart firewalld.service 到这里，你就可以用VNC客户端连接服务器桌面了（192.168.11.165:1）。 PS: VNC客户端各个系统版本都有，自行百度~。我这里用的是mac版的VNC Viewer。 对于其他用户，创建不同的端口文件vncserver@:2.service，参考步骤2，然后重复步骤3，4，5，6即可。 附加命令 停止VNC服务 1[root@krizna ~]# systemctl stop vncserver@:1.service 取消开机自动运行 1[root@krizna ~]# systemctl disable vncserver@:1.service 关闭防火墙（用于故障排除） 1[root@krizna ~]# systemctl stop firewalld.service 好运~ 翻译自: How to install VNC server on Centos 7","tags":[{"name":"centos7","slug":"centos7","permalink":"http://www.kekefund.com/tags/centos7/"},{"name":"VNC","slug":"VNC","permalink":"http://www.kekefund.com/tags/VNC/"},{"name":"gnome","slug":"gnome","permalink":"http://www.kekefund.com/tags/gnome/"}]},{"title":"github创建分支","date":"2017-03-23T08:02:32.000Z","path":"2017/03/23/git-branch-create/","text":"原理理论部分参考：创建与合并分支 实践软件一个阶段开发完成，需要建立一个分支来保存当前的稳定版本。 采用SourceTree 创建分支： 创建完成后再SourceTree左侧树型菜单会出现分支名称： 点击推送，将分支同步到服务器 github上就能看到分支了：","tags":[{"name":"git","slug":"git","permalink":"http://www.kekefund.com/tags/git/"},{"name":"branch","slug":"branch","permalink":"http://www.kekefund.com/tags/branch/"},{"name":"SourceTree","slug":"SourceTree","permalink":"http://www.kekefund.com/tags/SourceTree/"}]},{"title":"docker版FTP服务器","date":"2017-03-20T13:52:43.000Z","path":"2017/03/20/docker-ftp/","text":"docker版ftp服务器，适用于部署离线局域网服务器 下载来源：https://hub.docker.com/r/bogem/ftp/12345[root@VM_25_5_centos mydata]# docker pull bogem/ftp[root@VM_25_5_centos mydata]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/bogem/ftp latest a40e9c43c530 4 weeks ago 174.7 MB 运行1234567docker run -d -v /mydata:/home/vsftpd -p 20:20 -p 21:21 -p 47400-47470:47400-47470 \\-e FTP_USER=test -e FTP_PASS=test -e PASV_ADDRESS=0.0.0.0 --name ftp1 \\--restart=always bogem/ftp 注：PASV_ADDRESS如果设置成127.0.0.1，则只能本地访问；设置成0.0.0.0 则可以外网访问。详细参数参考:https://hub.docker.com/r/fauria/vsftpd/ 中的环境变量部分。 FileZilla客户端连接","tags":[{"name":"docker","slug":"docker","permalink":"http://www.kekefund.com/tags/docker/"},{"name":"ftp","slug":"ftp","permalink":"http://www.kekefund.com/tags/ftp/"}]},{"title":"docker部署mysql","date":"2017-03-19T15:54:44.000Z","path":"2017/03/19/docker-mysql/","text":"mysql在linux服务器上运行一直比较稳定，但是服务器迁移时mysql在新服务器上的配置是个比较头疼的问题，搞不好数据迁移过来了但是mysql启动不起来，坑比较多。特别是当新的服务器是离线时，安装mysql和数据同步软件更是困难重重。用docker来运行mysql服务是一个比较好的解决方案，mysql的运行环境在容器内已经封装好了，而数据可以直接挂载在宿主主机上。 一、下载镜像 官网地址：https://hub.docker.com/_/mysql/ 1docker pull mysql 查看镜像123[root@VM_25_5_centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/mysql latest 22be5748ecbe 13 days ago 405.6 MB 二、启动容器12$ docker run --name cbb-mysql1 -p 3307:3306 -v /home/mysql_data:/var/lib/mysql --restart=always -e MYSQL_ROOT_PASSWORD=123456 -d &lt;IMAGE-ID&gt;b344e219ff03a92d65f75f74ab5b227838cce8619cbe695ccd1b6889f9a3d174 -p：容器的3306映射到主机的3307端口 -v：容器的/var/lib/mysql目录挂载在主机的/home/mysql_data目录 -e 设置默认参数，支持参数： MYSQL_ROOT_PASSWORD MYSQL_DATABASE MYSQL_USER, MYSQL_PASSWORD MYSQL_ALLOW_EMPTY_PASSWORD MYSQL_RANDOM_ROOT_PASSWORD MYSQL_ONETIME_PASSWORD 参考：https://hub.docker.com/_/mysql/ 的环境参数部分（Environment Variables) 返回一长串字符，则说明创建成功。注：也可以是REPOSITORY+TAG，如docker.io/mysql: latest 三、进入容器123456789101112131415[root@VM_200_249_centos mysql-docker]# docker exec -it cbb-mysql1 mysql -uroot -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.7.17 MySQL Community Server (GPL)Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; 四、实践离线数据库mysql目录下所有文件拷贝到离线服务器上放到指定目录：/home/mysql_data执行命令，建立mysql容器12[localhost mysql]# docker run --name cbb-mysql1 -v /home/mysql_data:/var/lib/mysql -p 3307:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6629a150d4a9cb87080df3d89a6b91e3d56ddd8699e2f6a2d3a908f39d6f87e4c 1, 进入容器，新建账户12mysql&gt; use mysql;mysql&gt; create USER 'ts01'@'%' IDENTIFIED BY '123456'; 说明： username：你将创建的用户名 host：指定该用户在哪个主机上可以登陆，如果是本地用户可用localhost，如果想让该用户可以从任意远程主机登陆，可以使用通配符% password：该用户的登陆密码，密码可以为空，如果为空则该用户可以不需要密码登陆服务器 2, 授权:命令:1GRANT privileges ON databasename.tablename TO 'username'@'host' 说明: privileges：用户的操作权限，如SELECT，INSERT，UPDATE等，如果要授予所的权限则使用ALL databasename：数据库名 tablename：表名，如果要授予该用户对所有数据库和表的相应操作权限则可用表示，如.* 例子: 12GRANT SELECT, INSERT ON test.user TO 'pig'@'%';GRANT ALL ON *.* TO 'pig'@'%'; 注意: 用以上命令授权的用户不能给其它用户授权，如果想让该用户可以授权，用以下命令: 1GRANT privileges ON databasename.tablename TO 'username'@'host' WITH GRANT OPTION; 3，容器间连接其它容器想访问mysql容器，可以使用link来连接。例如运行：1docker run -d -p 80:8096 -v /home/gtdata:/gtdata --restart=always --name web01 --link=cbb-mysql1:mydb java_web:1.0 /root/run.sh 注： -p 端口映射 -v 宿主机目录挂载 –name 容器名 –link 连接的容器，另一个容器cbb-mysql在本容器中的名称为mydb，可以直接在本容器中使用mydb java_web:1.0容器里运行的是tomcat。进入jdbc.properties修改12# mydb是mysql容器在web01容器中的别名，mydb等价于192.168.11.121:3306jdbc.url=jdbc:mysql://mydb:3306/guotai?useUnicode=true&amp;characterEncoding=utf-8 其它 1, [宿主服务器上安装VNC] (http://www.krizna.com/centos/install-vnc-server-centos-7/)2, centos版workbench","tags":[{"name":"docker","slug":"docker","permalink":"http://www.kekefund.com/tags/docker/"},{"name":"mysql","slug":"mysql","permalink":"http://www.kekefund.com/tags/mysql/"}]},{"title":"mac下安装docker","date":"2017-03-17T07:01:10.000Z","path":"2017/03/17/docker-on-mac/","text":"安装系统要求：OS X EI Captian 10.11以上 docker默认是在linux下运行，要在mac下运行，需要安装linux的虚拟环境。好在docker官网提供了mac版的docker安装包。在https://www.docker.com/docker-mac 下载Docker.img。安装完成后，顶栏会出现 双击安装后，到终端查看：docker version1234567891011121314151617[cbb@~]$ docker versionClient: Version: 17.03.0-ce API version: 1.26 Go version: go1.7.5 Git commit: 60ccb22 Built: Thu Feb 23 10:40:59 2017 OS/Arch: darwin/amd64Server: Version: 17.03.0-ce API version: 1.26 (minimum version 1.12) Go version: go1.7.5 Git commit: 3a232c8 Built: Tue Feb 28 07:52:04 2017 OS/Arch: linux/amd64 Experimental: true 出现版本号即安装成功！ 配置可以设置开机启动 设置与宿主机的文件共享 设置CPU和内存大小，与VirtualBox一样。","tags":[{"name":"mac","slug":"mac","permalink":"http://www.kekefund.com/tags/mac/"},{"name":"docker","slug":"docker","permalink":"http://www.kekefund.com/tags/docker/"}]},{"title":"docker 自动构建，基于Dockerfile文件","date":"2017-03-06T04:48:22.000Z","path":"2017/03/06/docker-dockerfile-generated/","text":"1，Dockerfile的编写在centos中创建一个目录:/mydata/data/dockertest/，新建Dockerfile文件vim Dockerfile12345678910111213141516# Verison 0.6:# 基础镜像FROM chenbb/fofeasy:0.6# 维护者信息MAINTAINER cbbing@163.com# 镜像操作命令RUN rm -rf /opt/tomcat/webapps/fofeasyRUN rm -rf /opt/tomcat/webapps/fofeasy.warADD fofeasy.war /opt/tomcat/webapps/fofeasy.war# 容器启动命令#CMD [\"/opt/tomcat/bin/catalina.sh\", \"run\"] 编写完成后:wq保存。 2，构建基于Dockerfile构建镜像，在Dockerfile文件所在目录下执行123456789101112131415161718192021[root@VM_200_249_centos dockertest]# docker build -t chenbb/fofeasy:0.7 .Sending build context to Docker daemon 65.78 MBStep 1 : FROM chenbb/fofeasy:0.6 ---&gt; c441af7f5405Step 2 : MAINTAINER cbbing@163.com ---&gt; Running in f7cbd5cd3199 ---&gt; cef4cee90997Removing intermediate container f7cbd5cd3199Step 3 : RUN rm -rf /opt/tomcat/webapps/fofeasy ---&gt; Running in 79505ed64d7f ---&gt; 4f85be099a20Removing intermediate container 79505ed64d7fStep 4 : RUN rm -rf /opt/tomcat/webapps/fofeasy.war ---&gt; Running in be162f93530b ---&gt; c5cc2ba60023Removing intermediate container be162f93530bStep 5 : ADD fofeasy.war /opt/tomcat/webapps/fofeasy.war ---&gt; 8ede3a4f83e5Removing intermediate container b9b557e26828Successfully built 8ede3a4f83e5[root@VM_200_249_centos dockertest]# 注： chenbb/fofeasy:0.7为新镜像的名字fofeasy.war文件放到同一目录 1234[root@VM_200_249_centos dockertest]# ll -lh总用量 63M-rw-r--r-- 1 root root 322 3月 3 17:00 Dockerfile-rw-r--r-- 1 root root 63M 3月 3 16:46 fofeasy.war 3，启动1docker run -d -p 58080:8080 --name javaweb chenbb/javaweb:0.7 /root/run.sh 3，一些问题 容器启动不起来考虑是容器里的命令执行报错引起的，重新从镜像创建容器，排除问题，或者通过“docker logs &lt;容器ID&gt;” 查看错误日志 参考 http://www.jianshu.com/p/690844302df5","tags":[{"name":"docker","slug":"docker","permalink":"http://www.kekefund.com/tags/docker/"},{"name":"dockerfile","slug":"dockerfile","permalink":"http://www.kekefund.com/tags/dockerfile/"}]},{"title":"centos + docker部署web java项目","date":"2017-02-28T16:01:59.000Z","path":"2017/03/01/centos-docker-deploy/","text":"1，系统准备CentOS 具体要求如下： 必须是 64 位操作系统 建议内核在 3.8 以上 通过以下命令查看您的 CentOS 内核：12[root@VM_200_249_centos ~]# uname -r3.10.0-327.36.3.el7.x86_64 对于 CentOS 6.5 而言，内核版本默认是 2.6。首先，可通过以下命令安装最新内核：123rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpmyum -y --enablerepo=elrepo-kernel install kernel-lt 随后，编辑以下配置文件：1vi /etc/grub.conf 将default=1修改为default=0。最后，通过reboot或shutdown now命令重启操作系统。重启后如果不出意外的话，再次查看内核，您的 CentOS 内核将会显示为 3.10。 2，安装Docker只需通过以下命令即可安装 Docker 软件：12rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpmyum -y install docker-io 可通过以下命令启动 Docker 服务： 12service docker startchkconfig docker on # 设置开机启动 可使用以下命令，查看 Docker 是否安装成功：12345678910111213141516[root@VM_200_249_centos ~]# docker versionClient: Version: 1.12.3 API version: 1.24 Go version: go1.6.3 Git commit: 6b644ec Built: OS/Arch: linux/amd64Server: Version: 1.12.3 API version: 1.24 Go version: go1.6.3 Git commit: 6b644ec Built: OS/Arch: linux/amd64 若输出了 Docker 的版本号，则说明安装成功，我们下面就可以开始使用 Docker 了。 3， 安装镜像Docker 官网 提供了所有的镜像下载地址。（需要VPN翻墙）直接pull下来(https://hub.docker.com/r/nimmis/java-centos/)1docker pull nimmis/java-centos 可以通过对应的标签选择不同的jdk版本，例如”docker pull nimmis/java-centos:openjdk-7-jdk” latest - currently Oracle Java version 8 JRE openjdk-7-jdk - OpenJDK Java version 7 JDK openjdk-7-jre - OpenJDK Java version 7 JRE openjdk-7-jre-headless - OpenJDK Java version 7 JRE headless openjdk-8-jdk - OpenJDK Java version 8 JDK openjdk-8-jre - OpenJDK Java version 8 JRE openjdk-8-jre-headless - OpenJDK Java version 8 JRE headless oracle-7-jre - Oracle Java version 7 JRE oracle-7-jdk - Oracle Java version 7 JDK oracle-8-jre - Oracle Java version 8 JRE oracle-8-jdk - Oracle Java version 8 JDK 镜像加速器docker官网的镜像下载非常慢，国内提供了Docker镜像的下载点，如阿里、网易和DaoCloud。以阿里云为例：需要先注册阿里云账号，进到1https://cr.console.aliyun.com/#/accelerator 选择左侧“加速器”，找到你的专属加速器地址在centos下修改/etc/docker/daemon.json文件，添加：123&#123; \"registry-mirrors\": [\"https://yxz1pr3x.mirror.aliyuncs.com\"]&#125; 设置后能获得每秒1兆的下载速度。 最后，使用以下命令查看本地所有的镜像：1234[root@VM_200_249_centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdockerxman/docker-centos latest 4c89ecb22b17 2 minutes ago 395.6 MBdaocloud.io/library/centos latest 67591570dd29 10 weeks ago 191.8 MB 可以看到，系统中有2个docker镜像，“dockerxman/docker-centos”，也可以称其为仓库（Repository），镜像的标签（Tag）为lastest，此外还有镜像ID（IMAGE ID），大小有395兆。 4，启动容器容器在镜像的基础上运行，一旦容器启动了，我们就可以登录到容器中，安装自己所需的软件或应用程序。1docker run -i -t -v /mydata/data:/mnt/software --restart=always 67591570dd29 /bin/bash 命令解释：1docker run &lt;相关参数&gt; &lt;镜像 ID&gt; &lt;初始命令&gt; 其中，相关参数包括： -i：表示以“交互模式”运行容器 -t：表示容器启动后会进入其命令行 -v：表示需要将本地哪个目录挂载到容器中，格式：-v &lt;宿主机目录&gt;:&lt;容器目录&gt; 初始命令表示一旦容器启动，需要运行的命令，此时使用“/bin/bash”，表示什么也不做，只需进入命令行即可。需要说明的是，不一定要使用“镜像 ID”，也可以使用“仓库名:标签名”，例如：dockerxman/docker-centos:latest。 查看所有创建的容器1docker ps -a 查看正在运行的容器123[root@VM_200_249_centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa197e23e0ddb asy:0.1 \"/root/run.sh\" 24 hours ago Up 24 hours 0.0.0.0:58080-&gt;8080/tcp javaweb 启动容器1docker start &lt;CONTAINER ID&gt; 停止容器1docker stop &lt;CONTAINER ID&gt; 重新进入已创建的容器1docker attach &lt;CONTAINER ID&gt; 或1docker exec -it &lt;CONTAINER ID&gt; /bin/bash http://blog.csdn.net/u010397369/article/details/41045251 删除容器1docker rm &lt;CONTAINER ID&gt; 删除镜像1docker rmi &lt;IMAGE ID&gt; 注：需要先把镜像生成的容器全部删除才能删掉镜像。 5，安装软件由于我们选择的镜像是包含JDK的，所以我们只需要安装tomcat。tomcat我放在服务器上，用wget下载到容器中。1wget http://kekefund.com/soft/apache-tomcat-7.0.63.tar.gz 将tomcat放到/opt目录下，先移到到/opt目录，然后解压12cd /opttar -zxf /mnt/software/apache-tomcat-7.0.63.tar.gz -C . 重命名1mv apache-tomcat-7.0.63/ tomcat/ 6，运行脚本创建运行脚本：vi /root/run.sh123#!/bin/bashsource ~/.bashrcsh /opt/tomcat/bin/catalina.sh run 为运行脚本添加执行权限1chmod u+x /root/run.sh 7, 创建 Java Web 镜像使用以下命令，根据某个“容器 ID”来创建一个新的“镜像”：1docker commit d50f5048e212 chenbb/javaweb:0.1 该容器的 ID 是“ d50f5048e212”，所创建的镜像名是“ chenbb/javaweb:0.1”，随后可使用镜像来启动 Java Web 容器。 8, 启动 Java Web 容器通过docker images查看所有镜像12345[root@VM_200_249_centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEchenbb/javaweb 0.1 99e35759d5ed 24 hours ago 700 MBdockerxman/docker-centos latest 4c89ecb22b17 26 hours ago 395.6 MBdaocloud.io/library/centos latest 67591570dd29 10 weeks ago 191.8 MB 可见，此时已经看到了最新创建的镜像“ chenbb/javaweb:0.1”，其镜像 ID 是“ 99e35759d5ed”。正如上面所描述的那样，我们可以通过“镜像名”或“镜像 ID”来启动容器，与上次启动容器不同的是，我们现在不再进入容器的命令行，而是直接启动容器内部的 Tomcat 服务。此时，需要使用以下命令：1docker run -d -p 58080:8080 --name javaweb --restart=always chenbb/javaweb:0.1 /root/run.sh 参数介绍： -d：表示以“守护模式”执行/root/run.sh脚本，此时 Tomcat 控制台不会出现在输出终端上。 -p：表示宿主机与容器的端口映射，此时将容器内部的 8080 端口映射为宿主机的 58080 端口，这样就向外界暴露了 58080 端口，可通过 Docker 网桥来访问容器内部的 8080 端口了。 –name：表示容器名称，用一个有意义的名称命名即可。 9，浏览器查看在浏览器中，输入以下地址，即可访问 Tomcat 首页：http://192.168.1.124:58080/ 10，镜像打包打包后就可以移植到其他主机上运行了。打包：1docker save chenbb/fofeasy:0.1 &gt; /mydata/data/fofeasy0.1.tar 11，在另外的主机上导入镜像12docker load &lt; fofeasy0.1.tar #导入镜像docker images #查看存在的镜像 参考 1，迈出使用Docker的第一步，学习第一个Docker容器 2，使用 Docker 搭建 Java Web 运行环境3，使用 Docker 运行 Tomcat ＋ WAR 包 Java 应用4，docker的安装以及jdk和tomcat的环境配置5，运维人员的解放—-Docker快速部署","tags":[{"name":"centos","slug":"centos","permalink":"http://www.kekefund.com/tags/centos/"},{"name":"docker","slug":"docker","permalink":"http://www.kekefund.com/tags/docker/"},{"name":"tomcat","slug":"tomcat","permalink":"http://www.kekefund.com/tags/tomcat/"}]},{"title":"iReport一键生成PDF报告指南","date":"2017-02-15T02:10:06.000Z","path":"2017/02/15/ireport-gen-pdf/","text":"项目中需要生成PDF和Word文件的报告，文件中包含图片和表格。基于Java的解决方案有Freemarker模板引擎，是通过XML文件将填入的内容放上\\${}占位符。这种方式对于简单的文本是没问题，但如果占位符中有字符跟\\${}冲突，就比较难处理了。 iReport是一款可视化报表设计工具，看软件界面跟Qt的风格有几分相似，内置丰富的图表，能够创建复杂的报表。相比XML模板的方式，更加灵活和稳定。 主界面 本文以一个实际的导出PDF报告案例来讲解。 一、ireport可视化布局1，插入图片 从组件面板拖动Image到中间的页面Designer区域。 在右下角的Image属性列表中，设置Image Expression为$F{netImage}。 不过，首先得在Fields定义好变量：netImage，netImage可以在Java工程中传值传过来，也可以是静态图片或url。 Image Expression支持的三种方式： $F{netImage}; “D://net.png” “https://dn-abc.qbox.me/logo.png“ 生成的pdf效果： 2，插入表格为了便于扩展，表格放到Subreport中；在组件面板中新建一个Subreport，进入subreport设计表格； 这里有几个要点： a, 中文字符的显示：需要在属性中设置123Pdf Font: STSong-LightPdf Embedded: 勾选Pdf Encoding: UniGB-UCS2-H(Chinese Simplified) b, 数字的格式化由于Java传过来的值在ireport中默认都是String，在Expression Class中设置为java.lang.Double也没有用。需要在Text Field Expression中将字符串转成Double，然后在格式化，Pattern中设置“###0.0000”，表示保留4为小数。 c, 数字百分比的显示12Text Field Expression: ($F&#123;val&#125;.equals(\"\") ? \"--\" : new Double($F&#123;val&#125;))Pattern：#,##0.00% 如果val为空，则用显示–。按百分比格式化，保留2位小数。效果如下： 3，生成编译文件ireport源文件是jrxml格式，点击下图中的锤子，编译后生成jasper文件。 二、Java项目配置将生成的jasper文件导入到java工程 1，生成图片项目中的方案是从echarts中截图，将截图保存到本地文件夹12345678910111213141516171819202122232425262728293031/*** 调用js* 产生图片*/function genPic(callback)&#123; var data = \"pic=\"+encodeURIComponent(mainChart.getDataURL( &#123; type:\"png\", pixelRatio:1, excludeComponents:['toolbox', 'dataZoom'] &#125;)); var xmlhttp; if (window.XMLHttpRequest) &#123; // code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp = new XMLHttpRequest(); &#125; else &#123; // code for IE6, IE5 xmlhttp = new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125; xmlhttp.open(\"POST\",ctx+\"/productReport/genNetPic?fundId=\"+$('#netfundId').val(),true); xmlhttp.setRequestHeader(\"Content-type\",\"application/x-www-form-urlencoded\"); xmlhttp.onreadystatechange = function() &#123; if (xmlhttp.readyState == 4 &amp;&amp; xmlhttp.status == 200) &#123; // 回调生成结果 callback(true); &#125; else if (xmlhttp.status!=200)&#123; callback(false); &#125; &#125; xmlhttp.send(data); &#125; Controller中的函数：1234567891011121314151617181920212223242526272829@RequestMapping(value = \"genNetPic\", method = RequestMethod.POST)@ResponseBodypublic BaseResponse genNetPic(HttpServletRequest request)&#123; logger.info(\"开始生成图片\"); String pic = request.getParameter(\"pic\"); String fundId = request.getParameter(\"fundId\"); BaseResponse baseResponse = null; try&#123; String[] url = pic.split(\",\"); String u = url[1]; byte[] b = new BASE64Decoder().decodeBuffer(u); // 创建图片生成的路径:日期+产品id Date curDate = new Date(); StringBuilder filePath = new StringBuilder(\"\"); filePath.append(upload_path).append(DateUtil.formatDate(curDate, \"yyyy-MM-dd\")).append(\"/\").append(fundId); FileUtil.createDictionary(filePath.toString()); // 创建图片文件 OutputStream out = new FileOutputStream(new File(filePath.toString()+\"/net.png\")); out.write(b); out.flush(); out.close(); baseResponse = new BaseResponse(true); logger.info(\"完成生成图片\"); &#125; catch(Exception e)&#123; logger.error(\"生成图片失败，失败原因:&#123;&#125;\",e.getMessage()); baseResponse = new BaseResponse(false, \"图片生成失败\"); &#125; return baseResponse;&#125; 2，构建报告参数12345678910111213141516/*** 构建报表的参数* @return*/private FundReportDto getReportParams(String fundId, String fundName, Date curDate)&#123; String dateStr = DateUtil.formatDate(curDate, \"yyyy-MM-dd\"); // FundReportDto fundReportDto = new FundReportDto(); // 得到图片路径 StringBuilder netImage = new StringBuilder(jasper_chart_path); netImage.append(dateStr).append(\"/\").append(fundId).append(\"/\").append(\"net.png\"); fundReportDto.setNetImage(netImage.toString()); ... return fundReportDto;&#125; 返回参数用对象FundReportDto封装，FundReportDto是可序列化的对象：123456789101112131415161718192021import java.io.Serializable;import java.util.ArrayList;import java.util.List;public class FundReportDto implements Serializable&#123; /** * */ private static final long serialVersionUID = -3023588547102105811L; /** * 统计日期 */ private String statisticDate; /* * 图片 */ private String netImage; ... &#125; 3，生成报告报告支持pdf和word两种格式，生成步骤是先将报告保存在本地文件夹，然后调用浏览器下载。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/*** 一建生成报告*/import net.sf.jasperreports.engine.JRExporterParameter;import net.sf.jasperreports.engine.JasperExportManager;import net.sf.jasperreports.engine.JasperFillManager;import net.sf.jasperreports.engine.JasperPrint;import net.sf.jasperreports.engine.JasperReport;import net.sf.jasperreports.engine.data.JRBeanCollectionDataSource;import net.sf.jasperreports.engine.export.JRRtfExporter;import net.sf.jasperreports.engine.util.JRLoader;@RequestMapping(value = \"exportReport\", method = RequestMethod.GET)public void exportReport(String fundId, String fundName, String fileFormat, HttpServletResponse response)&#123; Date curDate = new Date(); String dateStr = DateUtil.formatDate(curDate, \"yyyy-MM-dd\"); // 文件路径 StringBuilder filePath = new StringBuilder(\"\"); filePath.append(upload_path).append(dateStr).append(\"/\").append(fundId); FileUtil.createDictionary(filePath.toString()); // 文件名 StringBuilder fileName = new StringBuilder(\"\"); fileName.append(fundName).append(\"评价报告_\").append(dateStr).append(\".\").append(fileFormat); // 文件全路径 StringBuilder fullName = new StringBuilder(\"\"); fullName.append(filePath).append(\"/\").append(fileName); // 加载报表对象 InputStream is = null; JasperReport jasperReport = null; JasperPrint jasperPrint = null; is = ProductDetailController.class.getClassLoader().getResourceAsStream(analysis_jasper_path); try&#123; // 响应头部信息设置 response.setContentType(\"text/plain\"); response.setHeader(\"Content-Disposition\", \"attachment; filename=\"+FileUtil.getAttachName(fileName.toString())); // jasperReport = (JasperReport) JRLoader.loadObject(is); Map&lt;String,Object&gt; params = new HashMap&lt;String,Object&gt;(); params.put(\"SUBREPORT_DIR\", getClassPath(subreport_dir)); params.put(\"fundName\", fundName); params.put(\"reportTitle\", fundName+\"报告\"); // List&lt;FundReportDto&gt; fundReportDtos = new ArrayList&lt;FundReportDto&gt;(); FundReportDto fundReportDto = getReportParams(fundId, fundName,curDate); fundReportDtos.add(fundReportDto); params.put(\"statisticDate\", fundReportDto.getStatisticDate()); // jasperPrint = JasperFillManager.fillReport(jasperReport, params, new JRBeanCollectionDataSource(fundReportDtos)); if (fileFormat.equals(\"pdf\")) &#123; JasperExportManager.exportReportToPdfFile(jasperPrint, fullName.toString()); &#125;else&#123; JRRtfExporter docReport = new JRRtfExporter(); docReport.setParameter(JRExporterParameter.OUTPUT_FILE_NAME,fullName.toString()); docReport.setParameter(JRExporterParameter.JASPER_PRINT, jasperPrint); docReport.exportReport(); &#125; FileUtil.download(fullName.toString(), response.getOutputStream()); &#125; catch(Exception e)&#123; e.printStackTrace(); logger.error(\"生成报告失败，失败原因:&#123;&#125;\",e.getMessage()); &#125; finally&#123; try&#123; if (is!=null) is.close(); &#125;catch(Exception e)&#123; logger.error(\"关闭文件输入流异常:&#123;&#125;\",e.getMessage()); &#125; &#125; &#125; 其中analysis_jasper_path为jasper文件的路径。这样，一份完整的pdf或word报告就制作出来了。 相关资料 ireport5.6 windows安装包 ireport用户手册 参考: http://blog.csdn.net/jackfrued/article/details/39449021http://blog.csdn.net/wlwlwlwl015/article/details/51312853http://zxs19861202.iteye.com/blog/1171118","tags":[{"name":"ireport","slug":"ireport","permalink":"http://www.kekefund.com/tags/ireport/"},{"name":"pdf","slug":"pdf","permalink":"http://www.kekefund.com/tags/pdf/"},{"name":"报告","slug":"报告","permalink":"http://www.kekefund.com/tags/报告/"}]},{"title":"自己动手，升级iMac——USB3.0 SSD固态硬盘解决方案","date":"2017-02-13T15:23:59.000Z","path":"2017/02/13/imac-ssd-outer-solution/","text":"家里的iMac是2013年买的，曾记得当年是自己开发的一款android程序在中国移动APP大赛上取得前十，这台imac就是用这笔奖金买的。小小的回忆下，虽然这款app已经没有接着开发了，想当初也是投入了满腔的热血加入移动开发的大营^_^。 如今iMac升级了好几代，但硬盘居然还是传统的机械硬盘，当然也有FusionDrive和SSD，但需要加钱。最新的imac除了显示器升级到视网膜屏外，其它的外观基本没什么变化，自家的老款imac仍然可以来装装*，除了运行速度越来越慢~~~ 开机启动要个3-5分钟，打开chrome，docker上的chrome图标要跳个30s左右才能完全把chrome打开，体验速度已经到了无法忍受的地步了！用DiskSpeedTest测试硬盘的读写速度： 5400转的SATA理论读写速度是90M/s，由于已经使用了3年多，实际读写只有理论的一半了。在网上看过不少将内置机械硬盘更换为SSD硬盘的帖子，感觉难度挺大，而且容易毁掉屏幕，还可能导致风扇异常，不愿冒这个险。参考帖子: http://blog.devtang.com/2014/01/26/add-ssd-to-old-imac/ 于是寻找外置硬盘的解决方案：apple的雷电接口速度比较快，可惜市面上很少有雷电口的SSD硬盘，幸好imac的USB接口是3.0的，USB3.0的最大传输带宽高达5.0Gbps（500MB/s），与SSD硬盘相比慢了一点，但速度比机械硬盘那快了不止一点半点。 参考iMac 2015 5K 版外接 USB3.0 硬盘盒+SSD 系统加速体验，从马云家采购来硬盘盒和SSD硬盘: Toshiba/东芝 OCZ饥饿鲨TR150 240G SSD固态硬盘 世特力裸族CSS25U3BK6G移动硬盘盒SSD固态硬盘盒 USB3.0 SATA3 6G 519+135，654块搞定！ 移动硬盘+硬盘盒接到iMac后面是这个样子的： 用DiskSpeedTest测试，读写速度分别能达到264M/s和128M/s，比iMac机械硬盘的速度提升了5倍！ 当然，还是不能跟内置的SSD硬盘相比，下面是macbook pro的硬盘测试结果： 因为imac的速度太慢，在家闲置了好久，换了这个SSD外置硬盘，速度又飞起来了，现在又可以启用了！","tags":[{"name":"iMac","slug":"iMac","permalink":"http://www.kekefund.com/tags/iMac/"},{"name":"SSD","slug":"SSD","permalink":"http://www.kekefund.com/tags/SSD/"},{"name":"USB3.0","slug":"USB3-0","permalink":"http://www.kekefund.com/tags/USB3-0/"},{"name":"移动硬盘盒","slug":"移动硬盘盒","permalink":"http://www.kekefund.com/tags/移动硬盘盒/"}]},{"title":"ireport在centos7下生成pdf缺失字体问题解决","date":"2017-02-06T07:55:11.000Z","path":"2017/02/06/ireport-centos-deploy-bug/","text":"在mac和window测试环境下调试均无问题，但部署到centos下生成报告时报如下错误：1211:07:19.093 ERROR c.d.f.report.controller.ProductDetailController - 生成报告失败，失败原因:Font 'STZhongsong' is not available to the JVM. See the Javadoc for more details. 按照错误提示，把STZHONGS.ttf字体复制到centos的fonts中，按照如何给CentOS安装字体库中的方法，12345mkfontscalemkfontdirfc-cache -fv 执行完成后，还是不行。 通过fc-list命令查看，发现 12345.../usr/share/fonts/TTF/STZHONGS.ttf: 华文中宋,STZhongsong:style=Regular... 系统中安装的是Regular样式的字体，而pdf用到的是STSong-Light字体。 来个暴力的方式，看看能不能解决。将window的fonts文件夹下的所有字体拷贝到centos的fonts下。再执行12345mkfontscalemkfontdirfc-cache -fv 重启tomcat，一键生成报告，pdf正常了！pdf截图：","tags":[{"name":"ireport","slug":"ireport","permalink":"http://www.kekefund.com/tags/ireport/"},{"name":"centos7","slug":"centos7","permalink":"http://www.kekefund.com/tags/centos7/"},{"name":"字体","slug":"字体","permalink":"http://www.kekefund.com/tags/字体/"}]},{"title":"mac版 eclipse 只能run不能debug的解决方案","date":"2017-01-10T03:07:36.000Z","path":"2017/01/10/mac-eclipse-debug/","text":"mac版 eclipse 只能run不能debug的解决方案mac：macOS Sierra 10.12.2eclipse：Version: Neon.1a Release (4.6.1) debug时进度一直停在93%，然后超时报错： 1234ERROR: transport error 202: gethostbyname: unknown hostERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510)JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [debugInit.c:750]FATAL ERROR in native method: JDWP No transports initialized, jvmtiError=AGENT_ERROR_TRANSPORT_INIT(197) 分析错误提示，是找不到主机host，google一下，在stackoverflow找到了解决方案，在hosts中加入1127.0.0.1 localhost -hosts修改方法：hosts不能直接在/etc中修改；在Finder中，点击shift+command+G，输入/etc，将hosts文件拷贝到桌面，修改后再拷贝回去（需要输入密码）。 再次debug，正常！ 参考: http://stackoverflow.com/questions/29188789/eclipse-mac-os-x-debug-error-fatal-error-in-native-method-jdwp-no-transports","tags":[{"name":"mac","slug":"mac","permalink":"http://www.kekefund.com/tags/mac/"},{"name":"eclipse","slug":"eclipse","permalink":"http://www.kekefund.com/tags/eclipse/"},{"name":"debug","slug":"debug","permalink":"http://www.kekefund.com/tags/debug/"}]},{"title":"python制作分布图","date":"2016-11-02T07:44:43.000Z","path":"2016/11/02/distribution-map/","text":"制作分布图类似密度图，在python中利用pandas来提取分布数据是比较方便的。主要用到pandas的cut和groupby等函数。 第一步，从数据库中提取数据1234567891011121314151617import pandasfrom sqlalchemy import create_enginehost_mysql_test = '127.0.0.1'port_mysql_test = 3306user_mysql_test = 'admin'pwd_mysql_test = '1234'db_name_mysql_test = 'mydb'engine_hq = create_engine('mysql+mysqldb://%s:%s@%s:%d/%s' % (user_mysql_test, pwd_mysql_test, host_mysql_test, port_mysql_test, 'hq_db'), connect_args=&#123;'charset': 'utf8'&#125;)sql = \"SELECT * FROM fund_data where quarter&gt;=8 order by yanzhi desc\"df = pd.read_sql(sql, engine)#将yanzhi数据转换为百分比df['yanzhi'] = df['yanzhi'].apply(lambda x: x * 100) 第二步，面元划分 cut函数：1pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False) 官方文档链接 主要参数为x和bins。x为数据源，数组格式的都支持，list，numpy.narray, pandas.Series。bins可以为int，也可以为序列。12bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]cats = pd.cut(df['yanzhi'], bins) 我们定义bins为一个序列，默认的为左开右闭的区间：12345678910111213141516In[]:print catsOut[]:0 (90, 100]1 (90, 100]2 (90, 100]3 (80, 90]4 (80, 90] ... 970 (10, 20]971 (10, 20]972 (10, 20]973 (10, 20]974 (10, 20]Name: yanzhi, dtype: categoryCategories (10, object): [(0, 10], (10, 20], (20, 30], (30, 40], ..., (60, 70], (70, 80], (80, 90] , (90, 100]] 第三步，groupby对言值列按cats做groupby，然后调用get_stats统计函数，再用unstack函数将层次化的行索引“展开”为列。1234567891011121314151617181920def get_stats(group): return &#123;'count': group.count()&#125;grouped = df['yanzhi'].groupby(cats)bin_counts = grouped.apply(get_stats).unstack()print bin_counts countyanzhi (0, 10] 0(10, 20] 5(20, 30] 22(30, 40] 92(40, 50] 258(50, 60] 357(60, 70] 178(70, 80] 51(80, 90] 9(90, 100] 3 第四步，重命名索引，pandas绘图1234bin_counts.index = ['0~10', '10~20', '20~30', '30~40', '40~50', '50~60', '60~70', '70~80', '80~90', '90~100']bin_counts.index.name = 'yanzhi'bin_counts.plot(kind='bar', alpha=0.5, rot=0) 扩展：其它工具绘制一，用G2绘制G2在之前的文章中有介绍，文章《python结合G2绘制精美图形》。 1，生成json数据123456789datas = []for ix, row in bin_counts.iterrows(): # if row['机构数量'] &gt; 0: sss = &#123;'name': ix, 'count': row['count']&#125; datas.append(sss)encodejson = json.dumps(datas, ensure_ascii=False)f = open('yanzhi.json', 'w')f.write(encodejson)f.close() 2，配置html文件123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;分布图&lt;/title&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://as.alipayobjects.com/g/datavis/g2-static/0.0.8/doc.css\" /&gt; &lt;!--如果不需要jquery ajax 则可以不引入--&gt; &lt;script src=\"https://a.alipayobjects.com/jquery/jquery/1.11.1/jquery.js\"&gt;&lt;/script&gt; &lt;script src=\"https://a.alipayobjects.com/alipay-request/3.0.3/index.js\"&gt;&lt;/script&gt; &lt;!-- 引入 G2 脚本 --&gt; &lt;script src=\"https://as.alipayobjects.com/g/datavis/g2/1.2.2/index.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=\"c1\"&gt;&lt;/div&gt; &lt;!-- G2 code start --&gt; &lt;script&gt; $.getJSON('yanzhi.json', function(data) &#123; var Frame = G2.Frame; var frame = new Frame(data); frame = Frame.combinColumns(frame, ['count'],'count','type',['name', 'count']); var chart = new G2.Chart(&#123; id: 'c1', width: 600, height: 400 &#125;); chart.source(frame, &#123; 'count': &#123;alias: '数量', min: 0&#125;, 'name': &#123;alias: '言值分布', min: 0&#125; &#125;); // 去除 X 轴标题// chart.axis('name', &#123;// title: null// &#125;); chart.legend(false);// 不显示图例 chart.intervalStack().position('name*count').color('type', ['#348cd1', '#43b5d8']); // 绘制层叠柱状图 chart.line().position('name*count').color('#5ed470').size(2).shape('smooth'); // 绘制曲线图 chart.point().position('name*count').color('#5ed470'); // 绘制点图 chart.render(); &#125;); &lt;/script&gt; &lt;!-- G2 code end --&gt; &lt;/body&gt;&lt;/html&gt; 3，显示结果 二、DataFrame密度图一句话绘制出来，但具体的区间段难以区分出来。1df[\"yanzhi\"].hist(bins=20, alpha=0.5) 三、bokeh绘图bokeh是python的一个优秀的绘图工具包，与pandas结合的比较好。bokeh文档 12345from bokeh.charts import Histogram, output_file,showhist=Histogram(df, values='yanzhi',bins=30, title='分布图', legend='top_right')output_file('hist.html', title='hist example')show(hist)","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"pandas","slug":"pandas","permalink":"http://www.kekefund.com/tags/pandas/"},{"name":"groupby","slug":"groupby","permalink":"http://www.kekefund.com/tags/groupby/"},{"name":"g2","slug":"g2","permalink":"http://www.kekefund.com/tags/g2/"},{"name":"bokeh","slug":"bokeh","permalink":"http://www.kekefund.com/tags/bokeh/"},{"name":"绘图","slug":"绘图","permalink":"http://www.kekefund.com/tags/绘图/"}]},{"title":"Scrapyd部署","date":"2016-10-10T03:10:30.000Z","path":"2016/10/10/scrapyd-deploy/","text":"scrapy爬虫写好后，需要用命令行运行，如果能在网页上操作就比较方便。scrapyd部署就是为了解决这个问题，能够在网页端查看正在执行的任务，也能新建爬虫任务，和终止爬虫任务，功能比较强大。 一、安装1，安装scrapyd1pip install scrapyd 2， 安装 scrapyd-deploy 1pip install scrapyd-client windows系统，在c:\\python27\\Scripts下生成的是scrapyd-deploy，无法直接在命令行里运行scrapd-deploy。解决办法：在c:\\python27\\Scripts下新建一个scrapyd-deploy.bat，文件内容如下：12@echo offC:\\Python27\\python C:\\Python27\\Scripts\\scrapyd-deploy %* 添加环境变量：C:\\Python27\\Scripts; 二、使用1，运行scrapyd首先切换命令行路径到Scrapy项目的根目录下，要执行以下的命令，需要先在命令行里执行scrapyd，将scrapyd运行起来 12345678910MacBook-Pro:~ usera$ scrapyd/usr/local/bin/scrapyd:5: UserWarning: Module _markerlib was already imported from /Library/Python/2.7/site-packages/distribute-0.6.49-py2.7.egg/_markerlib/__init__.pyc, but /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python is being added to sys.path from pkg_resources import load_entry_point2016-09-24 16:00:21+0800 [-] Log opened.2016-09-24 16:00:21+0800 [-] twistd 15.5.0 (/usr/bin/python 2.7.10) starting up.2016-09-24 16:00:21+0800 [-] reactor class: twisted.internet.selectreactor.SelectReactor.2016-09-24 16:00:21+0800 [-] Site starting on 68002016-09-24 16:00:21+0800 [-] Starting factory &lt;twisted.web.server.Site instance at 0x102a21518&gt;2016-09-24 16:00:21+0800 [Launcher] Scrapyd 1.1.0 started: max_proc=16, runner='scrapyd.runner' 2，发布工程到scrapyda，配置scrapy.cfg在scrapy.cfg中，取消#url = http://localhost:6800/前面的“#”，具体如下：,然后在命令行中切换命令至scrapy工程根目录，运行命令：1scrapyd-deploy &lt;target&gt; -p &lt;project&gt; 示例：1scrapd-deploy -p MySpider 验证是否发布成功 1234scrapyd-deploy -loutput:TS http://localhost:6800/ 一，开始使用1，先启动 scrapyd，在命令行中执行：1MyMacBook-Pro:MySpiderProject user$ scrapyd 2，创建爬虫任务1curl http://localhost:6800/schedule.json -d project=myproject -d spider=spider2 bug：scrapyd deploy shows 0 spiders by scrapyd-clientscrapy中有的spider不出现，显示只有0个spiders。 解决需要注释掉settings中的1234# LOG_LEVEL = \"ERROR\"# LOG_STDOUT = True# LOG_FILE = \"/tmp/spider.log\"# LOG_FORMAT = \"%(asctime)s [%(name)s] %(levelname)s: %(message)s\" When setting LOG_STDOUT=True, scrapyd-deploy will return ‘spiders: 0’. Because the output will be redirected to the file when execute ‘scrapy list’, like this: INFO:stdout:spider-name. Soget_spider_list can not parse it correctly. 3，查看爬虫任务在网页中输入：http://localhost:6800/ 下图为http://localhost:6800/jobs的内容： 4，运行配置配置文件：C:\\Python27\\Lib\\site-packages\\scrapyd-1.1.0-py2.7.egg\\scrapyd\\default_scrapyd.conf 1234567891011121314151617181920212223242526[scrapyd]eggs_dir = eggslogs_dir = logsitems_dir = itemsjobs_to_keep = 50dbs_dir = dbsmax_proc = 0max_proc_per_cpu = 4finished_to_keep = 100poll_interval = 5http_port = 6800debug = offrunner = scrapyd.runnerapplication = scrapyd.app.applicationlauncher = scrapyd.launcher.Launcher[services]schedule.json = scrapyd.webservice.Schedulecancel.json = scrapyd.webservice.Canceladdversion.json = scrapyd.webservice.AddVersionlistprojects.json = scrapyd.webservice.ListProjectslistversions.json = scrapyd.webservice.ListVersionslistspiders.json = scrapyd.webservice.ListSpidersdelproject.json = scrapyd.webservice.DeleteProjectdelversion.json = scrapyd.webservice.DeleteVersionlistjobs.json = scrapyd.webservice.ListJobs 参考 http://www.cnblogs.com/jinhaolin/p/5033733.htmlhttps://scrapyd.readthedocs.io/en/latest/api.html#cancel-json","tags":[{"name":"scrapy","slug":"scrapy","permalink":"http://www.kekefund.com/tags/scrapy/"},{"name":"scrapyd","slug":"scrapyd","permalink":"http://www.kekefund.com/tags/scrapyd/"},{"name":"部署","slug":"部署","permalink":"http://www.kekefund.com/tags/部署/"}]},{"title":"Django创建表单上传图片","date":"2016-08-31T01:38:40.000Z","path":"2016/08/31/django-forms-update/","text":"IOS开发中需要为创建的数据保存到网络后台长久存储，刚开始想到的是直接连接mysql，但要在ios中安装mysql的控件，实在是麻烦。于是定义一个restful接口，通过http请求的方式来上传和获取数据，是一种比较方便的方式。本文是基于Django框架，实现以下几个功能： Model和ModelForm创建表单 POST上传图片 一、建立Model与mysql连接1，定义model12345678910111213141516171819202122232425# models.pyfrom django.db import modelsfrom django.utils.timezone import now# Create your models here.class CureData(models.Model): STATUS_SIZES = ( (0, '进行中'), (1, '已完成'), ) name = models.CharField('名称', max_length=50) cureDuration = models.IntegerField('时长') create_at = models.DateTimeField(\"日期\", default=now()) note = models.CharField('备注', max_length=200, blank=True) image = models.ImageField('图片', upload_to='photos', blank=True) operator = models.CharField('操作者', max_length=50, blank=True) status = models.IntegerField('状态', default=0, choices=STATUS_SIZES) # 0,进行中; 1,已完成 class Meta: ordering = ['create_at'] def __unicode__(self): return self.name 2，配置数据库:在工程的settings.py中设置database 1234567891011121314# settings.py# Database# https://docs.djangoproject.com/en/1.9/ref/settings/#databasesDATABASES = &#123; 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mysite', 'USER': 'user', 'PASSWORD': 'yourpassword', 'HOST': '127.0.0.1', 'PORT': 3306, &#125;&#125; 3，models.py同步到数据库在shell中移到路径到当前工程根目录,执行命令: 12python manage.py makemigrations mysitepython manage.py migrate 二、建立表单1, forms.py12345678from django import formsfrom models import CureDataclass CureDataImageForm(forms.ModelForm): class Meta: model = CureData fields = '__all__' # ['name', 'create_at', ...] 表单中有ImageField，需要在项目的settings.py中添加MEDIA_ROOT路径： 1MEDIA_ROOT = './Data/media/' model中定义的image的参数upload_to=’photos’，上传的图片将保存至./Data/meida/photos/目录下。 2, views.py1234567891011121314from mysite.forms import CureDataImageFormdef update_data(request): if request.method == 'POST': form = CureDataImageForm(request.POST or None, request.FILES or None) if form.is_valid(): image = form.save() print image.image.url return HttpResponseRedirect('/mysite/success/') else: form = CureDataImageForm() return render_to_response('mysite/data_form.html', &#123;'form': form&#125;) 3, urls.py12345678910# mysite/urls.pyfrom django.conf.urls import url, staticfrom . import viewsurlpatterns = [ url(r'^update_data/$', views.update_data), url(r'^success/$', views.success),] 4, html文件views.py中需要的两个html文件放在mysite/templates/mysite/目录下。Form要支持上传图片，需要在form中设置 enctype=”multipart/form-data”，不设置的话文件不支持上传。 data_form.html 12345678910111213141516171819202122&lt;!-- data_form.html --&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;data update&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;data update&lt;/h1&gt; &#123;% if form.errors %&#125; &lt;p style=\"color: red;\"&gt; Please correct the error&#123;&#123; form.errors|pluralize &#125;&#125; below. &lt;/p&gt; &#123;% endif %&#125; &lt;form action=\"\" method=\"post\", enctype=\"multipart/form-data\"&gt; &lt;table&gt; &#123;&#123; form.as_p &#125;&#125; &lt;/table&gt; &lt;input type=\"submit\" value=\"Submit\"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; success.html 1234567891011&lt;!-- success.html --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Success!&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;Success!&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 三、另一种表单创建方式上面创建表单是一种比较简洁的方式，如果不想所有字段保存到数据库，可以用另一种方式： 1，forms.py1234567891011from django import formsfrom django.utils.timezone import nowclass CureDataForm(forms.Form): name = forms.CharField(label=\"名称\") cureDuration = forms.IntegerField(label=\"时长\") create_at = forms.DateTimeField(label=\"创建时间\", initial=now()) note = forms.CharField(label=\"备注\", required=False) image = forms.FileField(label=\"图片\", required=False) operator = forms.CharField(label=\"操作者\") status = forms.IntegerField(label=\"状态\") 2, views.py12345678910111213141516171819202122232425from mysite.forms import CureDataFormdef update_data(request): if request.method == 'POST': form = CureDataForm(request.POST or None, request.FILES or None) if form.is_valid(): cd = form.cleaned_data print cd # img_url = form['image'] # print img_url # 根据用户提交的注册信息在用户信息表中建立一个新的用户对象 cureData = CureData.objects.create( name = form.cleaned_data['name'], cureDuration = form.cleaned_data['cureDuration'], create_at = form.cleaned_data['create_at'], note=form.cleaned_data['note'], image=form.cleaned_data['image'], operator=form.cleaned_data['operator'], status=form.cleaned_data['status'], ) cureData.save() return HttpResponseRedirect('/bbcure/success/') else: form = CureDataForm() return render_to_response('bbcure/data_form.html', &#123;'form': form&#125;) 四、效果shell中执行:1manage.py runserver 0.0.0.0:8000 然后在浏览器中打开http://0.0.0.0:8000/mysite/update_data/ 注：mysite为项目名称 提交后，数据库中如下： 五、总结用Django创建站点很方便，只需很少的代码就能架构出一个功能很复杂的网页。本文只是冰山一角，从表单这一小块阐述Django的快速实现。","tags":[{"name":"Django","slug":"Django","permalink":"http://www.kekefund.com/tags/Django/"},{"name":"表单","slug":"表单","permalink":"http://www.kekefund.com/tags/表单/"},{"name":"图片上传","slug":"图片上传","permalink":"http://www.kekefund.com/tags/图片上传/"}]},{"title":"Scrapy结合Redis实现增量爬取","date":"2016-08-24T07:55:31.000Z","path":"2016/08/24/scrapy-redis-increment/","text":"Scrapy适合做全量爬取，但是，我们不是一次抓取完就完事了。很多情况，我们需要持续的跟进抓取的站点，增量抓取是最需要的。Scrapy与Redis配合，在写入数据库之前，做唯一性过滤，实现增量爬取。 一、官方的去重Pipeline官方文档中有一个去重的过滤器: 12345678910111213from scrapy.exceptions import DropItemclass DuplicatesPipeline(object): def __init__(self): self.ids_seen = set() def process_item(self, item, spider): if item['id'] in self.ids_seen: raise DropItem(\"Duplicate item found: %s\" % item) else: self.ids_seen.add(item['id']) return item 官方的这个过滤器的缺陷是只能确保单次抓取不间断的情况下去重，因为其数据是保存在内存中的，当一个爬虫任务跑完后程序结束，内存就清理掉了。再次运行时就失效了。 二、基于Redis的去重Pipeline为了能够多次爬取时去重，我们考虑用Redis，其快速的键值存取，对管道处理数据不会产生多少延时。 12345678910111213141516171819202122232425#pipelines.pyimport pandas as pdimport redisredis_db = redis.Redis(host=settings.REDIS_HOST, port=6379, db=4, password=settings.REDIS_PWD)redis_data_dict = \"f_uuids\"class DuplicatePipeline(object): \"\"\" 去重(redis) \"\"\" def __init__(self): if redis_db.hlen(redis_data_dict) == 0: sql = \"SELECT uuid FROM f_data\" df = pd.read_sql(sql, engine) for uuid in df['uuid'].get_values(): redis_db.hset(redis_data_dict, uuid, 0) def process_item(self, item, spider): if redis_db.hexists(redis_data_dict, item['uuid']): raise DropItem(\"Duplicate item found:%s\" % item) return item 首先，我们定义一个redis实例: redis_db和redis key：redis_data_dict。 在DuplicatePipeline的初始化函数init()中，对redis的key值做了初始化。当然，这步不是必须的，你可以不用实现。 在process_item函数中，判断redis的hash表中存在该值uuid，则为重复item。至于redis中为什么没有用list而用hash？ 主要是因为速度，hash判断uuid是否存在比list快好几个数据级。特别是uuid的数据达到100w+时，hash的hexists函数速度优势更明显。 最后别忘了在settings.py中加上： 123456# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; 'fund_spider.pipelines.DuplicatePipeline': 200, #'fund_spider.pipelines.MySQLStorePipeline': 300,&#125; 三、总结本文不是真正意义上的增量爬取，而只是在数据存储环节，对数据唯一性作了处理，当然，这样已经满足了大部分的需求。后续我会实现不需要遍历所有的网页，判断抓取到所有最新的item，就停止抓取。敬请关注！","tags":[{"name":"Scrapy","slug":"Scrapy","permalink":"http://www.kekefund.com/tags/Scrapy/"},{"name":"Redis","slug":"Redis","permalink":"http://www.kekefund.com/tags/Redis/"}]},{"title":"【IOS开发】WKWebView封装APP","date":"2016-08-09T03:11:54.000Z","path":"2016/08/09/ios-wkwebview/","text":"一年多没接触xcode了，这一年主要用python做开发，刹一接触xcode代码，还是有点陌生的感觉。在网上闲逛了一通，发现网上的ios教程用swift编写的比oc的多多了。看来苹果的swift推广的比较好。我偶尔写写简单的app，objective-c用过一段时间，这次还是用oc，swift等有时间了好好研究一下。前段时间有朋友让做一个ipad程序，用webview封装一个网站，实现一个独立的app应用。功能虽然简单，实现起来发现ios开发的好些功能都有涉及，丢了一年的ios开发中的概念捡起来不易，于是记录下来，以免后面重复造轮子时又忘了。 主要功能介绍： 自适应iphone和ipad 屏幕翻转自适应拉伸 自定义导航栏返回按钮 网页加载进度条显示 主屏幕左滑后退 以创建app的流程来编写。 一、新建工程1，新建一个Single View Application。依次点击 File -&gt; New -&gt; Project，选择IOS-&gt;Application-&gt;Single View Application。 点击Next，设置程序名称和组织名称和标识，选择开发语言为Objective-C，支持设备为Universal（iphone和ipad都支持），其它默认。 点击Next，Create项目。 2，程序目录结构a, 默认生成的目录结构如下： b，storyboard程序默认创建的Main.storyboard中有一个ViewController，本程序需要导航栏，先拖一个Navigation Controller到storyboard中 删除默认的TableViewController，将Navigation Controller和View Controller关联，拖动首启动箭头从View Controller到Navigation Controller。 二，代码实现浏览器用WKWebView。ViewController头文件如下：1234567891011//ViewController.h#import &lt;UIKit/UIKit.h&gt;@import WebKit;@interface ViewController : UIViewController&lt;WKUIDelegate, WKNavigationDelegate&gt;@property (nonatomic,strong)WKWebView *webView;@end 1，WKUIDelegate委托WKWebView默认只能访问https开头的网站，为了能够支持http开头的普通网站，实现下面的委托方法。 123456789101112#pragma mark - WKUIDelegate//系统阻止了不安全的连接- (WKWebView *)webView:(WKWebView *)webView createWebViewWithConfiguration:(WKWebViewConfiguration *)configuration forNavigationAction:(WKNavigationAction *)navigationAction windowFeatures:(WKWindowFeatures *)windowFeatures&#123; if (!navigationAction.targetFrame.isMainFrame) &#123; [webView loadRequest:navigationAction.request]; &#125; return nil;&#125; 2，WKNavigationDelegate委托didFinishNavigation方法主要作用用来更新导航栏的显示。updateNavigationItems在后续讲解导航栏时介绍。12345#pragma mark - WKNavigationDelegate//页面已全部加载- (void)webView:(WKWebView *)webView didFinishNavigation:(null_unspecified WKNavigation *)navigation &#123; [self updateNavigationItems];&#125; 3，自定义导航栏定义变量12@property(strong, nonatomic) UIBarButtonItem *navigationBackBarButtonItem;@property(strong, nonatomic) UIBarButtonItem *navigationRefresheBarButtonItem; 创建函数如下：12345678910111213141516171819202122232425262728293031323334353637383940414243#pragma mark - 导航按钮//自定义返回按钮- (UIBarButtonItem *)navigationBackBarButtonItem &#123; if (_navigationBackBarButtonItem) return _navigationBackBarButtonItem; UIImage* backItemImage = [[[UINavigationBar appearance] backIndicatorImage] imageWithRenderingMode:UIImageRenderingModeAlwaysTemplate]?:[[UIImage imageNamed:@\"backItemImage\"] imageWithRenderingMode:UIImageRenderingModeAlwaysTemplate]; UIGraphicsBeginImageContextWithOptions(backItemImage.size, NO, backItemImage.scale); CGContextRef context = UIGraphicsGetCurrentContext(); CGContextTranslateCTM(context, 0, backItemImage.size.height); CGContextScaleCTM(context, 1.0, -1.0); CGContextSetBlendMode(context, kCGBlendModeNormal); CGRect rect = CGRectMake(0, 0, backItemImage.size.width, backItemImage.size.height); CGContextClipToMask(context, rect, backItemImage.CGImage); [[self.navigationController.navigationBar.tintColor colorWithAlphaComponent:0.5] setFill]; CGContextFillRect(context, rect); UIImage *newImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); UIImage* backItemHlImage = newImage?:[[UIImage imageNamed:@\"backItemImage-hl\"] imageWithRenderingMode:UIImageRenderingModeAlwaysTemplate]; UIButton* backButton = [UIButton buttonWithType:UIButtonTypeSystem]; NSDictionary *attr = [[UIBarButtonItem appearance] titleTextAttributesForState:UIControlStateNormal]; [backButton setAttributedTitle:[[NSAttributedString alloc] initWithString:@\"返回\" attributes:attr] forState:UIControlStateNormal]; UIOffset offset = [[UIBarButtonItem appearance] backButtonTitlePositionAdjustmentForBarMetrics:UIBarMetricsDefault]; backButton.titleEdgeInsets = UIEdgeInsetsMake(offset.vertical, offset.horizontal, 0, 0); backButton.imageEdgeInsets = UIEdgeInsetsMake(offset.vertical, offset.horizontal, 0, 0); [backButton setImage:backItemImage forState:UIControlStateNormal]; [backButton setImage:backItemHlImage forState:UIControlStateHighlighted]; [backButton sizeToFit]; [backButton addTarget:self action:@selector(toBack) forControlEvents:UIControlEventTouchUpInside]; _navigationBackBarButtonItem = [[UIBarButtonItem alloc] initWithCustomView:backButton]; return _navigationBackBarButtonItem;&#125;//刷新按钮- (UIBarButtonItem *)navigationRefreshBarButtonItem &#123; if (_navigationCloseBarButtonItem) return _navigationCloseBarButtonItem; _navigationCloseBarButtonItem = [[UIBarButtonItem alloc] initWithTitle:@\"刷新\" style:0 target:self action:@selector(navigationIemHandleRefresh:)]; return _navigationCloseBarButtonItem;&#125; 按钮点击响应函数1234567891011- (void)toBack&#123; if ([self.webView canGoBack])&#123; [self.webView goBack]; &#125;&#125;- (void)navigationIemHandleRefresh:(UIBarButtonItem *)sender &#123;// [self.navigationController popViewControllerAnimated:YES];// [self dismissViewControllerAnimated:YES completion:NULL]; [self.webView reload];&#125; 调度函数，由WKNavigationDelegate调起12345678910111213141516- (void)updateNavigationItems &#123; [self.navigationItem setLeftBarButtonItems:nil animated:NO]; if (self.webView.canGoBack) &#123;// Web view can go back means a lot requests exist. UIBarButtonItem *spaceButtonItem = [[UIBarButtonItem alloc] initWithBarButtonSystemItem:UIBarButtonSystemItemFixedSpace target:nil action:nil]; spaceButtonItem.width = -6.5; self.navigationController.interactivePopGestureRecognizer.enabled = NO; if (self.navigationController.viewControllers.count == 1) &#123; [self.navigationItem setLeftBarButtonItems:@[spaceButtonItem, self.navigationBackBarButtonItem, self. navigationRefreshBarButtonItem] animated:NO]; &#125; else &#123; [self.navigationItem setLeftBarButtonItems:@[self. navigationRefreshBarButtonItem] animated:NO]; &#125; &#125; else &#123; self.navigationController.interactivePopGestureRecognizer.enabled = YES; [self.navigationItem setLeftBarButtonItems:nil animated:NO]; &#125;&#125; 4，进度条定义变量123456789101112@property (nonatomic,strong)UIProgressView *progressView;- (UIProgressView *)progressView &#123; if (_progressView) return _progressView; CGFloat progressBarHeight = 2.0f; CGRect navigationBarBounds = self.navigationController.navigationBar.bounds; CGRect barFrame = CGRectMake(0, navigationBarBounds.size.height - progressBarHeight, navigationBarBounds.size.width, progressBarHeight); _progressView = [[UIProgressView alloc] initWithFrame:barFrame]; _progressView.trackTintColor = [UIColor clearColor]; _progressView.autoresizingMask = UIViewAutoresizingFlexibleWidth | UIViewAutoresizingFlexibleTopMargin; return _progressView;&#125; 添加到View12345- (void)viewWillAppear:(BOOL)animated&#123; ... [self.navigationController.navigationBar addSubview:self.progressView]; ...&#125; 5，键值观察（Key-value observing, KVO）KVO允许一个对象观察另一个对象的属性。该属性值改变时，会通知观察对象。与NSNotificationCenter通知相似，多个KVO观察者可以观察单一属性。此外，KVO更动态，因为它允许对象观察任意属性，而不需任何新的API。 在viewDidLoad函数中添加：12[self.webView addObserver:self forKeyPath:@\"estimatedProgress\" options:NSKeyValueObservingOptionNew context:NULL];[self.webView addObserver:self forKeyPath:@\"title\" options:NSKeyValueObservingOptionNew context:NULL]; 定义键值变化的响应代码123456789101112131415161718192021222324252627- (void)observeValueForKeyPath:(NSString *)keyPath ofObject:(id)object change:(NSDictionary&lt;NSString *,id&gt; *)change context:(void *)context&#123; if ([keyPath isEqualToString:@\"estimatedProgress\"])&#123; if (object == self.webView)&#123; [self.progressView setAlpha:1.0f]; [self.progressView setProgress:self.webView.estimatedProgress]; if (self.webView.estimatedProgress &gt;= 1.0f)&#123; [UIView animateWithDuration:0.3 delay:0.3 options:UIViewAnimationOptionCurveEaseIn animations:^&#123; [self.progressView setAlpha:0.0f]; &#125;completion:^(BOOL finished)&#123; [self.progressView setProgress:0.0f]; &#125;]; &#125; &#125; else&#123; [super observeValueForKeyPath:keyPath ofObject:object change:change context:context]; &#125; &#125; else if([keyPath isEqualToString:@\"title\"])&#123; if (object == self.webView)&#123; self.title = self.webView.title; &#125; else&#123; [super observeValueForKeyPath:keyPath ofObject:object change:change context:context]; &#125; &#125;&#125; 别忘了在程序退出时注销键值观察1234- (void)viewWillDisappear:(BOOL)animated&#123; [self.webView removeObserver:self forKeyPath:@\"estimatedProgress\"]; [self.webView removeObserver:self forKeyPath:@\"title\"];&#125; 三，总结程序运行效果如下： IOS开发的流程比较清晰明了，实现界面现在都可以在storyboard中拖拽实现，真正需要代码实现的是一些界面之外的逻辑流程，都可以按模块来各自实现。理解透了，开发起来是比较顺手的。","tags":[{"name":"IOS开发","slug":"IOS开发","permalink":"http://www.kekefund.com/tags/IOS开发/"},{"name":"Objective-C","slug":"Objective-C","permalink":"http://www.kekefund.com/tags/Objective-C/"},{"name":"WKWebView","slug":"WKWebView","permalink":"http://www.kekefund.com/tags/WKWebView/"},{"name":"XCode","slug":"XCode","permalink":"http://www.kekefund.com/tags/XCode/"}]},{"title":"python结合G2绘制精美图形","date":"2016-08-05T10:13:41.000Z","path":"2016/08/05/pandas-and-g2/","text":"一、简介G2是阿里巴巴内部开放的数据可视化工具，提供丰富的图表类型，并且简单易上手，有比较完善的示例代码。其生成的图表简单漂亮，而且有JS互动显示，比较适合报告和文章插图。G2的数据来源是json格式数据。 G2绘制的图形 python的pandas库比较擅长对数据处理和分析，其DataFrame生成json也很方便。pandas自身集成了matplotlib的绘图功能，但是绘制的图形没有G2美观。 pandas 绘制的图形 二、pandas和G2结合绘图绘制流程如下： 1，pandas读取mysql数据库 2，pandas对数据加工处理 3，pandas生成json数据 4，创建含G2内容的html，嵌入json数据 5，调整G2参数，并显示 下面以具体的案例来说明 1，计算收益率排名前十的专家a，读取数据1234567from sqlalchemy import create_engineimport pandas as pdsql = \"select * from strategy order by pct desc\"df = pd.read_sql(sql, engine)df['pct'] = df['pct'] * 100 #收益率转换为百分比 b，生成json数据数据写到top10.json文件中12345678910import jsondatas = []for ix, row in df[:10].iterrows(): sss = &#123;'name': row['name'], 'pct': float(row['pct'])&#125; datas.append(sss)encodejson = json.dumps(datas, ensure_ascii=False)f = open('top10.json', 'w')f.write(encodejson)f.close() c，创建html从http://g2.alipay.com/demo/ 选取一个图表模板创建html文件, 这里选取的是双 Y 轴 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657*** top10.html ***&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;收益率排名TOP10&lt;/title&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://as.alipayobjects.com/g/datavis/g2-static/0.0.8/doc.css\" /&gt; &lt;!--如果不需要jquery ajax 则可以不引入--&gt; &lt;script src=\"https://a.alipayobjects.com/jquery/jquery/1.11.1/jquery.js\"&gt;&lt;/script&gt; &lt;script src=\"https://a.alipayobjects.com/alipay-request/3.0.3/index.js\"&gt;&lt;/script&gt; &lt;!-- 引入 G2 脚本 --&gt; &lt;script src=\"https://as.alipayobjects.com/g/datavis/g2/1.2.2/index.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt;&amp;nbsp; &lt;/div&gt; &lt;div&gt;&amp;nbsp;&lt;/div&gt; &lt;div&gt;&amp;nbsp;&lt;/div&gt; &lt;div id=\"c1\"&gt;&lt;/div&gt; &lt;!-- G2 code start --&gt; &lt;script&gt; $.getJSON('top10.json', function (data) &#123; var Frame = G2.Frame; var frame = new Frame(data); var chart = new G2.Chart(&#123; id: 'c1', width: 500, height: 400 &#125;); chart.source(frame, &#123; 'pct': &#123;alias: '年化相对收益率(%)'&#125;, &#125;); // 去除 X 轴标题 chart.axis('name', &#123; title: null, labels:&#123; 'font-size':'6', 'font-weight': 'bold' //文本粗细 &#125;, &#125;); chart.legend(false);// 不显示图例 //chart.coord('rect').transpose(); chart.interval().position('name*pct').color('name'); // 绘制层叠柱状图 //chart.line().position('name*correct_rate').color('#5ed470').size(2).shape('smooth'); // 绘制曲线图 //chart.point().position('name*correct_rate').color('#5ed470'); // 绘制点图 chart.render(); &#125;) &lt;/script&gt; &lt;!-- G2 code end --&gt; &lt;/body&gt;&lt;/html&gt; top10.html文件和top10.json文件在一个文件夹内。生成的图表如下(可点击交互)： 2，计算推荐次数最多的股票a，读取数据12345from sqlalchemy import create_engineimport pandas as pdsql = \"SELECT code FROM stock \"df = pd.read_sql(sql, engine) b，数据处理不同的分析师对一只股票可能有重复推荐，这就需要统计每只股票出现的次数，然后让总出现次数从高往低排序。用到了自然语言处理包nltk的FreqDist词频统计工具。12345678from nltk import FreqDistcodes = df['code'].get_values()print \"codes \", len(codes)fdist = FreqDist(codes) #生成词频类fdf = pd.DataFrame(fdist.items(), columns=['code', 'count']) #转成DataFramefdf.sort(columns='count', ascending=False, inplace=True) # 排序print \"fdf \", len(fdf) c，生成表格创建html跟一个案例比较相似，这里我们生成markdown格式的表格。定义一个markdown表格创建工具1234567891011121314151617181920212223242526\"\"\"markdown 工具\"\"\"def m_create_table(df): \"\"\" 从pandas的DataFrame生成markdown格式表格 :param df: :return: \"\"\" if len(df) == 0: return '' datas = [] head = '|'.join(df.columns) head = \"|\" + head + \"|\" datas.append(head) datas.append('-|-') for ix, row in df.iterrows(): data = '|'.join(map(lambda x: str(x), row.get_values())) data = \"|\" + data + \"|\" datas.append(data) result = '\\n'.join(datas) # print result return result 调用并打印显示1234567891011121314151617makeTable = m_create_table(fdf)print makeTable#输出|name|code||-|-||隆基股份|601012||美的集团|000333||贵州茅台|600519||华策影视|300133||国轩高科|002074||网宿科技|300017||阳光电源|300274||沧州明珠|002108||老板电器|002508||保利地产|600048| 表格如下： name code 隆基股份 601012 美的集团 000333 贵州茅台 600519 华策影视 300133 国轩高科 002074 网宿科技 300017 阳光电源 300274 沧州明珠 002108 老板电器 002508 保利地产 600048 3，统计饼图对于数据比较少的html，可以直接填入数据就能创建比较精美的图表了。如下，只需修改data的name和value值，就能马上创建一个动态的饼图。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;饼图&lt;/title&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"https://as.alipayobjects.com/g/datavis/g2-static/0.0.12/doc.css\" /&gt; &lt;!--如果不需要jquery ajax 则可以不引入--&gt; &lt;script src=\"https://a.alipayobjects.com/jquery/jquery/1.11.1/jquery.js\"&gt;&lt;/script&gt; &lt;script src=\"https://a.alipayobjects.com/alipay-request/3.0.3/index.js\"&gt;&lt;/script&gt; &lt;!-- 引入 G2 脚本 --&gt;&lt;script src=\"https://as.alipayobjects.com/g/datavis/g2/1.2.6/index.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=\"c1\"&gt;&lt;/div&gt; &lt;!-- G2 code start --&gt; &lt;script&gt; var data = [ &#123;name: '买入', value: 17776 &#125;, &#123;name: '增持', value: 19890&#125;, &#123;name: '中性', value: 6814&#125;, &#123;name: '减持', value: 4986&#125;, &#123;name: '卖出', value: 494&#125;, ]; var Stat = G2.Stat; var chart = new G2.Chart(&#123; id: 'c1', width: 600, height: 400 &#125;); chart.source(data); // 重要：绘制饼图时，必须声明 theta 坐标系 chart.coord('theta', &#123; radius: 0.8 // 设置饼图的大小 &#125;); chart.legend('bottom'); chart.intervalStack() .position(Stat.summary.percent('value')) .color('name') .label('name*..percent',function(name, percent)&#123; percent = (percent * 100).toFixed(2) + '%'; return name + ' ' + percent; &#125;); chart.render(); // 设置默认选中 var geom = chart.getGeoms()[0]; // 获取所有的图形 var items = geom.getData(); // 获取图形对应的数据 geom.setSelected(items[1]); // 设置选中 &lt;/script&gt; &lt;!-- G2 code end --&gt; &lt;/body&gt;&lt;/html&gt;","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"pandas","slug":"pandas","permalink":"http://www.kekefund.com/tags/pandas/"},{"name":"g2","slug":"g2","permalink":"http://www.kekefund.com/tags/g2/"}]},{"title":"跟着分析师炒股系列（二）","date":"2016-08-05T01:10:42.000Z","path":"2016/08/05/follow-analyst-trade2/","text":"在系列第一篇《跟着分析师炒股系列（一）》里，信谁大数据科学团队以分析师研报推荐的股票池，建立了一套股票组合轮动的交易策略，发掘出累积收益最高的一票分析师，其中最牛的招商证券分析师刘荣的累积收益竟达80倍！ 不过，累积收益还可能包含运气成分，比如大牛市下推荐股票都大涨，还不能算真英雄。这一次我们来看看相对收益，看能不能找出穿越牛熊的分析师，尤其是最近一年熊市震荡下他们的表现如何。 延续上一篇的交易策略，筛选分析师评级为买入和增持的股票，形成股票池。调仓周期为3个月。选取分析师评级为“买入、增持”的股票。实现3个月一轮换。具体在每三个月的月末，选取最近发表三篇研报推荐的股票： 若三只股票与前三个月推荐的完全一样，则继续持有； 若三只股票都不一样，则卖掉所有持仓，买入新推荐的三只股票； 若三只股票有一到两只不同，则卖掉收益率最高的持仓股票，买入新推荐的一到两只股票。 一，年化相对收益率排名TOP10我们挑选最近活跃的分析师（在1年内有发表研报），并且其交易周期≥1年。共747名分析师，先看结果： 注：相对收益率对比基准是上证指数 相对收益率看的是真本事，不论牛熊，能真正跑赢大盘的才是真正的王者。其中，招商证券电气设备新能源行业分析师胡毅取得第一名，相对年收益率为125.7%。这与他主攻研究储能与新能源电池产业，也有很大关系，从热门板块中发掘优质个股。 二，最近热门分析师相对收益率眼下，相信大家最关心的就是震荡市选股问题，如何在大势不好的情况下选出黑马股跑赢大盘。来看看2015年6月15日股灾过后，分析师们的表现如何？在这近1年的震荡中有没有战胜大势的分析师？挑选标准： 最近1年研报数量&gt;=10 最后一次发表研报在近6个月 按此标准，共筛选出符合要求的分析师693名。 近一年最高年化相对收益率排名： 注：最大相对收益率越大越好，最大回撤率越低越好。 最近一直是震荡行情，能在震荡行情实现较高的相对收益率，这样的分析师具备较高的选股能力。排名第一的是广发证券巨国贤分析师，年化相对收益率达到来 202.8%！但他在信谁平台上的“言值”并不高，言值为58。这是为什么呢？翻看巨国贤的履历，他从2007年转型进入证券行业，并在13年、14年拿到有色金属行业新财富最佳分析师，还被业界称为“大气晚成型”。言值低，就因为之前的荐股错误较多。对于这样历史表现不佳，而最近崛起的“黑马”分析师，我们可以格外关注。 来看看他的调仓记录：（初始资金设置为100万） 第1次调仓 交易日期 股票代码 操作 成交数量 成交价格 2015-07-30 赤峰黄金(600988) 买入 25500 13.06 2015-07-30 众和股份(002070) 买入 34300 9.70 当前市值：100000， 盈亏比例：0% 第2次调仓 交易日期 股票代码 操作 成交数量 成交价格 2015-10-30 赤峰黄金(600988) 卖出 25500 12.73 2015-10-30 众和股份(002070) 卖出 34300 17.6 2015-10-30 宝钛股份(600456) 买入 18600 17.9 2015-10-30 赣锋锂业(002460) 买入 19200 16.99 2015-10-30 融捷股份(002192) 买入 29100 20.73 当前市值：1262555， 盈亏比例：26..3% 注：002070由于停牌无法卖出，推迟到2015-12-07交易。 第3次调仓 交易日期 股票代码 操作 成交数量 成交价格 2016-01-28 赣锋锂业(002460) 卖出 19200 16.68 2016-01-28 融捷股份(002192) 卖出 16200 24.8 2016-01-28 豫光金铅(600531) 买入 165500 4.36 当前市值：1313868， 盈亏比例：31.4% 第4次调仓 交易日期 股票代码 操作 成交数量 成交价格 2016-04-28 宝钛股份(600456) 卖出 18600 16.3 2016-04-28 融捷股份(002192) 卖出 12900 42.12 2016-04-28 豫光金铅(600531) 卖出 165500 8.41 2016-04-28 正海磁材(300224) 买入 15700 19.29 2016-04-28 云海金属(002182) 买入 90700 15.35 2016-04-28 天齐锂业(002466) 买入 12700 42.69 当前市值：2238983， 盈亏比例：123.9% 第5次调仓 交易日期 股票代码 操作 成交数量 成交价格 2016-07-27 正海磁材(300224) 卖出 15700 20.58 2016-07-27 云海金属(002182) 卖出 90700 22.0 2016-07-27 天齐锂业(002466) 卖出 12700 39.15 当前市值：2817433， 盈亏比例：181.7% 期间大盘跌了19%，其相对年化收益率为202.8%。 高言值分析师与低言值分析师收益率对比： 注：最大相对收益率越大越好，最大回撤率越低越好。 从上面的散点图可以看出，高言值的分析师策略取得正收益的概率比低言值的高，但是收益率越高，风险越大（最大回撤率越大）。 三，分析师神推荐股票2016年分析师推荐股票次数排名TOP10 股票名称 股票代码 推荐次数 涨幅（%） 隆基股份 601012 104 19.3 美的集团 000333 102 47.9 贵州茅台 600519 85 49.1 阳光电源 300274 84 -7.8 国轩高科 002074 83 10.6 华策影视 300133 83 -15.1 网宿科技 300017 81 38.2 沧州明珠 002108 78 51.3 老板电器 002508 76 46.4 宋城演艺 300144 74 -12.0 注：收益率计算截止到7月31日 可见，推荐次数排名前十的股票，有7只上涨，3只下跌；上涨的平均收益为37.5%，下跌的平均亏损为-11.6%。 按最简单的均分法，将资金等额购买这10只股票，就能取得22.8%的收益率！而大盘今年截止目前跌幅是9.61%。 看样子，对于“懒人”投资者，直接跟着这个统计规律买卖的话也能战胜大盘！这背后其实是专业分析的力量，当分析师一致看好的时候，黑马股的概率更大。 四、分析师研报特点对券商分析师研报分析，发现分析师最爱的评级是增持和买入，占到83.65%。远大于评级为“卖出”和“减持”的数量。 你懂的，这就是卖方分析师的命脉，在A股现有投资氛围下唱多更容易受机构和投资者待见。 喜欢唱多不要紧，问题是真的符合评级的有多少，这就是信谁分析师言值评价的作用。 就像业内的传说： 据前辈口口相传，在那遥远的地方，有一群苦逼的策略分析师，日出而作，月落不息，苦练 《葵花宝典》和《屠龙刀法》，希望有朝一日能够在‘新财富’杯华山论剑中一举发威、技惊四座。能入行的，没有笨人。能入门的，没有懒人。想拿奖？唯有能拼命的猛人。","tags":[{"name":"股票","slug":"股票","permalink":"http://www.kekefund.com/tags/股票/"},{"name":"大数据","slug":"大数据","permalink":"http://www.kekefund.com/tags/大数据/"}]},{"title":"跟着分析师炒股系列（一）","date":"2016-07-25T01:26:04.000Z","path":"2016/07/25/follow-analyst-trade/","text":"上一回，我们利用大数据展示了分析师研报预测与个股真实涨跌的对比情况，发现了不少“高言值”分析师，说明专业研究的价值。点击回顾《大数据教你挑选分析师》 那么，结合个人实际持仓操作情况，如果我们就跟着分析师的推荐构建买卖组合，能取得怎样的成绩呢？信谁大数据科学团队这次就以分析师研报推荐的股票池，建立了一套股票组合轮动的交易策略，来看看结果如何。技术男的方法依然简单直接：一名普通分析师每个月发表的研报数量大多数在5篇以上，但选股在精不在多，实际也不可能买入分析师推荐的所有股票。而短线投资对时间精力要求高，仍提倡中长期投资为主。信谁数据科学实验室制定了一个简单有效的交易策略：1) 挑选分析师评级为“买入”的股票;2) 策略的调仓周期定为3个月；3) 具体在每三个月的月末，选取最近发表三篇研报推荐的股票： 若三只股票与前三个月推荐的完全一样，则继续持有； 若三只股票都不一样，则卖掉所有持仓，买入新推荐的三只股票； 若三只股票有一到两只不同，则卖掉收益率最高的持仓股票，买入新推荐的一到两只股票。 注：本交易策略没考虑交易费用。 一、策略结果1，累计收益率排名 排名前十的分析师的交易周期分别为： 分析师 证券公司 累计收益率(%) 交易周期 刘荣 招商证券 8244.3 2005-04-30 ~ 2016-01-28 姚杰 中信证券 2802.7 2005-11-25 ~ 2013-10-28 赵强 光大证券 2419.6 2004-11-24 ~ 2015-08-29 孙亮 中金公司 2044.9 2004-08-31 ~ 2015-12-21 葛军 长江证券 1903.4 2005-06-06 ~ 2015-09-27 陈美风 海通证券 1868.3 2004-08-19 ~ 2015-05-14 龙华 海通证券 1835.7 2006-02-28 ~ 2016-02-17 朱卫华 招商证券 1731.3 2004-03-15 ~ 2013-11-24 丁丹 国信证券 1714.6 2006-01-06 ~ 2015-07-28 丁频 海通证券 1547.8 2004-01-05 ~ 2016-01-26 可以看出，Top10交易周期都在10年左右，累计收益能达到几十倍，说明长期价值投资的回报率非常可观。 最牛的招商证券分析师刘荣的累积收益竟达80倍！如果初始资金10万已变成800万，完全弥补这些年追不上的房价 2，年化收益率排名累积收益率考察每位分析师的分析周期不尽相同，还不能看出谁最厉害。剔除周期因数，再来看看分析师的年化收益率情况。我们挑选发表研报周期超过3年的分析师，得出组合年化收益率结果： 分析师年化收益率TOP10 年化收益率越高，分析师的综合实力越强。 年化收益率的计算公式:$$ 年化收益率 = (累计收益率)^{\\frac{365}{N}} - 1 $$ 其中：N为区间天数 二、最牛操盘手操盘实录排名前十的分析师中，选择交易时间离我们最近的一名：招商证券张同，其年化收益率为116.3%。其策略交易周期自2013年12月-2015年11月，完整经历一轮牛熊变化下来，依然取得了超高年化收益回报。来看看他的调仓记录：（初始资金设置为100万） 第1次调仓 交易日期 股票代码 操作 成交数量 成交价格 2013-12-22 恒康医疗(002219) 买入 6100 107.73 当前市值：100000， 盈亏比例：0% 第2次调仓 交易日期 股票代码 操作 成交数量 成交价格 2014-8-23 恒康医疗(002219) 卖出 3400 153.6 |2014-8-23|西陇科学(002584)|买入|24200|18.18||2014-8-23|利德曼(300289)|买入|13500|31.37| 当前市值：1279807， 盈亏比例：28.0% 第3次调仓 交易日期 股票代码 操作 成交数量 成交价格 2014-11-11 利德曼(300289) 卖出 13500 32.78 2014-11-11 西陇科学(002584) 卖出 24200 23.82 2014-11-11 恒康医疗(002219) 买入 1500 136.53 2014-11-11 浙江震元(000705) 买入 7100 51.72 2014-11-11 瑞康医药(002589) 买入 7400 60.11 当前市值：1462141， 盈亏比例：46.2% 第4次调仓 交易日期 股票代码 操作 成交数量 成交价格 2015-3-31 瑞康医药(002589) 卖出 7400 112.62 2015-3-31 沃森生物(300142) 买入 7000 120.78 当前市值：2209633， 盈亏比例：121.0% 第5次调仓 交易日期 股票代码 操作 成交数量 成交价格 2015-6-29 恒康医疗(002219) 卖出 4200 276.3 2015-6-29 浙江震元(000705) 卖出 7100 87.56 当前市值：4774789， 盈亏比例：377.5% 第6次调仓 交易日期 股票代码 操作 成交数量 成交价格 2015-11-11 沃森生物(300142) 卖出 7000 200.2 当前市值：4547709， 盈亏比例：354.8% 从张同的调仓记录来看，其在2年中操作的股票数量只有6只，精选个股；交易特点是在上涨趋势还未形成时提前布局，逐步加仓买入，坚持持有，长期投资。 可以看出，跟着优秀的分析师炒股，要想实现超高收益，并不是频繁的调仓，而是选中价值股，跟着买入并持有，到约定的时间就卖出。 注：交易策略的股票买卖价格采用的是后复权价格。 三、分析师整体收益率水平1、累计收益率分布 累计收益率大多数集中在0-100%之间，越往高的收益，分析师人数越少。 2、年化收益率分布通过分布图可以看出，分析师的平均收益率大部分集中在0-20%，反映了投资者一般的投资回报率，同样也反映出了分析师的专业研究还是比较有价值的。","tags":[{"name":"股票","slug":"股票","permalink":"http://www.kekefund.com/tags/股票/"},{"name":"大数据","slug":"大数据","permalink":"http://www.kekefund.com/tags/大数据/"}]},{"title":"大数据教你挑选分析师","date":"2016-06-27T09:10:47.000Z","path":"2016/06/27/fenxishi-band/","text":"每个证券公司都有一个分析师团队，不定期发布投资研究报告。分析师的研报准吗？分析师的“言值”究竟有多高？“信谁”的大数据科学团队对这个问题也很好奇。究竟准不准，我们信数据不信故事。技术男的方法非常简单暴力： 把分析师发布的研报预测与相关股票最后涨跌的结果作对比，判断该研报预测的准确性 把分析师发布的所有研报汇总起来，就得到分析预测的正确率，即“言值”，“言值”可以是从0-100分。 汇总所有分析师的数据，就可以得到全部分析师的“言值”分布。 我们从“信谁”平台收集了几乎全部证券公司的3022名分析师、45万条研报数据进行分析。通过大数据挖掘和提取，总结了一些有意思的数据与大家分享。 一、中国分析师分布情况国内的机构分析师基本集中在北京、上海、深圳等地。与国内的财富聚集地区一致。 二、高言值分析师“信谁”数据科学团队在评价分析师“言值”时设定了每位分析师至少发布10篇研报的门槛，这样计算分析师“言值”才有统计意义。经过建模分析，可以看出分析师“言值”呈现一个偏右的正态分布图。这说明优秀的分析师总是占少数，大多数分析师的分析水平都维持在中等偏上水平。 1，神预测——百发百中榜单预测准确度达到100%的分析师有26名，他们发表的文章数量从10~41篇不等。 分析师 机构 文章总数 正确率 应振洲 申银万国 41 100% 郑及游 华融证券 23 100% 邓涛 长江证券 22 100% 孙金霞 光大证券 22 100% 范佳瓅 光大证券 20 100% 丁芸洁 平安证券 19 100% 胡晓华 华泰证券 18 100% 梅剑锋 申银万国 17 100% 王秀钢 海通证券 16 100% 戚海鹏 湘财证券 15 100% 徐颢 湘财证券 12 100% 原瑞政 中信建投 12 100% 朱玮琳 国海证券 12 100% 刘伟杰 山西证券 11 100% 郭湛 东方证券 11 100% 张博 渤海证券 11 100% 易凯 湖南金证投资 11 100% 太平洋证券研究院 太平洋证券 11 100% 张润毅 国泰君安 11 100% 马蘅 中投证券 10 100% 农林牧渔研究小组 德邦证券 10 100% 朱颖 中信证券 10 100% 舒亮 国金证券 10 100% 邱义鹏 长城证券 10 100% 栾钊 湘财证券 10 100% 王赛楠 太平洋证券 10 100% 对于这些“股神”级分析师，我们可以保持高度关注，今后他们说的话是相当“靠谱”的，我们随后也会选择“股神”进行个人专访，满足粉丝们的好奇心。 2，最有言值担当分析师榜单言值担当的分析师不仅要有预测准确度，还要一定的预测文章数量保证，产出量是检验分析师“言值”担当的另外一个标准。 挑选标准: 预测准确度 ≥ 85% 发表文章数 ≥ 30篇 按此标准，正好挑选出108名分析师，这些分析师兼备高言值和高产出，对同期的市场的趋势把握准确。我们挑选排名TOP10的分析师，展示其预测准确度和研报总数。 &gt; 注: 蓝色为研报总数，绿色为准确率。得到这批分析师推荐、特别是多名高“言值”分析师同时推荐的股票，应该会有更好的投资价值！只要关注“信谁”微信公号，就可以免费、定期获得这些股票列表！## 3，超低言值——“百发百不中”榜单有“高富帅”的分析师，同样也有屌丝分析师。数据科学团队找出了这些低言值的分析师，发现预测准确度低到可以和北京小汽车摇号比例媲美，最低的准确度居然不到1%，125篇预测文章，只有1篇正确！对于这样的选手，我们只能呵呵了。但可以反其道而行之，他预测上涨的我们就坚决回避，“信谁”也提供“避雷针”，帮助大家回避这些高风险股票。&gt; 注: 蓝色为研报总数，绿色为准确率。# 三、哪家机构言值最高数据科学团队还将分析师映射到其所在机构，对45万条预测言论做分析，发现预测准确度是以60%-70%为中心的正态分布。一些高产的机构，比如长江证券，16000篇研报，能够达到70%以上的准确度，相比一些发表研报不过百但言值高的小机构，也一样非常不易。## 研报产量TOP10 &amp; 预测准确率TOP10如下： 注: 蓝色为研报总数，绿色为准确率。 四、股票也有言值？——分析师最爱的股票榜单目前A股2862只股票，对于分析师来说，分析一只股票是要花不少时间，将A股所有股票都分析一遍是一个不可能完成的任务，因此每个分析师都会有自己的选股风格，将自己的专业优势集中在少数几只股票上。数据科学团队提取出所有被预测的股票，然后对股票对按出现频率从高到低排序，可以看出分析师最关注的股票类型。 1，股票关注度TOP10： 前十大股票基本都来自主板股票，只有1只中小板股票。说明机构对于主板的股票较为关注，主板股票总数量较多也是分析师们关注度高的另一个原因。 2，分析师关注板块分析通过统计分析师关注股票的所属板块和行业，我们可以发现分析师的整体投资风格。 按关注板块热度划分：注：板块划分的股票统计范围：创业板开板（2009年12月30日）~ 至今。 按行业分类来划分：注：图中数字为该行业的股票预测次数。 从各板块被分析的股票占比来看，分析师们对于创业板和中小板更加热衷，这与近年来创业板、中小板更加活跃也有一定关系。 附，研报计算公式：研报计算公式 投资建议 说明 买入 涨幅相对大盘指数大于15% 增持 涨幅相对大盘指数大于7.5% 持有 震荡幅度相比大盘不超过15% 减持 跌幅超过大盘指数7.5% 卖出 跌幅超过大盘指数15%","tags":[{"name":"股票","slug":"股票","permalink":"http://www.kekefund.com/tags/股票/"},{"name":"大数据","slug":"大数据","permalink":"http://www.kekefund.com/tags/大数据/"}]},{"title":"Groupby使用小例子","date":"2016-06-27T08:51:37.000Z","path":"2016/06/27/example-pandas-groupby/","text":"最近需要根据已有的数据计算这样一组数据： 股票名称 股票代码 推荐人数 平均分数 最大幅度 看到这样的需求，首先想到的是利用pandas的groupby功能。 一、获取数据 1234567891011sql = \"select ID, CODE, NAME, SCORE, Target from table_info\"df = pd.read_sql(sql, engine)df.head()Out[44]: ID CODE NAME SCORE Target0 &#123;00044A0F-3D2A-49E9-B2FD-C42890B10C10&#125; 002285 世联行 0.933333 0.0205881 &#123;001F206E-341A-4613-8AD5-BF9BFD3BB731&#125; 002285 世联行 0.928571 0.1472282 &#123;002C6D88-A011-4E55-93F1-F13CA5B0ACD4&#125; 000058 深 赛 格 0.800000 0.0298383 &#123;0036D69B-32FB-4D07-B23B-93181A573749&#125; 300095 华伍股份 1.000000 0.0717054 &#123;003DEFB1-3664-4F51-B867-A0D31C63A7EE&#125; 002588 史丹利 0.911504 0.000000 二、GroupBy处理1, 获取推荐人数123456789101112grouped = df.groupby(['CODE','NAME'])df_count =grouped['ID'].count()df_count.head()Out[48]: CODE NAME000001 平安银行 17000006 深振业Ａ 2000018 神州长城 3000023 深天地Ａ 1000026 飞亚达Ａ 6Name: ID, dtype: int64 2, 获取平均分数12345678910df_score_mean = grouped['SCORE'].mean()df_score_mean.head()Out[50]: CODE NAME000001 平安银行 0.899111000006 深振业Ａ 0.943299000018 神州长城 0.898596000023 深天地Ａ 0.806186000026 飞亚达Ａ 0.837766Name: SCORE, dtype: float64 3，获取最大幅度123456789df_target_max = grouped['Target'].max()CODE NAME000001 平安银行 0.096940000006 深振业Ａ 0.054782000018 神州长城 0.074779000023 深天地Ａ 0.000000000026 飞亚达Ａ 0.005254Name: Target, dtype: float64 三、数据重组求得了各项的值后，我们需要把这些值重组聚合起来。 利用pandas的concat函数 1234567891011dfs = pd.concat([df_count, df_score_mean, df_target_max], axis=1, join='inner')dfs.head()Out[54]: ID SCORE TargetCODE NAME 000001 平安银行 17 0.899111 0.096940000006 深振业Ａ 2 0.943299 0.054782000018 神州长城 3 0.898596 0.074779000023 深天地Ａ 1 0.806186 0.000000000026 飞亚达Ａ 6 0.837766 0.005254 axis=1：作用在列上join=’inner’：采用内连接，取交集 由于code和name是行索引，需要把这两项变成单独的两列: 1234567891011121314151617181920df_index = pd.DataFrame(dfs.index)codesnames = df_index[0].get_values()codes = []names = []for code, name in codesnames: codes.append(code) names.append(name)dfs['code'] = codesdfs['name'] = namesprint dfs.head() ID SCORE Target code nameCODE NAME 000001 平安银行 17 0.899111 0.096940 000001 平安银行000006 深振业Ａ 2 0.943299 0.054782 000006 深振业Ａ000018 神州长城 3 0.898596 0.074779 000018 神州长城000023 深天地Ａ 1 0.806186 0.000000 000023 深天地Ａ000026 飞亚达Ａ 6 0.837766 0.005254 000026 飞亚达Ａ","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"pandas","slug":"pandas","permalink":"http://www.kekefund.com/tags/pandas/"},{"name":"groupby","slug":"groupby","permalink":"http://www.kekefund.com/tags/groupby/"}]},{"title":"pandas GroupBy使用","date":"2016-06-17T10:12:02.000Z","path":"2016/06/17/pandas-groupby/","text":"一、GroupBy技术GroupBy技术是Hadley Wickham（热门R语言包作者）创造的，用于表示分组运算，“split-apply-combine”（拆分 - 应用 - 合并）。 二、分组键分组键可以有多种形式，且类型不必相同 列表或数组，其长度与待分组的轴一样。 表示DataFrame某个列名的值。 字典或Series，给出带分组轴上的值与分组名之间的对应关系。 函数，用于处理轴索引或索引中的各个标签。 三、分组对于数据:123456789101112df = DataFrame(&#123;'key1':['a','a','b','b','a'],'key2':['one','two','one','two','one'],'data1':np.random.randn(5),'data2':np.random.randn(5)&#125;)dfOut[7]: data1 data2 key1 key20 0.585600 0.670239 a one1 1.993652 1.081585 a two2 -0.188506 -1.506499 b one3 1.016836 -0.084283 b two4 -1.428577 -0.464163 a one 1，Series为分组键按key1分组，计算data1的平均值： 123456789101112grouped = df['data1'].groupby(df['key1'])grouped.mean()Out[9]: key1a 0.383558b 0.414165Name: data1, dtype: float64 其索引为key1列中的唯一值。 如果我们一次传入多个数组，就会得到不同的结果。 123456789101112means = df['data1'].groupby([df['key1'], df['key2']]).mean()meansOut[13]: key1 key2a one -0.421489 two 1.993652b one -0.188506 two 1.016836Name: data1, dtype: float64 通过两个键对数组进行分组，得到的Series具有一个层次化索引。 12345678means.unstack()Out[14]: key2 one twokey1 a -0.421489 1.993652b -0.188506 1.016836 2，数组为分组键分组键可以是任何长度适当的数组。 12345678910111213states = np.array(['Ohio', 'Clifornia', 'California', 'Ohio', 'Ohio'])years = np.array([2005, 2005, 2006, 2005, 2006])df['data1'].groupby([states, years]).mean()Out[19]: California 2006 -0.188506Clifornia 2005 1.993652Ohio 2005 0.801218 2006 -1.428577Name: data1, dtype: float64 3，列名为分组键可以是字符串、数字或其他Python对象。 123456789101112131415161718df.groupby('key1').mean()Out[21]: data1 data2key1 a 0.383558 0.429221b 0.414165 -0.795391df.groupby(['key1','key2']).mean()Out[22]: data1 data2key1 key2 a one -0.421489 0.103038 two 1.993652 1.081585b one -0.188506 -1.506499 two 1.016836 -0.084283 只能对为数字的列求平均值 4，字典或Series为分组键对于数据： 1234567891011121314people = DataFrame(np.random.randn(5,5), columns=['a','b','c','d','e'],index=['Joe','Steve','Wes','Jim','Travis'])people.ix[2:3,['b','c']] = np.nanpeopleOut[56]: a b c d eJoe 0.561986 -0.473072 0.195847 -0.939898 -0.031220Steve -1.113971 1.043835 -0.759549 -0.426541 -0.175329Wes 0.933713 NaN NaN 0.134446 0.001153Jim 0.262178 -1.242311 -1.482147 -0.206859 0.928791Travis 0.714951 0.760905 -1.027728 1.124935 0.213150 假设已知列的分组关系，并希望根据分组计算列的总计： 1234567891011121314mapping = &#123;'a':'red', 'b':'red', 'c':'blue','d':'blue','e':'red','f':'orange'&#125;by_col = people.groupby(mapping, axis=1)by_col.count()Out[61]: blue redJoe 2 3Steve 2 3Wes 1 2Jim 2 3Travis 2 3 Series也具有同样的功能，它可以被看做一个固定大小的映射。 123456789101112131415161718192021222324252627map_series = Series(mapping)map_seriesOut[64]: a redb redc blued bluee redf orangedtype: objectpeople.groupby(map_series, axis=1).count()Out[65]: blue redJoe 2 3Steve 2 3Wes 1 2Jim 2 3Travis 2 3 5，函数为分组键任何被当做分组键的函数都会在各个索引值上被调用一次，其返回值就会被用作分组名称。 12345678people.groupby(len).sum()Out[66]: a b c d e3 1.757877 -1.715382 -1.286300 -1.012311 0.8987245 -1.113971 1.043835 -0.759549 -0.426541 -0.1753296 0.714951 0.760905 -1.027728 1.124935 0.213150 将函数跟数组、列表、字典、Series混合使用也不是问题，因为任何东西最终都会被转换成数组： 12345678910key_list = ['one']*3 + ['two']*2people.groupby([len,key_list]).min()Out[71]: a b c d e3 one 0.561986 -0.473072 0.195847 -0.939898 -0.031220 two 0.262178 -1.242311 -1.482147 -0.206859 0.9287915 one -1.113971 1.043835 -0.759549 -0.426541 -0.1753296 two 0.714951 0.760905 -1.027728 1.124935 0.213150 6，根据索引级别分组12345678910111213141516171819202122232425columns = pd.MultiIndex.from_arrays([['US','US', 'US', 'JP','JP'],[1,3,5,1,3]],names=['cty','tenor'])hier_df = DataFrame(np.random.randn(4,5), columns=columns)hier_dfOut[74]: cty US JP tenor 1 3 5 1 30 -1.187599 0.232050 0.977682 2.480848 0.0076771 -0.876277 0.028064 0.504568 0.229450 -0.6509782 2.320974 0.211266 0.649322 0.222107 1.6588053 -0.744610 0.779133 0.919815 0.333189 0.081555hier_df.groupby(level='cty', axis=1).count()Out[75]: cty JP US0 2 31 2 32 2 33 2 3 7，对分组进行迭代GroupBy对象支持迭代，可以产生一组二元元组。 12345678910111213141516for name, group in df.groupby('key1'): print name print group a data1 data2 key1 key20 0.585600 0.670239 a one1 1.993652 1.081585 a two4 -1.428577 -0.464163 a oneb data1 data2 key1 key22 -0.188506 -1.506499 b one3 1.016836 -0.084283 b two 对于多重键的情况，元组的第一个元素将会是由键值组成的元组： 1234567891011121314151617181920for name, group in df.groupby(['key1','key2']): print name print group ('a', 'one') data1 data2 key1 key20 0.585600 0.670239 a one4 -1.428577 -0.464163 a one('a', 'two') data1 data2 key1 key21 1.993652 1.081585 a two('b', 'one') data1 data2 key1 key22 -0.188506 -1.506499 b one('b', 'two') data1 data2 key1 key23 1.016836 -0.084283 b two 还可将数组片段做出一个字典 123456789pieces = dict(list(df.groupby('key1')))pieces['b']Out[43]: data1 data2 key1 key22 -0.188506 -1.506499 b one3 1.016836 -0.084283 b two 8，按列分组groupby默认是在axis=0上进行分组，通过设置也可以在其他任何轴上进行分组。例如，可以根据dtype对列进行分组： 1234567891011121314151617181920212223242526272829df.dtypesOut[47]: data1 float64data2 float64key1 objectkey2 objectdtype: objectgrouped = df.groupby(df.dtypes, axis=1)dict(list(grouped))Out[49]: &#123;dtype('float64'): data1 data2 0 0.585600 0.670239 1 1.993652 1.081585 2 -0.188506 -1.506499 3 1.016836 -0.084283 4 -1.428577 -0.464163, dtype('O'): key1 key2 0 a one 1 a two 2 b one 3 b two 4 a one&#125; 四、数据聚合如果要使用自己的聚合函数，只需将其传入aggreate或agg方法即可： 123456789101112131415df = DataFrame(&#123;'key1':['a','a','b','b','a'],'key2':['one','two','one','two','one'],'data1':np.random.randn(5),'data2':np.random.randn(5)&#125;)grouped = df[['data1','data2']].groupby(df['key1'])def peak_to_peak(arr): return arr.max() - arr.min()grouped.agg(peak_to_peak)Out[82]: data1 data2key1 a 0.650906 1.229130b 0.813135 1.789667 表：经过优化的groupby方法 函数名 说明 count 分组中非NA值得数量 sum 非NA值得和 mean 非NA值得平均值 median 非NA值得算术中位数 std、var 无偏（分母为n-1）标准差和方差 min、max 非NA值得最小值和最大值 prod 非NA值的积 first、last 第一个和最后一个非NA值 1，面向列的多函数应用可以对不同列使用不同的聚合函数，或一次应用多个函数。数据源：tips.csv 123456789101112131415tips = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/master/ch08/tips.csv')tips['tip_pct'] = tips['tip']/tips['total_bill']tips.head()Out[89]: total_bill tip sex smoker day time size tip_pct0 16.99 1.01 Female No Sun Dinner 2 0.0594471 10.34 1.66 Male No Sun Dinner 3 0.1605422 21.01 3.50 Male No Sun Dinner 3 0.1665873 23.68 3.31 Male No Sun Dinner 2 0.1397804 24.59 3.61 Female No Sun Dinner 4 0.146808 根据sex和smoker对tips进行分组： 1234567891011121314grouped = tips.groupby(['sex','smoker'])group_pct = grouped['tip_pct']group_pct.agg('mean')Out[92]: sex smokerFemale No 0.156921 Yes 0.182150Male No 0.160669 Yes 0.152771Name: tip_pct, dtype: float64 如果传入一组函数或函数名，得到一个DataFrame（列名默认为函数名）： 123456789group_pct.agg(['mean','std',peak_to_peak])Out[93]: mean std peak_to_peaksex smoker Female No 0.156921 0.036421 0.195876 Yes 0.182150 0.071595 0.360233Male No 0.160669 0.041849 0.220186 Yes 0.152771 0.090588 0.674707 可以传入由 (name, function)元组组成的列表，指定DataFrame的列名： 123456789group_pct.agg([('foo','mean'),('bar',np.std)])Out[94]: foo barsex smoker Female No 0.156921 0.036421 Yes 0.182150 0.071595Male No 0.160669 0.041849 Yes 0.152771 0.090588 直接传入列名到函数的字典： 12345678910grouped.agg(&#123;'tip':np.max, 'size':'sum'&#125;)Out[95]: tip sizesex smoker Female No 5.2 140 Yes 6.5 74Male No 9.0 263 Yes 10.0 150 1234567891011grouped.agg(&#123;'tip_pct':['min','max','mean','std'], 'size':'sum'&#125;)Out[96]: tip_pct size min max mean std sumsex smoker Female No 0.056797 0.252672 0.156921 0.036421 140 Yes 0.056433 0.416667 0.182150 0.071595 74Male No 0.071804 0.291990 0.160669 0.041849 263 Yes 0.035638 0.710345 0.152771 0.090588 150 2，apply：一般性的“拆分 - 应用 - 合并”定义函数，分组选出最高的5个tip_pct值。 1234567891011121314def top(df, n=5, column='tip_pct'): return df.sort_index(by=column)[-n:]top(tips, n=6)Out[98]: total_bill tip sex smoker day time size tip_pct109 14.31 4.00 Female Yes Sat Dinner 2 0.279525183 23.17 6.50 Male Yes Sun Dinner 4 0.280535232 11.61 3.39 Male No Sat Dinner 2 0.29199067 3.07 1.00 Female Yes Sat Dinner 1 0.325733178 9.60 4.00 Female Yes Sun Dinner 2 0.416667172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 如果对smoker分组并用该函数调用apply，就会得到： 12345678910111213141516tips.groupby('smoker').apply(top)Out[99]: total_bill tip sex smoker day time size tip_pctsmoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 如果传给apply的函数能够接受其他参数或关键字，则可以将这些内容放在函数名的后面一并传入： 12345678tips.groupby('smoker').apply(top, n=1, column='total_bill')Out[101]: total_bill tip sex smoker day time size tip_pctsmoker No 212 48.33 9 Male No Sat Dinner 4 0.186220Yes 170 50.81 10 Male Yes Sat Dinner 3 0.196812 3，用特定于分组的值填充缺失值对于缺失数据的清理工作，有时你会用dropna将其滤除，而有时则可能会希望用一个固定值或由数据集本身所衍生出来的值去填充NA值。这时就得用fillna工具了。 示例：用平均值填充NA值12345678910111213141516171819202122232425262728s = Series(np.random.randn(6))s[::2] = np.nansOut[104]: 0 NaN1 0.5960702 NaN3 0.0110864 NaN5 0.166135dtype: float64s.fillna(s.mean())Out[105]: 0 0.2577641 0.5960702 0.2577643 0.0110864 0.2577645 0.166135dtype: float64 对不同的分组填充不同的值对以下数据： 123456789101112131415161718192021222324252627282930states = ['Ohio', 'New York', 'Vermont', 'Florida', 'Oregon', 'Nevada','California', 'Idaho']group_key = ['East']*4 + ['West']*4data = Series(np.random.randn(8), index = states)data[['Vermont', 'Nevada', 'Idaho']] = np.nandataOut[111]: Ohio 0.900479New York 0.486912Vermont NaNFlorida -0.627281Oregon -0.580583Nevada NaNCalifornia 2.033416Idaho NaNdtype: float64data.groupby(group_key).mean()Out[112]: East 0.253370West 0.726417dtype: float64 用分组平均值取填充NA值 12345678910111213141516fill_mean = lambda g : g.fillna(g.mean())data.groupby(group_key).apply(fill_mean)Out[114]: Ohio 0.900479New York 0.486912Vermont 0.253370Florida -0.627281Oregon -0.580583Nevada 0.726417California 2.033416Idaho 0.726417dtype: float64 此外，也可以在代码中预定义各组的填充值。由于分组具有一个name属性，所以我们可以拿来用一下： 123456789101112131415161718fill_values = &#123;'East':0.5, 'West':-1&#125;fill_func = lambda g : g.fillna(fill_values[g.name])data.groupby(group_key).apply(fill_func)Out[118]: Ohio 0.900479New York 0.486912Vermont 0.500000Florida -0.627281Oregon -0.580583Nevada -1.000000California 2.033416Idaho -1.000000dtype: float64","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"pandas","slug":"pandas","permalink":"http://www.kekefund.com/tags/pandas/"},{"name":"groupby","slug":"groupby","permalink":"http://www.kekefund.com/tags/groupby/"},{"name":"分组","slug":"分组","permalink":"http://www.kekefund.com/tags/分组/"}]},{"title":"gensim文档相似度判断","date":"2016-05-27T09:55:25.000Z","path":"2016/05/27/gensim-similarity/","text":"在文本处理中，比如商品评论挖掘，有时需要了解每个评论分别和商品的描述之间的相似度，以此衡量评论的客观性。 文本相似度计算的需求始于搜索引擎，搜索引擎需要计算“用户查询”和爬下来的众多“网页”之间的相似度，从而把最相似的排在最前，返回给用户。 一、基本概念TF-IDF TF：term frequency，词频 $$ 词频(TF) = 某个词在文章中的出现次数 $$ $$ 词频(TF) = \\frac{某个词在文章中的出现次数}{文章的总次数} $$ $$ 词频(TF) = \\frac{某个词在文章中的出现次数}{该文出现次数最多的词的出现次数} $$ IDF：inverse document frequency，逆文档频率 $$ IDF = log(\\frac{语料库的文档总数}{包含该词的文档数+1}) $$ TF-IDF $$ TF-IDF = 词频(TF) \\times逆文档频率(IDF) $$ 主要思想是：如果某个词或短语在一篇文章中出现的频率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 TF-IDF计算步骤 第一步：把每个网页文本分词，称为词包（bag of words） 第二步：统计网页（文档）总数M 第三步：统计第一个网页次数N，计算第一个网页第一个词在该网页中出现的次数n，再找出该词在所有文档中出现的次数m。 则该词的tf-idf为： $$ \\frac{\\frac{n}{N}}{\\frac{m}{M}} $$ 第四步：重复第三步，计算出一个网页所有词的tf-idf值。 第五步：重复第四步，计算出所有网页每个词的tf-idf值。 SVD，奇异值分解（Singular value decomposition）奇异值分解是一个有着明显的物理意义的一种方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。就像是描述一个人一样，给别人描述说这个人长得浓眉大眼，方脸，络腮胡，而且带个黑框的眼镜，这样寥寥的几个特征，就让别人脑海里面就有一个较为清楚的认识，实际上，人脸上的特征是有着无数种的，之所以能这么描述，是因为人天生就有着非常好的抽取重要特征的能力，让机器学会抽取重要的特征，SVD是一个重要的方法。 LSI，浅层语义索引（Latent Semantic Indexing）潜在语义索引，指的是通过海量文献找出词汇之间的关系。当两个词或一组词大量出现在一个文档中时，这些词就可以被认为是语义相关的。 潜在语义索引是一种用SVD（Singular Value Decomposition）奇异值分解方法获得在文本中术语和概念之间关系的索引和获取方法。该方法的主要依据是在相同文章中的词语一般有类似的含义。该方法可以从一篇文章中提取术语关系，从而建立起主要概念内容。 余弦相似度 (cosine similiarity)$$ cos\\theta=\\frac{a^2+b^2-c^2}{2ab} $$ 二、相似度计算步骤1，处理用户查询 第一步：对用户查询进行分词 第二步： 根据网页库（文档）的数据， 计算用户查询中每个词的tf-idf值。 2，相似度的计算 使用余弦相似度来计算用户查询和每个网页之间的夹角。夹角越小，越相似。 三、gensim介绍Gensim是一个相当专业的主题模型Python工具包。是一个用于主题建模、文档索引以及使用大规模语料数据的相似性检索。相比RAM，它能处理更多的输入数据。作者称它是“根据纯文本进行非监督性建模最健壮、最有效的、最让人放心的软件。” gensim安装： pip install gensim 四、实现步骤1，中文分词以数据库中关于美联储的新闻6000条，为例。 首先对标题和内容进行分词。标题分词的结果如下： Python代码123456789101112131415161718self.df 为pandas 的DataFrame结构#self.df = pd.read_sql(sql, engine)#df.columns: title, content, href, etc..import reregex = re.compile(ur\"[^\\u4e00-\\u9f5aa-zA-Z0-9]\") # 中英文和数字def jieba_cut(self): \"\"\" 对标题和内容分词 :return: \"\"\" import jieba jieba.load_userdict('Data/userdict.txt') # 自己准备用户词典，也可不指定 # 1，去掉标点和特殊字符 # 2，分词 self.df['title_fenci'] = self.df['title'].apply(lambda x : '|'.join(jieba.cut(regex.sub('',x)))) 2，去掉频率为1的词123456789101112131415161718def remove_low_freq_word(self, texts, times=1): \"\"\" 去掉低频词 :param times:出现次数 :return: \"\"\" all_tokens = sum(texts, []) title_token_once = set(word for word in set(all_tokens) if all_tokens.count(word) == times) texts_result = [[word for word in text if word not in title_token_once] for text in texts] return texts_resulttexts = []for ix, row in self.df.iterrows(): texts_cuts = row['title_fenci'].split('|') texts.append(texts_cuts)texts = self.remove_low_freq_word(texts) 3，建立LSI模型通过上一步的texts抽取一个“词袋（bag of words），将文档的token映射为id。 123456dictionary = corpora.Dictionary(texts)print dictionaryprint dictionary.token2idDictionary(179 unique tokens: [u'\\u8868\\u793a', u'', u'\\u53bb\\u5e74', u'\\u4ee5\\u6765', u'\\u800c']...)&#123;u'\\u8868\\u793a': 0, u'': 124, u'\\u53bb\\u5e74': 127, u'\\u4ee5\\u6765': 1, u'\\u800c': 113, u'\\u5219': 121, u'\\u7f57\\u68ee\\u683c\\u4f26': 175, ...&#125; 接下来用字符串表示的文档转换为用id表示的文档向量. 1234corpus = [dictionary.doc2bow(text) for text in texts]print corpus[[(0, 3), (1, 5), (2, 1), (3, 2), (4, 2), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 2), (12, 3), (13, 2), (14, 1), (15, 1), (16, 1), (17, 5), (18, 2), (19, 1), (20, 1), (21, 8), (22, 5), (23, 1), (24, 2), (25, 1), (26, 4), (27, 1), (28, 1), (29, 2), (30, 1), (31, 2), (32, 1), (33, 2), (34, 3), (35, 8), (36, 7), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 5), (43, 4), (44, 3), (45, 5), (46, 9), (47, 1), (48, 2), (49, 1), (50, 2), (51, 4), (52, 2), (53, 3), (54, 2), (55, 2), (56, 2), (57, 1), (58, 1), (59, 1), (60, 3), (61, 6), (62, 3), (63, 2), (64, 3), (65, 1), (66, 2), (67, 1), (68, 2), (69, 1), (70, 1), (71, 1), (72, 1), (73, 5), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 3), (82, 3), (83, 2), (84, 2), (85, 1), (86, 3), (87, 1), (88, 1), (89, 1), (90, 1), (91, 2), (92, 1), (93, 3), (94, 1), (95, 13), (96, 1), (97, 1), (98, 2), (99, 1), (100, 1), (101, 1), (102, 1)], [(0, 1), (3, 2), (6, 1), (7, 1), (8, 4), (9, 2), (12, 2), (13, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (21, 10), (23, 2), (25, 1), (26, 1), (34, 1), (35, 2), (38, 1), (39, 1), (42, 3), (43, 1), (44, 2), (45, 2), (46, 1), (48, 1), (50, 1), (52, 1), (58, 1), (61, 1), (63, 1), (64, 2), (65, 1), (68, 3), (69, 1), (73, 2), (75, 1), (79, 1), (85, 1), (86, 1), (89, 1), (91, 1), (94, 1), (95, 3), (96, 4), (99, 2), (103, 1), (104, 2), (105, 1), (106, 1), (107, 1), (108, 1), (109, 2), (110, 1), (111, 2), (112, 4), (113, 1), (114, 1), (115, 1), (116, 2), (117, 2), (118, 1), (119, 1), (120, 2), (121, 1), (122, 3), (123, 1)], 例如，最后一列的(123,1)表示第二篇文档中id为123的单词出现了1次。 接下来基于这个“训练文集”计算TF-IDF模型： 12tfidf = models.TfidfModel(corpus)corpus_tfidf = tfidf[corpus] 有了tf-idf值的文档向量，接下来开始训练LSI模型： 1234567lsi = models.LsiModel(corpus_tfidf, id2word=self.dictionary, num_topics=10)2016-05-21 22:27:45,601 : INFO : topic #0(2.646): 1.000*\"\" + 0.000*\"柯薛拉柯塔\" + 0.000*\"应该\" + 0.000*\"加息\" + 0.000*\"不\" + 0.000*\"美联储\" + 0.000*\"今年\" + 0.000*\"下降\" + -0.000*\"有\" + 0.000*\"2014\"2016-05-21 22:27:45,601 : INFO : topic #1(1.594): 0.375*\"的\" + 0.266*\"美联储\" + 0.244*\"美国\" + 0.232*\"br\" + 0.200*\"加息\" + 0.191*\"柯薛拉柯塔\" + 0.163*\"在\" + 0.162*\"罗森格伦\" + 0.138*\"应该\" + 0.131*\"是\"2016-05-21 22:27:45,602 : INFO : topic #2(1.055): 0.320*\"柯薛拉柯塔\" + 0.257*\"美联储\" + -0.245*\"埃文斯\" + 0.216*\"罗森格伦\" + -0.214*\"增速\" + 0.213*\"加息\" + -0.205*\"到\" + -0.205*\"了\" + -0.197*\"都\" + -0.188*\"美国\"2016-05-21 22:27:45,602 : INFO : topic #3(0.989): 0.420*\"柯薛拉柯塔\" + -0.355*\"罗森格伦\" + 0.304*\"应该\" + -0.270*\"鉴于\" + -0.270*\"处于\" + -0.245*\"利率\" + 0.233*\"今年\" + 0.222*\"不\" + -0.216*\"目前\" + -0.194*\"很\"2016-05-21 22:27:45,604 : INFO : topic #4(0.908): -0.347*\"到\" + -0.347*\"了\" + -0.346*\"都\" + -0.331*\"增速\" + -0.244*\"美联储\" + -0.203*\"埃文斯\" + -0.176*\"就业\" + -0.158*\"柯薛拉柯塔\" + -0.151*\"市场\" + 0.131*\"月\" lsi最核心的意义是将训练文档向量组成的矩阵SVD分解，并做了一个秩为2的近似SVD分解。有了LSI模型，建立索引: 12index = similarities.MatrixSimilarity(lsi[corpus])2016-05-21 22:32:20,877 : INFO : creating matrix with 4 documents and 4 features 4，计算相似度1234567891011121314# 计算某一篇文档的相似度tl_bow = dictionary.doc2bow(df.ix[10, 'title_fenci'].split('|'))tl_lsi = lsi[tl_bow]# print tl_lsisims = index[tl_lsi]# print simssort_sims = sorted(enumerate(sims), key=lambda item: -item[1]) #print sort_sims[0:10]# Output:[(10, 1.0), (12, 0.74251199), (0, 0.62192106), (1, 0.61248362), (13, 0.45317733), (11, 0.44128361), (8, 0.40342486), (2, 0.0), (3, 0.0), (4, 0.0)] 第10篇的为它自己，相似度为1，完全相似；与第12篇的相似度为0.74等等。 五、进阶计算出文章的相似度，就可以对相似度设定一个阈值，高于阈值的文章算是重复文章。这样就引出了另外一个用途，文章去重！ 本文的应用背景是将多个资讯平台的文章汇总，很有可能出现不同的平台报道同样的内容，这是大概率事件，所以，为了保证内容的唯一性，需要对汇总的文章去重处理。 Python实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206#coding:utf-8\"\"\"文章去重\"\"\"import pandas as pdimport osimport cPicklefrom gensim import corpora, models, similaritiesfrom db_config import engineimport reregex = re.compile(ur\"[^\\u4e00-\\u9f5aa-zA-Z0-9]\")import logginglogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)class NewsCorpus(): def __init__(self, column): self.dictionary = [] self.column = column self.similarity = 0.85 self.num_topics = 10 self.filename = 'Data/pkl/pkl_news' if not os.path.exists(self.filename): sql = 'select id, title, content from article_news order by published_date desc limit 1000' self.df = pd.read_sql(sql, engine) with open(self.filename, 'wb') as f: cPickle.dump(self.df, f) else: with open(self.filename, 'rb') as f: self.df = cPickle.load(f) print '标题去重前:&#123;&#125;'.format(len(self.df)) self.df = self.df.drop_duplicates(['title']) print '标题去重后:&#123;&#125;'.format(len(self.df)) def jieba_cut(self): \"\"\" 对标题和内容分词 :return: \"\"\" if self.column+\"_fenci\" in self.df.columns: return import jieba jieba.load_userdict('userdict.txt') # 自己准备用户词典，也可不指定 # 去掉标点和特殊字符 然后分词 self.df[self.column+'_fenci'] = self.df[self.column].apply(lambda x : '|'.join(jieba.cut(regex.sub('',x))) if x and len(x) else '') with open(self.filename, 'wb') as f: cPickle.dump(self.df, f) def remove_low_freq_word(self, texts, times=1): \"\"\" 去掉低频词 :param times:出现次数 :return: \"\"\" all_tokens = sum(texts, []) title_token_once = set(word for word in set(all_tokens) if all_tokens.count(word) == times) texts_result = [[word for word in text if word not in title_token_once] for text in texts] return texts_result def create_dictionary(self, df): \"\"\" 创建词典 :return: \"\"\" try: texts = [] for ix, row in df.iterrows(): texts_cuts = row[self.column+'_fenci'].split('|') texts.append(texts_cuts) texts = self.remove_low_freq_word(texts) self.dictionary = corpora.Dictionary(texts) print self.dictionary print self.dictionary.token2id corpus = [self.dictionary.doc2bow(text) for text in texts] print corpus tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] # 训练topic数量为10的LSI模型 lsi = models.LsiModel(corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics) print lsi.print_topic(3) # 建立索引 index = similarities.MatrixSimilarity(lsi[corpus]) # 计算相似度 sims_all = [] for ix, row in df.iterrows(): tl_bow = self.dictionary.doc2bow(row[self.column+'_fenci'].split('|')) tl_lsi = lsi[tl_bow] # print tl_lsi sims = index[tl_lsi] # print sims sims_all.append(sims) sort_sims = sorted(enumerate(sims), key=lambda item: -item[1]) print sort_sims[0:10] return sims_all except Exception,e: print 'create_dictionary:', e return [] def get_results(self, df, sims_all): df_sims = pd.DataFrame(sims_all) df_sims[df_sims &lt; self.similarity] = 0 print df_sims indexs_dict = &#123;&#125; df_sims = df_sims[df_sims&gt;0] ixs = [] # 记录已跑过的index for ix, row in df_sims.iterrows(): row0 = row[row &gt; 0] ixs.append(ix) if len(row0) == 0: continue # print row0 index = row0.index.tolist() index = set(index) - set(ixs) if len(index) &gt; 0: indexs_dict[ix] = list(index) file_name = 'Data/&#123;&#125;_&#123;&#125;.xlsx'.format(self.column, df.ix[0, 'date']) print file_name if indexs_dict: with pd.ExcelWriter(file_name) as writer: for key in indexs_dict.keys(): index_list = [] index_list.append(key) index_list.extend(indexs_dict[key]) print df['title'].head() df_ouput = df[df['id'].apply(lambda x : x in index_list)] print df_ouput.head() if len(df_ouput) == 0: continue # dfs_ouput.append(df_ouput) df_ouput.to_excel(writer, sheet_name=str(key)) indexs_all = sum(indexs_dict.values(), []) print indexs_all[:5] indexs_all = list(set(indexs_all)) se_result = df['id'].apply(lambda x : x not in indexs_all) df_result = df[se_result==True] print len(df_result) print df_result['title'].head() return df_result def run(self): self.jieba_cut() # 按周期(天)分隔 df = self.df if len(df) &lt;= 1: continue df.index = pd.Int64Index(range(len(df))) df['id'] = df.index print df['title'].head() print \"len:\", len(df) sims_all = self.create_dictionary(df) if len(sims_all) == 0: continue df_result = self.get_results(df, sims_all) if len(df_result): df_results.append(df_result) df_o = pd.concat(df_results, ignore_index=True) df_o.to_excel('Data/sim_&#123;&#125;.xlsx'.format(self.column)) # df_o.to_csv('Data/sim_&#123;&#125;.csv'.format(self.column))if __name__ == '__main__': news = NewsCorpus('content') news.similarity = 0.95 news.num_topics = 10 news.run() 参考 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 如何计算两个文档的相似度","tags":[{"name":"gensim","slug":"gensim","permalink":"http://www.kekefund.com/tags/gensim/"},{"name":"主题模型","slug":"主题模型","permalink":"http://www.kekefund.com/tags/主题模型/"},{"name":"相似度","slug":"相似度","permalink":"http://www.kekefund.com/tags/相似度/"},{"name":"文档去重","slug":"文档去重","permalink":"http://www.kekefund.com/tags/文档去重/"}]},{"title":"构建推荐系统（二）","date":"2016-05-18T02:43:56.000Z","path":"2016/05/18/recommendation-system2/","text":"基于用户的协同过滤我们将一个用户和其他所有用户进行对比，找到相似的人。这种算法有两个弊端： 扩展性 随着用户数量的增加，其计算量也会增加。这种算法在只有几千个用户的情况下能够工作得很好，但达到一百万个用户时就会出现瓶颈。 稀疏性 大多数推荐系统中，物品的数量要远大于用户的数量，因此用户仅仅对一小部分物品进行了评价，这就造成了数据的稀疏性。比如亚马逊有上百万本书，但用户只评论了很少一部分，于是就很难找到两个相似的用户了。 鉴于以上两个局限性，我们不妨考察一下基于物品的协同过滤算法。 基于物品的协同过滤假设我们有一种算法可以计算出两件物品之间的相似度，比如Phoenix专辑和Manners很相似。如果一个用户给Phoenix打了很高的分数，我们就可以向他推荐Manners了。 需要注意这两种算法的区别： 基于用户的协同过滤是通过计算用户之间的距离找出最相似的用户，并将他评价过的物品推荐给目标用户； 而基于物品的协同过滤则是找出最相似的物品，再结合用户的评价来给出推荐结果。 基于用户的协同过滤又称为内存型协同过滤，因为我们需要将所有的评价数据都保存在内存中来进行推荐。 基于物品的协同过滤也称为基于模型的协同过滤，因为我们不需要保存所有的评价数据，而是通过构建一个物品相似度模型来做推荐。 修正的余弦相似度我们使用余弦相似度来计算两个物品的距离。上一章提过“分数膨胀”现象，因此我们会从用户的评价中减去他所有评价的均值，这就是修正的余弦相似度。 U表示同时评价过物品i和j的用户集合。 计算修正余弦相似度的Python代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# -*- coding: utf-8 -*-from math import sqrtusers3 = &#123;\"David\": &#123;\"Imagine Dragons\": 3, \"Daft Punk\": 5, \"Lorde\": 4, \"Fall Out Boy\": 1&#125;, \"Matt\": &#123;\"Imagine Dragons\": 3, \"Daft Punk\": 4, \"Lorde\": 4, \"Fall Out Boy\": 1&#125;, \"Ben\": &#123;\"Kacey Musgraves\": 4, \"Imagine Dragons\": 3, \"Lorde\": 3, \"Fall Out Boy\": 1&#125;, \"Chris\": &#123;\"Kacey Musgraves\": 4, \"Imagine Dragons\": 4, \"Daft Punk\": 4, \"Lorde\": 3, \"Fall Out Boy\": 1&#125;, \"Tori\": &#123;\"Kacey Musgraves\": 5, \"Imagine Dragons\": 4, \"Daft Punk\": 5, \"Fall Out Boy\": 3&#125;&#125;def computeSimilarity(band1, band2, userRatings): \"\"\" :param band1：物品1 :param band2：物品2 :param userRatings：用户评分dict averages = &#123;&#125; for (key, ratings) in userRatings.items(): averages[key] = float(sum(ratings.values()))/len(ratings.values()) num = 0 # 分子 dem1 = 0 # 分母的第一部分 dem2 = 0 # for (user, ratings) in userRatings.items(): if band1 in ratings and band2 in ratings: avg = averages[user] num += (ratings[band1] - avg) * (ratings[band2] - avg) dem1 += (ratings[band1] - avg)**2 dem2 += (ratings[band2] - avg)**2 return num / (sqrt(dem1) * sqrt(dem2))print computeSimilarity('Kacey Musgraves', 'Lorde', users3)print computeSimilarity('Imagine Dragons', 'Lorde', users3)print computeSimilarity('Daft Punk', 'Lorde', users3)Output:0.320959291341-0.2525265372290.784114958467 相似度矩阵的预测 这个矩阵看起来不错，那下面该如何使用它来做预测呢？比如我想知道David有多喜欢Kecey Musgraves？ p(u,i)表示我们会来预测用户u对物品i的评分，所以p(David, Kacey Musgraves)就表示我们将预测David会给Kacey打多少分。 N是一个物品的集合，有如下特性：用户u对集合中的物品打过分；物品i和集合中的物品有相似度数据（即上文中的矩阵）。 表示物品i和N的相似度， 表示用户u对物品N的评分。 归一化转换为了让公式的计算效果更佳，对物品的评价分值最好介于-1和1之间。由于我们的评分系统是1至5星，所以需要使用一些运算将其转换到-1至1之间。 比如一位用户给Fall Out Boy打了2分，那修正后的评分为： $$ NR_{u,N} = \\frac{2(2-1) - (5-1)}{5-1} = -2/4 = -0.5 $$ 反过来则是： 下表为求得的David的修正评分： 则David对Kacey Musgraves的预测评分为： 转换为5星评价体系为： 最终的预测结果为4.506分。 Slope One算法还有一种比较流行的基于物品的协同过滤算法，名为Slope One，它的最大的优势是简单，因此易于实现。 我们用一个简单的例子来了解这个算法。假设Amy给PSY打了3分，Whitney Houston打了4份；Ben给PSY打了4分。我们要预测Ben会给Whitney Houston打几分。 第一步：计算差值我们先为上述例子增加一些数据： 计算物品之间差异的公式是： 其中，card(S)表示S中有多少个元素；X表示所有评分值的集合； 表示同时评价过物品j和i的用户数。 我们来考察PSY和Taylor Swift之间的差值，因为只有两个用户（Amy和Ben）同时对PSY和Taylor Swift打过分。 $$ dev_{swift, psy} = (4-3)/2 + (5-2)/2 = 2 $$ 所以PSY和Taylor Swift的差异为2。反过来，Taylor Swift和PSY的差异： $$ dev_{psy, swift} = (3-4)/2 + (2-5)/2 = -2 $$ 其他物品之间的差异 0 Taylor Swift PSY Whitney Houston Taylor Swift 0 2 1 PSY -2 0 -0.75 Whitney Houston -1 0.75 0 第二步：使用加权的Slope One算法进行预测公式为： 其中： 表示我们将预测用户u对物品i的评分。比如表示Ben对Whitney Houston的预测评分。 表示遍历Ben评价过的所有歌手，除了Whitney Houston之外。 整个分子的意思是：对于Ben评价过的所有歌手（Whitney Houston除外），找出Whitney Houston和这些歌手之间的差值，并将差值加上Ben对这个歌手的评分。同时，我们要将这个结果乘以同时评价过两位歌手的用户数。 Slope One的Python实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071users2 = &#123;\"Amy\": &#123;\"Taylor Swift\": 4, \"PSY\": 3, \"Whitney Houston\": 4&#125;, \"Ben\": &#123;\"Taylor Swift\": 5, \"PSY\": 2&#125;, \"Clara\": &#123;\"PSY\": 3.5, \"Whitney Houston\": 4&#125;, \"Daisy\": &#123;\"Taylor Swift\": 5, \"Whitney Houston\": 3&#125;&#125;class Recommender(): def __init__(self, data, k=1, metric='pearson', n=5): self.data = data # 以下变量将用于Slope One算法 self.frequencies = &#123;&#125; self.deviations = &#123;&#125; def computeDeviations(self): \"\"\" 计算物品之间的差异 :return: \"\"\" # 获取每位用户的评分数据 for ratings in self.data.values(): # 对于该用户的每个评分项（歌手、分数） for (item, rating) in ratings.items(): self.frequencies.setdefault(item, &#123;&#125;) self.deviations.setdefault(item, &#123;&#125;) # 再次遍历该用户的每个评分项 for (item2, rating2) in ratings.items(): if item != item2: # 将评分的差异保存到变量中 self.frequencies[item].setdefault(item2, 0) self.deviations[item].setdefault(item2, 0.0) self.frequencies[item][item2] += 1 self.deviations[item][item2] += rating - rating2 for (item, ratings) in self.deviations.items(): for item2 in ratings: ratings[item2] /= self.frequencies[item][item2] def slopeOneRecommendations(self, userRatings): \"\"\" 加权的Slope One算法 :param userRatings: :return: \"\"\" recommendations = &#123;&#125; frequencies = &#123;&#125; # 遍历目标用户的评分项（歌手、分数） for (userItem, userRating) in userRatings.items(): # 对目标用户未评价的歌手进行计算 for (diffItem, diffRatings) in self.deviations.items(): if diffItem not in userRatings and userItem in self.deviations[diffItem]: freq = self.frequencies[diffItem][userItem] recommendations.setdefault(diffItem, 0.0) frequencies.setdefault(diffItem, 0) # 分子 recommendations[diffItem] += (diffRatings[userItem] + userRating) * freq # 分母 frequencies[diffItem] += freq recommendations = [(k, v / frequencies[k]) for (k, v) in recommendations.items()] # 排序并返回 recommendations.sort(key=lambda artistTuple: artistTuple[1], reverse=True) return recommendations&gt;&gt;&gt; r = Recommender()&gt;&gt;&gt; r.computeDeviations()&gt;&gt;&gt; r.deviations&#123;'PSY': &#123;'Taylor Swift': -2.0, 'Whitney Houston': -0.75&#125;, 'Taylor Swift': &#123;'PSY': 2.0, 'Whitney Houston': 1.0&#125;, 'Whitney Houston': &#123;'PSY': 0.75, 'Taylor Swift': -1.0&#125;&#125;&gt;&gt; &gt; r.slopeOneRecommendations(users2['Ben']) [('Whitney Houston', 3.375)] 参考 1,《集体智慧编程》 2, https://github.com/egrcc/guidetodatamining/blob/master/chapter-3.md","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"机器学习","slug":"机器学习","permalink":"http://www.kekefund.com/tags/机器学习/"},{"name":"推荐算法","slug":"推荐算法","permalink":"http://www.kekefund.com/tags/推荐算法/"}]},{"title":"构建推荐系统（一）","date":"2016-05-11T01:35:47.000Z","path":"2016/05/11/recommendation-system/","text":"协同型过滤 ( Collaborative filtering)一个协作型过滤算法通常的做法是对一大群人进行搜索，并从中找出与我们品味相近的一小群人。算法会对这些人所偏爱的其他内容进行考查，并将它们组合起来构造出一个经过排名的推荐列表。 一、相似度评价方法0，数据集本文中的数据集都是以嵌套字典的形式出现，如下：字典的key为用户名，value为对各个物品的评价分数。 123456789101112131415161718192021222324252627282930313233users=&#123;'Lisa Rose': &#123;'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, 'The Night Listener': 3.0&#125;,'Gene Seymour': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, 'You, Me and Dupree': 3.5&#125;,'Michael Phillips': &#123;'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, 'Superman Returns': 3.5, 'The Night Listener': 4.0&#125;,'Claudia Puig': &#123;'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, 'The Night Listener': 4.5, 'Superman Returns': 4.0, 'You, Me and Dupree': 2.5&#125;,'Mick LaSalle': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, 'You, Me and Dupree': 2.0&#125;,'Jack Matthews': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5&#125;,'Toby': &#123;'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0&#125;&#125;critics=&#123;'Lisa Rose': &#123;'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, 'The Night Listener': 3.0&#125;,'Gene Seymour': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, 'You, Me and Dupree': 3.5&#125;,'Michael Phillips': &#123;'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0, 'Superman Returns': 3.5, 'The Night Listener': 4.0&#125;,'Claudia Puig': &#123;'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, 'The Night Listener': 4.5, 'Superman Returns': 4.0, 'You, Me and Dupree': 2.5&#125;,'Mick LaSalle': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0, 'You, Me and Dupree': 2.0&#125;,'Jack Matthews': &#123;'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5&#125;,'Toby': &#123;'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0&#125;&#125; 1，曼哈顿距离最简单的距离计算方式是曼哈顿距离。在二维模型中，每个人都可以用(x, y)的点来表示，这里我用下标表示不同的人，(x1,y2)表示艾米，(x2,y2)表示神秘的X先生，那么他们之间的曼哈顿距离就是 Python实现：1234567891011121314def sim_manhattan(rating1, rating2): \"\"\" 计算曼哈顿距离。 :param rating1: 形如&#123;'The Strokes': 3.0, 'Slightly Stoopid': 2.5&#125; :param rating2: 形如&#123;'The Strokes': 3.0, 'Slightly Stoopid': 2.5&#125; \"\"\" distance = 0 for key in rating1: if key in rating2: distance += abs(rating1[key] - rating2[key]) return distance 2，欧几里德距离对于二维模型，计算的是两点之间的直线距离。 对于N维模型，如下表： @ Angelica Bill Blues Traveler 3.5 2 Broken Bells 2 3.5 Deadmau5 - 4 Norah Jones 4.5 - Phoenix 5 2 Slightly Stoopid 1.5 3.5 The Strokes 2.5 - Vampire Weekend 2 3 计算Angelica和Bill的欧几里得距离： $$ Euclidean = \\sqrt{(3.5-2)^2 + (2-3.5)^2+(5-2)^2+(1.5-3.5)^2+(2-3)^2} = 4.3 $$ Python实现：123456789101112131415161718192021222324252627282930from math import sqrtdef sim_euclidean(users, person1, person2): \"\"\" 欧几里得距离 返回一个有关person1与person2的基于距离的相似度评价 :param users:&#123;'Lisa Rose': &#123; 'Snakes on a Plane': 3.5,...&#125;,'Gene Seymour': &#123;'Lady in the Water': 3.0, ...&#125; :param person1: 形如'Lisa Rose' :param person2: 形如'Lisa Rose' :return:介于[0, 1]，值为1表明两个人对每一样物品均有着完全一致的评价 \"\"\" # 得到shared_items的列表 shared_items = &#123;&#125; for item in users[person1]: if item in users[person2]: shared_items[item] = 1 # 如果两者没有共同之处，则返回0 if len(shared_items) == 0: return 0 # 计算所有差值的平方和 sum_of_squares = sum([pow(users[person1][item]-users[person2][item], 2) for item in users[person1] if item in users[person2]]) return 1/(1+sqrt(sum_of_squares)) dis = sim_euclidean(critics, 'Lisa Rose', 'Jack Matthews')print dis 3, 皮尔逊相关度评价用户的问题让我们仔细看看用户对乐队的评分，可以发现每个用户的打分标准非常不同： Bill没有打出极端的分数，都在2至4分之间； Jordyn似乎喜欢所有的乐队，打分都在4至5之间； Hailey是一个有趣的人，他的分数不是1就是4。 那么，如何比较这些用户呢？解决方法之一是使用皮尔逊相关系数。 该计算系数是判断两组数据与某一直线拟合程度的一种度量。对应的公式比欧几里德距离评价的计算公式要复杂，但是它在数据不是很规范的时候（比如影评者对影片的评价总是相对于平均水平偏离很大时），会倾向于给出更好的结果。 我们先看下面的数据： @ Blues Traveler Norah Jones Phoenix Strokes Weird AI Clara 4.75 4.5 5 4.25 4 Robert 4 3 5 2 1 这种现象在数据挖掘领域称为“分数膨胀”。Clara最低给了4分——她所有的打分都在4至5分之间。我们将它绘制成图表： 一条直线——完全吻合！！！ 直线即表示Clara和Robert的偏好完全一致。他们都认为Phoenix是最好的乐队，然后是Blues Traveler、Norah Jones。如果Clara和Robert的意见不一致，那么落在直线上的点就越少。 皮尔逊相关系数的计算公式： Python实现123456789101112131415161718192021222324252627282930313233343536373839404142434445def sim_pearson(users, person1, person2): \"\"\" 返回p1和p2的皮尔逊相关系数 :param users:&#123;'Lisa Rose': &#123; 'Snakes on a Plane': 3.5,...&#125;,'Gene Seymour': &#123;'Lady in the Water': 3.0, ...&#125; :param person1: 形如'Lisa Rose' :param person2: 形如'Lisa Rose' :return: 介于[-1, 1]，值为1表明两个人对每一样物品均有着完全一致的评价 \"\"\" # 得到双方都曾评价过的物品列表 shared_items = &#123;&#125; for item in users[person1]: if item in users[person2]: shared_items[item] = 1 # 得到列表元素的个数 n = len(shared_items) # 如果没有共同之处，则返回1 if n == 0: return 1 # 对所有偏好求和 sum1 = sum([users[person1][item] for item in shared_items]) sum2 = sum([users[person2][item] for item in shared_items]) # 求平方和 sum1_sq = sum([pow(users[person1][item], 2) for item in shared_items]) sum2_sq = sum([pow(users[person2][item], 2) for item in shared_items]) # 求乘积之和 pSum = sum([users[person1][item] * users[person2][item] for item in shared_items]) # 计算皮尔逊评价值 num = pSum - (sum1*sum2/n) den = sqrt((sum1_sq - pow(sum1, 2)/n) * (sum2_sq - pow(sum1, 2)/n)) if den == 0: return 0 r = num/den return rdis = sim_pearson(critics, 'Lisa Rose', 'Jack Matthews')print dis 4，余弦相似度它在文本挖掘中应用得较多，在协同过滤中也会使用到。这里记录了每个用户播放歌曲的次数，我们用这些数据进行推荐。 简单扫一眼上面的数据，我们可以发现Ann和Sally的偏好更为相似。 可以看到，Moonlight Sonata这首歌我播放了25次，但很有可能你一次都没有听过。事实上，上面列出的这些歌曲可能你一手都没听过。此外，iTunes上有1500万首音乐，而我只听过4000首。所以说单个用户的数据是稀疏的，因为非零值较总体要少得多。当我们用1500万首歌曲来比较两个用户时，很有可能他们之间没有任何交集，这样一来就无从计算他们之间的距离了。 余弦相似度的计算中会略过这些非零值。它的计算公式是： $$ cos(x, y) = \\dfrac{x\\cdot y}{||x||\\times||y||} $$ 其中，“·”号表示数量积。“||x||”表示向量x的模，计算公式为: 对于下表： @ Blues Traveler Norah Jones Phoenix Strokes Weird AI Clara 4.75 4.5 5 4.25 4 Robert 4 3 5 2 1 有两个向量： 12x = (4.75, 4.5, 5, 4.25, 4)y = (4, 3, 5, 2, 1) 它们的模是： $$ ||x|| = \\sqrt{4.75^2 + 4.5^2 + 5^2 + 4.25^2 + 4^2} = 10.09 $$ $$ ||y|| = \\sqrt{4^2 + 3^2 + 5^2 + 2^2 + 1^2} = 7.416 $$ 数量积的计算： 因此余弦相似度是：$$ cos(x, y) = \\dfrac{70}{10.093\\times7.416} = 0.935 $$ 5，应该使用哪种相似度？ 如果数据存在“分数膨胀”问题，就使用皮尔逊相关系数 如果数据比较“密集”，变量之间基本都存在公有值，且这些距离数据是非常重要的，那就使用欧几里德或曼哈顿距离。 如果数据是稀疏的，则使用余弦相似度。 二、应用1，推荐用户 —— K最邻近算法使用K最邻近算法来找出K个最相似的用户。 12345678910111213141516171819202122232425def top_matches(users, person, k=5, similarity=sim_pearson): \"\"\" 从反映偏好的字典中返回最为匹配者 返回结果的个数和相似度函数均为可选参数 :param users: :param person: :param k: :param similarity: :return: \"\"\" scores = [(similarity(users, person, other), other) for other in users if other != person] # 对列表进行排序，评价值最高者排载最前面 scores.sort() scores.reverse() return scores[:k]scores = top_matches(critics, 'Toby', k=3)print scoresOutput&gt;&gt;:[(0.9912407071619299, 'Lisa Rose'), (0.9244734516419049, 'Mick LaSalle'), (0.8934051474415647, 'Claudia Puig')] 2，推荐物品通过一个经过加权的评价值来为影片打分，评论者的评分结果因此形成了先后的排名。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def getRecommendations(users, person, similarity=sim_pearson): \"\"\" 利用所有他人评价值的加权平均，为某人提供建议 :param users: :param person: :param similarity: 可指定相似度评价函数 :return: \"\"\" totals = &#123;&#125; simSums = &#123;&#125; for other in users: # 不要和自己做比较 if other == person: continue sim = similarity(users, person, other) # 忽略评价值为零或者小于零的情况 if sim &lt;= 0: continue for item in users[other]: # 只对自己还未曾看过的影片进行评价 if item not in users[person] or users[person][item] == 0: # 相似度 * 评价值 totals.setdefault(item, 0) totals[item] += users[other][item] * sim # 相似度之和 simSums.setdefault(item, 0) simSums[item] += sim # 建立一个归一化的列表 rankings = [(total/simSums[item], item) for item, total in totals.items()] # 返回经过排序的列表 rankings.sort() rankings.reverse() return rankingsrankings = getRecommendations(critics, 'Toby')print 'pearson\\n', rankingsrankings = getRecommendations(critics, 'Toby', similarity=sim_euclidean)print 'euclidean\\n', rankingsOutput:pearson[(3.3477895267131013, 'The Night Listener'), (2.8325499182641614, 'Lady in the Water'), (2.5309807037655645, 'Just My Luck')]euclidean[(3.457128694491423, 'The Night Listener'), (2.778584003814924, 'Lady in the Water'), (2.4224820423619167, 'Just My Luck')]","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"机器学习","slug":"机器学习","permalink":"http://www.kekefund.com/tags/机器学习/"},{"name":"相似度评价","slug":"相似度评价","permalink":"http://www.kekefund.com/tags/相似度评价/"}]},{"title":"基金从业资格考试笔记","date":"2016-05-03T07:51:54.000Z","path":"2016/05/03/fund-qualification-examination/","text":"第一章 金融、资产管理与投资基金1，金融资产一般分为债券类金融资产和股权类金融资产。 债券类金融资产以票据、债券等契约型投资工具为主，股权类金融资产以各类股票为主。 2，投资基金的主要类别主要按照所投资的对象的不同进行区分： 证券投资基金基金所投资的有价证券主要是在证券交易所或银行间市场上公开交易的证券，包括股票、债券、货币、金融衍生工具等。可分为公募证券投资基金和私募证券投资基金等种类。 私募股权基金私募股权基金（private equity，PE）指通过私募形式对私有企业，即非上市企业进行的权益性投资。 风险投资基金风险投资基金（venture capital，VC），又叫创业基金，它以一定的方式吸收机构和个人的资金，投向与那些不具备上市资格的初创期的或者是小型的新型企业，尤其是高新技术企业，帮助所投资的企业尽快成熟，取得上市资格，从而使资本增值。 对冲基金（hedge fund），意为“风险对冲过的基金”，它是基于投资理论和极其复杂的金融市场操作技巧，充分利用各种金融衍生产品的杠杆作用，承担高风险、追求高收益的投资模式。 另类投资基金是指投资于传统的股票、债券之外的金融和实物资产的基金，如房地产、证券化资产、对冲基金、大宗商品、黄金、艺术品等。 第二章 证券投资基金概述1，证券投资基金的特点 集合理财、专业管理 组合投资、分散风险 利益共享、风险共担 严格管理、信息透明 独立托管、保障安全 2, 基金与股票、债券的差异股票反映的是一种所有权关系，是一种所有权凭证，投资者购买股票后就成为公司的股东；债券反映的是债权债务关系，是一种债权凭证，投资者购买债券后就成为公司的债权人；基金反映的则是一种信托关系，是一种受益凭证，投资者购买基金份额就成为基金的受益人。 3，封闭式基金与开放式基金依据运作方式的不同，可以将基金分为封闭式基金与开放式基金。 封闭式基金是指基金份额在基金合同期限内固定不变，基金份额可以在依法设立的证券交易所交易，但基金份额持有人不得申请赎回的一种基金运作方式。 开放式基金是指基金份额不固定，基金份额可以在基金合同约定的时间和场所进行申购或赎回的一种基金运作方式。这里所指的开放式基金专指传统的开放式基金，不包括交易型开放式指数基金（ETF）和上市开放式基金（LOF）等新型开放式基金。 开放式基金与封闭式基金的比较 项目 开放式基金 封闭式基金 规模 不固定 固定 存续期限 不确定，理论上可以无限期存续 确定 交易方式 一般不上市，通过向基金管理公司和代销机构进行申购赎回 上市流通 交易价格 按照每日基金单位资产净值 根据市场行情变化，相对于单位资产净值可能折价或溢价，多为折价 信息披露 每日公布基金单位资产净值，每季度公布资产组合，每6个月公布变更的招募说明书 每周公布基金单位资产净值，每季度公布资产组合 投资策略 强调流动性管理，基金资产中要保持一定现金及流动性资产 全部资金可进行长期投资 4，基金起源第一只公认的证券投资基金——英国“海外及殖民地政府信托” 第一只开放式公司型基金 —— 美国“马萨诸塞投资信托基金” 第三章 证券投资基金的类型1，基金的分类（一）根据运作方式分类： 封闭式基金 开放式基金 （二）根据投资对象分类： 股票基金：基金资产80%以上投资于股票 债券基金：基金资产80%以上投资与债券 货币市场基金：仅投资于货币市场工具 混合基金：投资于股票、债券和货币市场工具，但股票投资和债券投资的比例不符合股票基金、债券基金规定的为混合基金。 基金中的基金：80%以上的基金资产投资于其他基金份额 （三）根据投资理念分类： 主动型基金：一类力图取得超越基准组合表现的基金。 被动（指数）型基金：不主动寻求取得超越市场的表现，而是试图复制指数的表现。被动型基金一般选取特定的指数作为跟踪的对象，因此通常又被称为指数型基金。 （四）根据募集方式分类： 公募基金 私募基金 （五）特殊类型基金 系列基金：又称伞形基金，是指多个基金共用一个基金合同，子基金独立运作，基金直接可以进行相互转换的一种基金结构形式。 保本基金：指通过一定的保本投资策略进行运作，同时引入保本保障机制，以保住基金份额持有人在保本周期到期时，可以获得投资本金保住的基金。 上市交易型开放式指数基金（ETF）：又称为交易所交易基金（exchange traded funds，ETF），是一种在交易所上市交易的、基金份额可变的一种开放式基金。 上市开放式基金：（listed open-ended funds， LOF），是一种既可以在场外市场进行基金份额申购、赎回，又可以在交易所（场内市场）进行基金份额交易和基金份额申购或赎回的开放式基金。 QDII基金：（qualified domestic institutional investors，合格境内机构投资者），是指在一国境内设立，经该国有关部门批准从事境外证券市场的股票、债券等有价证券投资的基金。 分级基金：指通过事先约定基金的风险收益分配，将基础份额分为预期风险收益不同的子份额，并可将其中部分或全部份额上市交易的结构化证券投资基金。 2，股票基金价值型股票通常是指收益稳定、价值被低估、安全性较高的股票，其市盈率、市净率通常较低。 成长型股票通常是指收益增长速度快、未来发展潜力大的股票，其市盈率、市净率通常较高。 3，债券基金根据债券发行者，可以将债券分为政府债券、企业债券、金融债券等。 根据债券到期日，可以将债券分为短期债券、长期债券等。 根据债券信用等级，可以将债券分为低等级债券、高等级债券等。 我国市场上的债券基金分类，常见的有以下类型： 标准债券型基金，仅投资于固定收益类金融工具，不能投资股票市场，常称“纯债基金”。 普通债券型基金，即主要进行债券投资（80%以上基金资产），但也投资于股票市场，这类基金在我国市场上占主要部分。 4，货币市场基金与其他类型基金相比，货币市场基金具有风险低、流动性好的特点。 货币市场工具通常指到期日不足1年的短期金融工具。 5，保本基金保本策略： 国际上比较流行的投资组合保险策略主要有对冲保险策略与固定比例投资组合保险策略（constant proportion portfolio insurance， CPPI） 对冲保险策略主要依赖金融衍生产品，如股票期权、股指期货等，实现投资组合价值的保本与增值。国内保本基金主要选择恒定比例投资组合保险策略作为投资的保本策略。 例子： 若某投资者投资10万元认购南方保本基金，加上该笔认购按照100%比例全部予以确认，并且持有到保本期到期，认购费率为1.0%。假设募集期间产生的利息为50元，持有期间基金累计分红0.08元/基金份额。则认购份额为： 净认购金额 = 100 000 / (1+1.0%) = 99009.90 （元） 认购费用 = 100 000 - 99009.90 = 990.1 （元） 认购份额 = （99009.90 + 50）/ 1.00 = 99059.90（份） （1）若保本期到期日，基金份额净值为0.90元 保本金额 = 基金份额持有人认购并持有到期的基金份额的投资金额 = 100000 + 50 = 100050.00 （元） 到期的基金份额与净值的乘积 = 99059.90 * 0.9 = 89153.91（元） 累计分红 = 99059.90 * 0.08 = 7924.79（元） 总金额 = 89153.91 + 7924.79 = 97078.70（元） 即：总金额 &lt; 保本金额 担保人应赔付的差额 = 100050.00 - 97078.70 = 2971.30（元） （2）若保本期到期日，基金份额净值为1.38元 基金份额与净值的乘积 = 99059.90 * 1.38 = 136702.66（元） 累计分红 = 99059.90 * 0.08 = 7924.79（元） 总金额 = 136702.66 + 7924.79 = 144627.45（元） 即：总金额 &gt; 保本金额 到期日可赎回金额 = 136702.66（元） 6，交易型开放式指数基金（ETF） 三大特点： 被动操作的指数基金 ETF是以某一选定的指数所包含的成分证券为投资对象，依据构成指数的股票种类和比例，采取完全复制或抽样复制，进行被动投资的指数基金。 独特的实物申购、赎回机制。 申购ETF时，需拿这只ETF指定的一篮子股票来换取；赎回时得到的不是现金，而是相应的一篮子股票，如果想变现，需要再卖出这些股票。 实行一级市场与二级市场并存的交易制度。 ETF与LOF的区别LOF与ETF都具备开放式基金可以申购、赎回和场内交易的特点，但两者存在本质区别，主要表现在： (1) 申购、赎回的标的不同。ETF与投资者交换的是基金份额与一篮子股票，LOF的申购、赎回是基金份额与现金的对价。 (2) 申购、赎回的场所不同。ETF的申购、赎回通过交易所进行；LOF的申购、赎回既可以在代销网点进行，也可以在交易所进行。 (3) 对申购、赎回限制不同。只要资金在一定规模以上的投资者（基金份额通常要求在50万份以上）才能参与ETF的申购、赎回交易，LOF没有特别要求。 (4) 基金投资策略不同。ETF通常采用完全被动式管理方法，以拟合某一指数为目标；LOF可以是指数型基金，也可以是主动型基金。 (5) 在二级市场的净值报价上，ETF每15秒提供一个基金参考净值（IOPV）报价，LOF的净值报价频率要比ETF低，通常1天只提供1次或几次基金净值报价。 ETF联接基金ETF联接基金是讲绝大部分基金财产投资于某一ETF（称为目标ETF），密切跟踪标的指数表现，可以在场外（银行渠道等）申购赎回的基金。 7，QDII基金在境内募集资金进行境外证券投资的机构称为合格境内机构投资者（qualified domestic institutional investor，QDII）。 8，分级基金分级基金是指通过事先约定基金的风险收益分配，将母基金份额分为预期风险收益不同的子份额，并可将其中部分或全部类别份额上市交易的结构化证券投资基金。其中，分级基金的基础份额称为母基金份额，预期风险收益较低的子份额称为A类份额，预期风险收益较高的子份额称为B类份额。 基金成立后，投资者在场内认购的母基金份额自动分离为A类份额和B类份额，并上市交易；对于从场内申购的母基金份额，投资者即可选择将其分拆为A类份额和B类份额并上市交易，也可选择不进行基金份额分拆而保留母基金份额。 因此，证券交易所场内可存在三类份额：母基金份额、A类份额和B类份额，其中母基金份额通常只能够被申购和赎回，而A类份额和B类份额则只可上市交易。 A类份额具有低风险、收益稳定的特征，比较适合保守型、偏好固定收益品种的投资者； B类份额具有高风险、高预期收益的特征，比较适合偏好杠杆投资的激进型投资者； 母基金份额等同于普通股票指数基金份额，具有较高风险、较高预期收益的特征，比较适合具有较高风险承受能力的配置型投资者。 第四章 证券投资基金的监管1，基金管理人依据《证券投资基金法》的规定，基金管理人由依法设立的公司或者合伙企业担任。而担任公开募集基金的基金管理人的主体资格受到严格限制，只能由基金管理公司或者经中国证监会按照规定核准的其他机构担任。所谓“中国证监会按照规定核准的其他机构”，是指依据中国证监会2013年2月18日发布的《资产管理机构开展公募证券投资基金管理暂行规定》，在股东、高级管理人员、经营期限、管理的基金财产规模等方面符合规定条件的证券公司、保险资产管理公司以及专门从事非公开募集基金管理业务的资产管理机构。 2，基金经理任职条件： 取得基金从业资格； 通过中国证监会或者其授权机构组织的证券投资法律知识考试； 具有3年以上证券投资管理经历； 没有《公司法》《证券投资基金法》等法律、行政法规规定的不得担任公司董事、监事、经理和基金从业人员的情形； 最近3年没有受到证券、银行、工商和税务等行政管理部门的行政处罚。 2013年《证券投资基金法》借鉴发达国家的监管思路和做法，一方面允许基金从业人员进行证券投资，另一方面强化对其监管。即在避免利益冲突的情况下，允许基金从业人员投资股票、债权、封闭式基金、可转债等证券；同时，要求相关人员进行事先申报，披露其投资行为，接受各方面的监督。 3，公开募基金募集期限基金管理人应当自收到准予注册文件之日起6个月内进行基金募集。 投资范围： 上市交易的股票、债券 中国证监会规定的其他证券及其衍生品种。 4，非公开募集基金依据《证券投资基金法》的规定，非公开募集基金应当向合格投资者募集，合格投资者累计不得超过200人。 私募基金的合格投资者指具备相应风险识别能力和风险承担能力，投资于单只私募基金的金额不低于100万元且符合下列相关标准的单位和个人： 净资产不低于1000万元的单位； 金融资产不低于300万元或者最近3年个人年均收入不低于50万元的个人。 上述金融资产包括银行存款、股票、债券、基金份额、资产管理计划、银行理财产品、信托计划、保险产品、期货权益等。 私募基金投资范围包括买卖公开发行的股票、债券、基金份额，以及中国证监会规定的其他证券及其衍生品种。 第5章 基金职业道德1，内蒙交易的定义内幕信息的构成三要素： 来源可靠的信息。来源不可靠、模棱两可的信息，即便对证券价格产生影响，也不构成内幕信息。 “重要”的信息。即该信息对于证券价格的影响明确。如果该信息的披露会对证券价格产生影响或者属于理性投资者在做投资决策前希望知悉的，那么该信息就是“重要”的。 “非公开”的信息。一般认为，在市场得到一个信息之前，这个信息就是“非公开”的。 第6章 投资管理基础1，资产负债表balance sheet 称为企业的“第一会计报表”。资产负债表报告了企业在某一时点的资产、负债和所有者权益的状况，报告时点通常为会计季末、半年末或者会计年末。 资产 = 负债 + 所有者权益 所有者权益又称股东权益或净资产，是指企业总资产中扣除负债所余下的部分，表示企业的资产净值，即在清偿各种债务以后，企业股东所拥有的资产价值。 包括以下四部分： 股本，即按照面值计算的股本金； 资本公积，包括股票发行溢价、法定财产重估增值、接受捐赠资产、政府专项拨款转入等； 盈余公积，又分为法定盈余公积和任意盈余公积； 未分配利润，指企业留待以后年度分配的利润或待分配利润。 2，利润表income statement，亦称损益表，反映一定时期（如一个会计季度或会计年度）的总体经营成果，揭示企业财务状况发生变动的直接原因。 利润表由三部分构成： 营业收入； 与营业收入相关的生产性费用、销售费用和其他费用； 利润 在评价企业的整体业绩时，重点在于企业的净利润，即息税前利润（earning before interest and tax，EBIT）减去利息费用和税费。 3，现金流量表cash flow statement，也叫账务状况变动表，所表达的是在特定会计期间内，企业的现金（包含现金等价物）的增减变动等情形。 现金流量表的基本结构分为三部分： 经营活动产生的现金流量（cash flow from operation，CFO） 投资活动产生的现金流量（cash flow from investment，CFI） 筹资（也称融资）活动产生的现金流量（cash flow from financing，CFF） 净现金流 （net cash flow，NCF）的公式 NCF = CFO + CFI + CFF 净现金流（NCF）为正或为负并非判断企业财务现金流量健康的唯一标准，关键要分析现金流量结构。 现金流量结构可以反映企业的不同发展阶段。在企业的起步、成长、成熟和衰退等不同周期阶段，企业现金流量模式不同，企业的现金流量也存在较大的差异性。 初创企业：新兴的、快速成长的企业，由于需要不断进行资本投资，其经营活动现金流量可能为负，而筹资活动产生的现金流量可能为正。 发展中的企业：而随着企业的成长，依赖外部融资的程度会逐渐减低。典型成熟的企业会产生经营活动现金净流量，并将其部分或全部用于再投资，因此在财务特征上表现为经营活动现金净流量为正，投资活动现金流量为负。 成熟的企业：随着企业不断发展并迈入成熟阶段，则可能会减少外部融资，甚至为减少外部融资成本而更多使用现金偿还债务。 4，财务比率分析财务比率分析是指用财务比率来描述企业财务状况、盈利能力以及流动性的分析方法。 （一）流动性比率流动性比率是用来衡量企业的短期偿债能力的比率，旨在分析短期内企业在不致使财务状况恶化的前提下，利用手中持有的流动资产偿还短期负债的能力大小。 流动资产主要包括现金及现金等价物、应收票据、应收账款和存货等几项资产； 流动负债是指企业要在一年或一个营业周期内偿付的各类短期债务，包括短期借款、应付票据、应付账款等。 1，流动比率流动比率 = 流动资产 / 流动负债 对于短期债权人来说，流动比率越高越好，因为越高意味着他们回收债款的风险越低；但对于企业来说，因为流动资产的收益率较低，这部分比重过大势必影响到企业的经营获利或者投资盈利状况。 2，速动比率相对于其他流动资产来说，存货的流动性较差。因为存货的变现需要通过销售来实现。 速动比率 = （流动资产 - 存货）/ 流动负债 相对于流动比率来说，速动比率对于短期偿债能力的衡量更加直观可信。一般来说，速动比率大于2时，企业才能维持较好的短期偿债能力和财务稳定状况。 （二）财务杠杆比率财务杠杆比率衡量的是企业长期偿债能力。由于企业的长期负债与企业的资本结构即使用的财务杠杆有关，所以称为财务杠杆比率。 1，资产负债率资产负债率是负债总额（包括短期负债和长期负债）占总资产的比例，即： 资产负债率= 负债 / 资产 资产负债率在同行业企业的比较重有较大的参考价值。 2，权益乘数和负债权益比权益乘数 = 资产 / 所有者权益 负债权益比 = 负债 / 所有者权益 其中，权益乘数又称杠杆比率。 由于资产 = 负债 + 所有者权益，所以： 权益乘数 = 1 / ( 1 - 资产负债率） 负债权益比 = 资产负债率 / ( 1 - 资产负债率） 资产负债率、权益乘数和负债权益比三个比率其实是同一意思，由其中一个比率可以很容易计算出另外两个比率，并且都是数值越大代表财务杠杆比率越高，负债越重。 3，利息倍数衡量企业对于长期债务利息保障程度的是利息倍数，其公式为： 利息倍数 = EBIT / 利息 式中，EBIT是息税前利润。 为了维持正常的偿债能力，利息倍数至少应该为1，并且越高越好。若利息倍数过低，企业将面临亏损、偿债的稳定性与安全性下降的风险。 （三）营运效率比率营运效率用来体现企业经营期间的资产从投入到产出的流转速度，可以反映企业资产的管理质量和利用效率。 1，存货周转率存货周转率显示了企业在一年或者一个经营周期内存货的周转次数。其公式为： 存货周转率 = 年销售成本 / 年均存货 年均存货通常是指年内期初存货和期末存货的算术平均数。例如假设企业年初存货是20000元，年末存货是5000元，那么年均存货就是(20000+5000)/2 = 12500元。 存货周转天数，其公式为： 存货周转天数 = 365天 / 存货周转率 例如，如果一家企业的存货周转率是4，这意味着这家企业平均只要花 365 / 4 ≈ 92天就能将库存的存货全部销售出去。 2，应收账款周转率存货周转率告诉我们存货的销售速度能有多块，而应收账款周转率则告诉我们能够以多快的速度回收销售收入。 应收账款周转率显示了企业在一年或一个经营周期内，应收账款的周转次数。其公式为： 应收账款周转率 = 销售收入 / 年均应收账款 应收账款周转天数 = 365天 / 应收账款周转率 3，总资产周转率总资产周转率衡量的是一家企业所有资产的使用效率，它的计算公式是： 总资产周转率 = 年销售收入 / 年均总资产 这里的年均总资产也是企业年内期初资产和期末资产的算术平均数。总资产周转率越大，说明企业的销售能力越强，资产利用效率越高。 （四）盈利能力比率评价企业盈利能力的比率有很多，其中最重要的有三种：销售利润率（ROS）、资产收益率（ROA）、净资产收益率（ROE）。这三种比率都使用的是企业的年度净利润。 1，销售利润率销售利润率是指每单位销售收入所产生的利润，其计算公式为： 销售利润率 = 净利润 / 销售收入 2，资产收益率资产收益率计算的是每单位资产能带来的利润，其计算公式为： 资产收益率 = 净利润 / 总资产 资产收益率是应用最为广泛的衡量企业盈利能力的指标之一。资产收益率高，表明企业有较强的利用资产创造利润的能力，企业在增加收入和节约资金使用等方面取得了良好的效果。 资产收益率的特点是，它所考虑的净利润仅仅是股东可以获得的利润，而资产却是包括股东资产和债权人资产在内的总资产。 3，净资产收益率净资产收益率也称权益报酬率，强调每单位的所有者权益能够带来的利润，其计算公式为： 净资产收益率 = 净利润 / 所有者权益 由于现代企业最重要的经营目标就是最大化股东财富，因而净资产收益率是衡量企业最大化股东财富能力的比率。 5，杜邦分析法杜邦分析法（DuPont Analysis）是一种用来评价企业盈利能力和股东权益汇报水平的方法，它利用主要的财务比率之间的关系来综合评价企业的财务状况。 杜邦分析法的基本思想是将企业净资产收益率逐级分解为多项财务比率乘积，从而有助于深入分析比较企业经营业绩。 净资产收益率 = 净利润 / 所有者权益 = (净利润 / 总资产) * (总资产 / 所有者权益) = 资产收益率 * 权益乘数 资产收益率 = 净利润 / 总资产 = (净利润 / 销售收入) * (销售收入 / 总资产) = 销售利润率 * 总资产周转率 杜邦恒等式： 净资产收益率 = 销售利润率 * 总资产周转率 * 权益乘数 通过杜邦恒等式，我们可以看到一家企业的盈利能力综合取决于企业的销售利润率、使用资产的效率和企业的财务杠杆。 6，终值、现值和贴现终值已知期初投入的现值为PV，求将来值即第n期期末的终值PV，其计算公式为： $$ FV = PV * (1+i)^n $$ i表示年利率， 现值是指将来货币金额的现在价值。 $$ PV = \\dfrac{FV}{(1+i)^n} $$ 贴现将未来某时点资金的价值折算为现在时点的价值称为贴现。在现值计算中，利率i也被称为贴现率。 例如，某公司发行了面值为1000元的5年期零息债券，现在的市场利率为8%，那么该债券的现值为：1000 / (1+ 8%)^5 = 680.58 (元) 7，名义利率与实际利率实际利率是指在物价不变且购买力不变的情况下的利率，或者是指当物价有变化，扣除通货膨胀补偿以后的利息率。 名义利率是指包含对通货膨胀补偿的利率，当物价不断上涨时，名义利率比实际利率高。 费雪方程式 式中：in为名义利率， ir 为实际利率，p为通货膨胀率 8，单利与复利$$ FV = PV * (1+i)^n $$ (1+i)^n称为复利终值系数或1元的复利终值，用符号(FV, i, n)表示。例如(FV, 6%, 3)表示利率为6%，3期复利终值系数。 $$ PV = \\dfrac{FV}{(1+i)^n} $$ (1+i)^(-n)称为复利现值系数或1元的复利现值，用符号(PV, i, n)表示。 利用复利现值系数可以计算按揭贷款的每期偿还数额。例如，以等额本息法偿还本金为100万元，利率为7%的5年期住房贷款。假设每年末需要偿还的本金利息总额为M元，则有： 计算可知，M = 243890.69 In [16]: 1000000/(1/1.07+1/1.07**2+1/1.07**3+1/1.07**4+1/1.07**5) Out[16]: 243890.6944413741 9，即期利率与远期利率即期利率(spot rate)是金融市场中的基本利率，常用St表示，是指已设定到期日的零息票债券的到期收益率。 贴现因子$$ d{t} = \\dfrac{1}{(1+s{t})^t} $$ 其中，St为即期利率。 远期利率foward rate指的是资金的远期价格，它是指隐含在给定的即期利率中从未来的某一时点到另一时点的利率水平。具体表示为未来两个日期间借入货币的利率，也可以表示投资者在未来特定日期购买的零息票债券的到期收益率。 例如，1年和2年期的即期利率分别为S1=7%和S2=8%，求远期利率 根据无套利原则，存在 $$ (1+s{2})^2 = (1+s{1})(1+f) $$ $$ f = \\dfrac{(1+s{2})^2}{(1+s{1})}-1 $$ f = 1.08*1.08/1.07 - 1 = 9.01% 10，随机变量的统计量期望（均值）随机变量X的期望（或称均值，记做 E(X) ）衡量了X取值的平均水平；它是对X所有可能取值按照其发生概率大小加权后得到的平均值。 $$ E(X) = \\sum{i=1}^{n}p{i}x{i} = p{1}x{1} + p{2}x{2} + … + p{n}x_{n} $$ 在X的分布未知时，我们用抽取样本X1, … , Xn的算术平均数（也称样本均值） $$ \\overline X =\\dfrac{1}{n}\\sum{i=1}^{n}X{i} $$ 作为E(X)的估计值。 方差与标准差对于投资收益率r，我们用方差或者标准差来衡量它偏离期望值的程度。 其中， $$ \\sigma^2 = E[(r - Er)^2] $$ 它的数值越大，表示收益率r偏离期望收益率的程度越大，反之亦然。 第7章 权益投资1，权益资本两种最主要的权益证券是普通股和优先股 普通股common shares 是股份有限公司发行的一种基本股票，代表公司股份中的所有权份额，其持有者享有股东的基本权利和义务。 优先股preferred shares和普通股一样代表对公司的所有权，它的优先权主要指：持有人分得公司利润的顺序先于普通股，在公司解散或破产清偿时先于普通股获得剩余财产。优先股的股息率往往是事先规定好的、固定的，它不因公司经营业绩的好坏而有所变动。 普通股和优先股的风险收益比较优先股在分配股利和清算时剩余财产的索取权优先于普通股，因而风险较低。此外，固定的股息收益也降低了优先股的风险。然而，优先股的收益也因此被限定为固定的股息收益，当公司盈利多时，相比普通股而言，优先股获利更少。 普通股股东享有对剩余利润的要求权意味着其有较高的潜在收益率。当公司运营良好时，普通股股东可以获得丰厚的收益，而优先股股东只能取得固定的股息。因此，相比于优先股，普通股具有较高风险和较高收益的特征。 2，莫迪利亚尼-米勒定理（MM定理）在不考虑税、破产成本、信息不对称并且假设在有效市场里面，企业价值不会因为企业融资方式改变而改变。 修正的MM定理企业可以运用避税政策，通过改变企业的资本结构来改变企业的市场价值，即企业发行债券或获取贷款越多，企业市场价值越大。 均衡理论认为随着企业债务增加而提高的经营风险和可能产生的破产成本，会增加企业的额外成本，而最佳的资本结构应当是负债和所有者权益之间的一个均衡点，这一均衡点D*就是最佳负债比率。 3，股票的价值(1) 股票的票面价值（face value）又称面值，即在股票票面上标明的金额。股票的票面价值在初次发行时有一定参考意义。 (2) 股票的账面价值（book value）又称股票净值或每股净资产，是每股股票所代表的实际资产的价值。 (3) 股票的清算价值（liquidation value）是公司清算时每一股份所代表的实际价值。 (4) 股票的内在价值（intrinsic value）即理论价值，是指股票未来收益的现值。股票的内在价值决定股票的市场价格，股票的市场价格总是围绕其内在价值波动。 4，存托凭证存托凭证（depository receipt）是指在一国证券市场上流通的代表外国公司有价证券的可转让凭证。存托凭证一般代表外国公司股票。 全球存托凭证（global depository receipts，GDSs） 美国存托凭证（American depository receipts, ADSs)是以美元计价且在美国证券市场上交易的存托凭证。ADR是最主要的存托凭证，其流通量最大。 5，可转换债券是指在一段时期内，持有者有权按照约定的转换价格（conversion price）或转换比率（conversion ratio）将其转换成普通股股票的公司债券。 转换价格 = 可转换债券价值 / 转换比例 6，权证权证（warrant）是指标的证券发行人或其以外的第三人发行的，约定在规定期间内或特定到期日，持有人有权按约定价格向发行人购买或出售标的证券，或以现金结算方式收取结算差价的有价证券。 按基础资产的来源分类，权证可分为认股权证和备兑权证。 认股权证是股份公司发行的，行权时上市公司增发新股售予认股权证的持有人。 备兑权证是由投资银行发行的，行权时备兑权证持有者认兑的是市场上已流通的股票而非增发的，上市公司股本不变。 按照持有人权利的性质分类，权证可分为认购权证和认沽权证。 认购权证近似于看涨期权，行权时其持有人可按照约定的价格购买约定数量的标的资产。 认沽权证近似于看跌期权，行权时其持有人可按照约定的价格卖出约定数量的标的资产。 按行权时间分类： 美式权证可在权证失效日之前任何交易日行权； 欧式权证仅可在失效日当日行权； 百慕大式权证可在失效日之前一段规定时间内行权。 7，权益类证券投资收益风险资产期望收益率 = 无风险资产收益率 + 风险溢价 其中，无风险资产收益率即无风险利率（risk-free interest rate）。 8，股票基本面分析三步估价法，宏观 —行业—个股，“自上而下”的层次分析法； （一）宏观经济分析宏观经济指标： 国内生产总值（GDP） GDP = C + I + （X - M）+ G 式中，C代表消费；I代表投资；X-M代表净出口；G代表政府支出。 通货膨胀 居民消费价格指数、生产者物价指数、商品价格指数等。 利率 汇率 预算赤字 政府的预算赤字是政府支出和政府收入之间的差额。 失业率 采购经理指数（purchasing managers’ index， PMI） （二）行业分析任何一个行业都要经理一个生命周期： 初创期 成长期 平台期（成熟期） 衰退期 （三）公司内在价值与市场价格股票的内在价值即理论价值，是指股票未来收益的现值，由公司资产、收益、股息等因素所决定。 证券的市场价格是由市场供求关系所决定的，市场价格不仅受到资产内在价值与未来价值因素的影响，还可能受到市场情绪、技术、投机等因素的影响。 9，超额收益贴现模型：经济附加值（EVA）模型经济附加值（economic value added， EVA）指标源于企业经营绩效考核的目的。 经济附加值等于公司税后净营业利润减去全部资本成本（股本成本与债务成本）后的净值。计算公式为： EVA = NOPAT - 资本成本 式中：NOPAT表示税后经营利润，或称息前税后利润，是指息税前利润EBIT扣除经营所得税；资本成本等于WACC乘以实际投入资本总额；WACC表示加权平均资本成本。 第8章 固定收益投资1，债券市场债券（bond），通常又称固定收益证券（fixed-income securities），因为这类金融工具能够提供固定数额或根据固定公式计算出的现金流。 货币市场证券主要是短期性、高流动性证券，例如银行拆借市场、票据承兑市场、回购市场等交易的债券；而固定收益资本市场中交易的则是长期债券。 2，国债收益率曲线国债收益率曲线是反映远期利率的有效途径，它的水平和斜率反映了经济主体对未来通货膨胀的预期和对未来基本的经济形势的判断。其实市场无风险利率最合适的替代，从而为其他债券和金融资产以及投资项目提供定价的基准。 3，按债券持有人收益方式分类 固定利率债券固定收益债券（fixed-rate bond）是由政府和企业发行的主要债券种类，有固定的到期日，并在偿还期内有固定的票面利率和不变的面值。 浮动利率债券浮动利率债券（floating-rate bond）和固定利率债券的主要不同是其票面利率不是固定不变的，而通常与一个基准利率挂钩，在其基础上加上利差（可正可负）以反映不同债券发行人的信用。浮动利率可表达为： 1浮动利率 = 基准利率 + 利差 基准利率，在国际金融市场上通常采用伦敦银行间同业拆解利率（Libor），在我国，上海银行间同业拆借利率（Shibor）是被广泛采纳的货币市场基准利率。国际惯例将利差用基点（basis point）表示，1个基点（1bps）等于0.01%。 零息债券零息债券（zero-coupon bond）和固定利率债券一样有一定的偿还期限，但在期间不支付利息，而在到期日一次性支付利息和本金，一般其值为债券面值。因此，零息债券以低于面值的价格发行，到期日支付的面值和发行时价格的差额即为投资者的收益。 4，按计息与付息方式分类债券可分为息票债券和贴现债券。 息票债券是指债券发行时规定，在债券存续期内，在约定的时间以约定的利率按期向债券持有人支付利息的中、长期债券。 贴现债券则是无息票债券或零息债券，这种债券在发行时不规定利率，券面也不附息票，发行人以低于债券面额的价格出售债券，即这家发行，债券到期时发行人按债券面额兑付。 5，按嵌入的条款分类 可赎回债券可赎回债券（callable bond）为发行人提供在债券到期前的特定时段以事先约定价格买回债券的权力。约定的价格称为赎回价格（call price） 可回售债券可回售债券（puttable bond）为债券持有者提供在债券到期前的特定时段以事先约定价格将债券回售给发行人的权力，约定的价格成为回售价格（put price） 可转换债券可转换债券是指在一段时间后，持有者有权按约定的转换价格或转换比率将公司债券转换为普通股票。 通货膨胀联结债券 结构化债券 资产证券化（asset securitization）指以其他债券组成的资产池尾支持，构建新的债券产品形式。此类新构建的债券成为结构化债券，主要是住房抵押贷款支持证券和资产支持证券。 6，债券违约时的受偿顺序有保证债券 &gt; 优先无保证债券 &gt; 优先次级债券 &gt; 次级债券 &gt; 劣后次级债券 7，投资债券的风险信用风险信用风险（credit risk）又叫违约风险（default risk），是指债券发行人未按照契约的规定支付债券的本金和利息，给债券投资者带来损失的可能性。 国际上知名的独立信用评级机构有三家： 穆迪投资者服务公司（Moody’s Investor Service） 标准·普尔评级服务公司（Standard &amp; Poor’s） 惠誉国际信用评级有限公司（Fitch Investor Service） 国内主要的债券评级机构包括： 大公国际资信评估有限公司 中诚信国际信用评级有限公司 联合资产评估有限公司 上海新世纪资信评估投资服务有限公司 中债资信评估有限责任公司 利率风险利率风险（interest rate risk）是指利率变动引起债券价格波动的风险。 债券的价格与利率呈反向变动关系：利率上升时，债券价格下降；利率下降时，债券价格上升。 通胀风险所有种类的债券都面临通胀风险（inflation risk），因为利息和本金都是不随通胀水平变化的名义金额。 流动性风险是指债券投资者将手中的债券变现的能力。 再投资风险指在市场利率下行的环境中，附息债券收回的利息或者提前于到期日收回的本金只能以低于原债券到期收益率的利率水平再投资于相同属性的债券，而产生的风险。 提前赎回风险又称为回购风险，是指债券发行者在债券到期日前赎回有提前赎回条款的债券所带来的风险。 8，中国债券交易市场体系从20世纪80年代开始逐步发展起来的 以柜台市场为主（1988 - 1991年） 以交易所市场为主（1992 - 2000年） 以银行间市场为主（2001年至今） 9，债券的估值方法1，零息债券估值法由于面值是投资者未来唯一的现金流，所有贴现债务的内在价值由一下公式决定： $$ V = M\\dfrac{1}{(1+r)^t} $$ 式中，V表示贴现债券的内在价值；M表示面值；r表示市场利率；t表示债券到期时间。 由于多数零息债券期限小于一年，因此上述贴现公式应简单调整为： $$ V = M(1-\\dfrac{t}{360}r) $$ 某种贴现式国债面额为100元，贴现率为3.82%，到期时间为90天，则该国债的内在价值为100(1-90/360 3.82%) = 99.045 （元） 2，固定利率债券估值法固定利率债券是一种按照票面金额计算利息，票面上附有（也可不附有）作为定期支付利息凭证的期票的债券。投资者未来的现金流包括两部分：本金和利息。其内在价值公式如下： $$ V = \\dfrac{C}{1+r} + \\dfrac{C}{(1+r)^2} + … + \\dfrac{C}{(1+r)^n} + \\dfrac{M}{(1+r)^2} $$ 式中，C表示每期支付的利息；V表示贴现债券的内在价值；M表示面值；r表示市场利率；n表示债券到期时间。 例： 某种附息国债面额为100元，票面利率为5.21%，市场利率为4.89%，期限为3年，每年付息1次，则该国债的内在价值为： $$ V = \\dfrac{5.21}{1+0.0489} + \\dfrac{5.21}{(1+0.0489)^2}+ \\dfrac{5.21}{(1+0.0489)^3} + \\dfrac{100}{(1+0.0489)^3} = 100.873 （元）$$ 3，统一公债估值法统一公债是一种没有到期日的特殊债券。在现代企业中，优先股的股东可以无期限地获得固定股息，因此，也相当于一种统一公债。其内在价值计算公式如下： $$ V = \\dfrac{C}{1+r} + \\dfrac{C}{(1+r)^2} + … + \\dfrac{C}{(1+r)^n} = \\dfrac{C}{r} $$ 10，当期收益率、到期收益率与债券价格的关系当期收益率current yield，又称当前收益率，是债券的年利息收入与当前的债券市场价格的比率。其计算公式为： $$ I = \\dfrac{C}{P} $$ 式中：I表示当期收益率，C表示年息票利息，P表示债券市场价格。 当期收益率没有考虑债券投资所获得的资本利得或损失。 到期收益率yield to maturity， YTM，又称内部收益率，是可以使投资购买债券获得的未来现金流的现值等于债券当前市价的贴现率。 到期收益率隐含两个重要假设： 投资者持有至到期 利息再投资收益率不变 到期收益率一般用y表示，债券市场价格和到期收益率的关系式为： $$ P = \\sum_{t=1}^{n}\\dfrac{C}{(1+y)^t} + M(\\dfrac{1}{1+y})^n $$ 式中：P表示债券市场价格；C表示每期支付的利息，n表示时期数；M表示债券面值。 例： 票面金额为100元的2年期债券，第一年支付利息6元，第二年支付利息6元，当前市场价格为95元，则该债券的到期收益率和当前价格之间的关系可表达为： 95 = 6/(1+y) + 106/(1+y)^2 求解得y=8.836% 11，信用利差信用利差（credit spread），是指除了信用评级不同外，其余条件全部相同（包括但不限于期限、嵌入条款等）两种债券收益率的差额。 12，债券的久期和凸度利率变化是影响债券价格的主要因素之一，久期和凸度是衡量债券价格随利率变化特性的两个重要指标。 久期麦考利久期（duration），又称为存续期，指的是债券的平均到期时间，它是从现值角度度量了债券现金流的加权平均年限，即债券投资者收回其全部本金和利息的平均时间。 凸性凸性是债券价格与到期收益率之间的关系用弯曲程度的表达方式。 13，货币市场工具的特点 均是债务契约 期限在1年以内（含1年） 流动性高 大宗交易，主要由机构投资者参与，个人投资者很少有机会参与买卖 本机安全性高，风险较低 常用的货币市场工具（一）银行定期存款（二）短期回购协议回购协议是指资金需求方在出售证券的同时与证券的购买方约定在一定期限后按约定价格购回所卖证券的交易行为。 证券回购协议的主要功能有三个： 中国人民银行以此为工具进行公开市场操作，方便中央银行投放（收回）基础货币，形成合理的短期利率； 为商业银行的流动性和资产结构的管理提供了必要的工具； 各类非银行金融机构可以通过证券回购协议实现套期保值、头寸管理、资产管理、增值等目的。 回购协议是一种证券抵押贷款，以国债为主。证券的出售方为正回购方，证券的购买方为逆回购方。 （三）中央银行票据是由中央银行发行的用于调节商业银行超额准备金的短期债务凭证，简称央行票据或央票。 （四）短期政府债券期限在1年及1年以内的债务凭证。三个特点： 违约风险小 流动性强 利息免税 （五）短期融资券是境内具有法人资格的非金融企业发行的，仅在银行间债券市场上流通的短期债务工具。 （六） 中期票据中期票据的期限一般为1年以上、10年以下，我国的中期票据的期限通常为3年或5年。 中期票据采用注册发行，最大注册额度不超过企业净资产的40%。 （七）证监会、中国人民银行认可的其他具有良好流动性的货币市场工具。 同业拆借同业拆借是指金融机构之间以货币借贷方式进行短期资金融通的行为。 上海银行间同业拆借利率（SHIBOR） 同业拆借的利息是按日结算的。 银行承兑汇票银行承兑汇票是由在承兑银行开立存款账户的存款人出票，向开户银行申请并经银行审查同意承兑的，保证在指定日期无条件支付确定的金额给收款人或持票人的票据。 商业票据是指发行主体为满足流动资金的需求所发行的期限为2天至270天的、可流通转让的债务工具。 大额可转让定期存单 同业存单是存款类金融机构在全国银行间市场上发行的记账式定期存款凭证。 第9章 衍生工具1，衍生工具的定义衍生工具（derivatives），是指一种衍生类合约，其价值取决于一种或多种基础资产。这些基础资产通常被称作合约标的资产（underlying）。合约标的可以是股票、债券、货币等金融资产，也可以是黄金、原油等大宗商品或贵金属。 衍生工具的特点： 跨期性 杠杆性 联动性 不确定性或高风险性 2，按合约特点分类 远期合约forward contract 是指交易双发约定在未来的某一确定的时间，按约定的价格买入或卖出一定数量的某种合约标的资产的合约。是非标准化的合约。 期货合约futures contract，相比于远期合约，期货合约是标准化合约。2006年9月，中国金融期货交易所正式成立。2010年4月，推出沪深300指数期货。 期权合约option contract，又称作选择权合约。 互换合约swap contract。 结构化金融衍生工具。 3，期货市场的交易制度 保证金制度比例通常在5%~10% 盯市制度盯市（marking to market）是期货交易最大的特征，又称为“逐日结算”，即在每个营业日的交易停止以后，成交的经纪人之间不直接进行现金结算，而是将所有清算事务都交由清算机构办理。 对冲平仓制度 交割制度 4，期权合约期权合约的要素 标的资产 期权的买方买方为买入期权的一方，即支付费用从而获得权利的一方，也称期权的多头。 期权的卖方卖方为卖出期权的一方，即获得费用因而承担在规定的时间内履行该期权合约义务的一方，也称期权的空头。 执行价格又称协议价格 期权费 通知日 到期日 按期权买方执行期权的时限分类 欧式期权，指期权的而买房只有值期权到期日才能执行期权，既不能提取也不能推迟。 美式期权，允许期权买方在期权到期前的任何时间执行期权。 按期权买方的权利分类 看涨期权 看跌期权 影响期权价格的因素 合约标的资产的市场价格与期权的执行价格 期权的有效期 无风险利率水平 标的资产价格的波动率 合约标的资产的分红 表：影响期权价格的因素及其影响方向 影响因素 看涨期权 看跌期权 合约标的资产的市场价格⬆️ ⬆️ ⬇️ 期权的执行价格⬆️ ⬇️ ⬆️ 期权的有效期⬆️ ⬆️ ⬆️ 标的资产价格的波动率 ⬆️ ⬆️ ⬆️ 无风险利率水平⬆️ ⬆️ ⬇️ 合约标的的资产分红⬆️ ⬇️ ⬆️ 5，互换合约（一）利率互换interest swap，是指互换合约双方同意在约定期限内按不同的利息计算方式向对方支付由币种相同的名义本金额所确定的利息。 双方进行利率互换的主要原因是双方在固定利率和浮动利率市场上分别具有比较优势。 （二）货币互换6，远期合约、期货合约、期权合约和互换合约的区别（1）交易场所与合约期货合约只在交易所交易，期权合约大部分在交易所交易，远期合约和互换合约通常在场外交易，采用非标准形式进行。 （2）损益特性远期合约、期货合约和大部分互换合约都包括买卖双方在未来应尽的义务，称为双边合约 而期权合约和信用违约互换合约只有一方在未来有义务，称为单边合约。 （3）信用风险（4）执行方式（5）杠杆期货、期权有杠杆，远期合约和互换合约通常没有杠杆效应。 第10章 另类投资另类投资（alternative investment）是指传统公开市场交易的权益资产、固定收益类资产和货币类资产之外的投资类型。通常包括私募股权、房产与商铺、矿业与能源、大宗商品、基础设施、对冲基金、收藏市场等领域。 1，私募股权投资的战略形式（一）风险投资一般采用股权形式将资金投入提供具有创新性的专门产品或服务的初创型企业（start-up） （二）成长权益成长权益战略投资于已经具备成型的商业模型和较好的顾客群，同时具备正现金流的企业。成长权益投资者通过提供资金，帮助企业扩大规模。 （三）并购投资是指专门进行企业并购的基金。 并购投资包含多种不同类型： 杠杆收购，是指筹资过程当中所包含的债券融资比例较高的收购形式。 并购投资者偏好于将那些最近经营不像预期一样好那具备可成长空间的企业设定为目标企业。 （四）危机投资（五）私募股权二级市场投资第11章 投资者需求1，企业年金基金企业年金基金，是指企业年金计划筹集的资金及其投资运营收益形成的企业补充养老保险基金。企业年金基金财产的投资范围，限于银行存款、国债和其他具有良好流动性的金融产品，包括短期债券回购、信用等级在投资级以上的金融债和企业债、可转换债、投资性保险产品、证券投资基金、股票等。 第12章 投资组合管理1，系统性风险一般为宏观层面的因素，主要包含政治因素、宏观经济因素、法律因素以及某些不可抗力因素。 2，最小方差法最小方差法适应于投资者对预期收益率有一个最低要求的情形。投资者希望在投资组合的预期收益率达到给定目标的条件下最小化投资组合的风险，并且投资者以方差来度量投资组合的风险。 3，有效前沿有效前沿是由全部有效投资组合构成的集合。如果一个投资组合在所有风险相同的投资组合中具有最高的预期收益率，或者在所有预期收益率相同的投资组合中具有最小的风险，那么这个投资组合就是有效的。 4，资本资产定价模型资本资产定价模型（capital asset pricing model，CAPM）以马克维茨证券组合理论为基础，研究如果投资者都按照分散化的理念去投资，最终证券市场达到均衡时，价格和收益率如何决定的问题。 主要思想 资本资产定价模型认为只有证券或证券组合的系统性风险才能获得收益补偿，其非系统性风险将得不到收益补偿。 证券市场线 证券市场线描述了一个资产或资产组合的预期收益率与其贝塔值之间的关系。贝塔值越高，则它的预期收益率越高，对于贝塔值为零的资产来说，它的预期收益率就应当等于无风险收益率。 5，市场有效性 弱有效市场 是指证券价格能够充分反映价格历史序列中包含的所有信息，如证券的价格、交易量等。如果这些历史信息对证券价格的变动不会产生任何影响，则意味着证券市场达到了弱有效。 半强有效市场 是指证券价格不仅已经反映了历史价格信息，而且反映了当前所有与公司证券有关的公开有效信息，例如盈利预测、红利发放、股票分析、公司并购等各种公告信息。 强有效市场 是指与证券有关的所有信息，包括公开发布的信息和未公开发布的内部信息，都已经充分、及时地反映到了证券价格之中。 6，被动投资与跟踪误差跟踪误差是度量一个股票组合相对于某基准组合偏离程度的重要指标。该指标被广泛用于被动投资及主动投资管理者的业绩考核。跟踪误差（tracking error）是证券组合相对基准组合的跟踪偏离度的标准差，其中跟踪偏离度（tracking difference）的计算公式如下： 跟踪偏离度 = 证券组合的真实收益率 - 基准组合的收益率 7，主动投资主动收益（active return）即相对于基准的超额收益，其计算方法如下： 主动收益 = 证券组合的真实收益 - 基准组合的收益 8，资产配置分类 从范围上看，可分为全球资产配置、股票债券资产配置和行业风格资产配置等 从时间跨度和风格类别上看，可分为战略性资产配置、战术性资产配置和资产混合配置等 从配置策略上可分为买入并持有策略、恒定混合策略、投资组合保险策略和动态资产配置策略等 第13章 投资交易管理1，报价驱动市场报价驱动（quote driven）中，最为重要的角色就是做市商，因此报价驱动市场也被称为做市商制度。做市商通常由具备一定实力和信誉的证券投资法人承担，本身拥有大量可交易证券，买卖双方均直接与做市商交易，而买卖价格则由做市商报出。与股票不同的是，几乎所有的债券和外汇都是通过做市商交易的。 2，保证金交易在我国，保证金交易被称为“融资融券”。 融资即投资者借入资金购买证券，也叫买空交易。融券即投资者借入证券卖出，也称卖空交易。 维持保证金比例 = （现金+信用证券账户内证券市值总和）/ （融资买入金额 + 融券卖出证券数量 * 当前市价 + 利息及费用总和） 第14章 投资风险的管理与控制1，市场风险 政策风险 经济周期性波动风险 利率风险 购买力风险（通货膨胀风险） 汇率风险 2，贝塔系数贝塔系数（β）是评估证券或投资组合系统性风险的指标，反映的是投资对象对市场变化的敏感度。 贝塔系数大于0时，该投资组合的价格变动方向与市场一致；贝塔系数小于0时，该投资组合的价格变动方向与市场相反。贝塔系数等于1时，该投资组合的价格变动幅度与市场一直。 3，最大回撤最大回撤是从资产最高价格到接下来最低价格的损失。投资的期限越长，这个指标就越不利。 4，风险价值风险价值（value at risk， VaR），又称在险价值、风险收益、风险报酬，是指在一定的持有期和给定的置信水平下，利率、汇率等市场风险要素发生变化时可能对某项资金头寸、资产组合或投资机构造成的潜在最大损失。 5，持股集中度$$ 持股集中度 = \\dfrac{前十大重仓股投资市值}{基金股票投资总市值} * 100\\% $$ 第15章 基金业绩评价1，绝对收益(1) 持有区间收益率持有区间所获得的收益通常来源于两部分： 资产回报，是指股票、债券、房地产等资产价格的增加/减少 收入回报，包括分红、利息、租金等。 $$ 资产回报率 = \\dfrac{期末资产价格 - 期初资产价格}{期初资产价格} * 100\\% $$ $$ 收入回报率 = \\dfrac{期间收入}{期初资产价格} * 100\\% $$ 例： 假设某投资者在2013你那12月31日，买入1股A公司股票，价格为100元，2014年12月31日，A公司发放3元分红，同时其股价为105元，那么该区间内： 资产回报率 = (105 - 100) /100 * 100% = 5% 收入回报率 = 3/100 * 100% = 3% 总持有区间的收益率 = 5% + 3% = 8% (2) 基金收益率公募基金每天公布单位资产净值（NAV），其计算公式为： 期末基金单位资产净值 = 期末基金资产净值 / 期末基金单位总份额 假设红利发放后立即对本基金进行再投资，且红利以除息前一日的单位净值为计算基准立即进行再投资，分别计算每次分红期间的分段收益率，考察期间的时间加权收益率可由分段收益率连乘得到： 例： 假设某基金在2012年12月3日的单位净值为1.4848元，2013年9月1日的单位净值为1.7886元。期间该基金曾于2013年2月28日每份额派发红利0.275元。该基金2013年2月27（除息前一天）的单位净值为1.8976元，则该基金在这段时间内的时间加权收益率为： $$ R = \\frac{1.8976}{1.4848} \\dfrac{1.7886}{1.8976-0.275}-1 100\\% = 40.87\\% $$ (3) 平均收益率已知某基金近三年来累计收益率为26%，那么应用几何平均收益率计算的该基金的年(几何)平均收益率应为： $$ R_{G} = [(1+26\\%)^{1/3}-1] * 100\\% = 8.01\\% $$ 2，夏普比率夏普比率是诺贝尔经济学奖得主威廉·夏普于1966年根据资本资产定价模型（CAPM）提出的经风险调整的业绩测度指标。此比率是用某一时期内投资组合平均超额收益除以这个时期收益的标准差。用公式表示为： $$ S{p} = \\dfrac{\\overline R{p} - \\overline R{f}}{\\sigma{p}} $$ 式中：Sp表示夏普比率，Rp表示基金的平均收益率，Rf表示平均无风险收益率，sigma p表示基金收益率的标准差。 夏普比率数值越大，代表单位风险超额回报率越高，基金业绩越好。 3，特雷诺比率特雷诺比率来源于CAPM理论，表示的是单位系统风险下的超额收益率。用公式表示为： $$ T{p} = \\dfrac{\\overline R{p} - \\overline R{f}}{\\beta{p}} $$ 特雷诺比率与夏普比率相似，两者的区别在于特雷诺比率使用的是系统风险，而夏普比率则是对全部风险进行了衡量。 第16章 基金的募集、交易与登记1，基金的募集程序 募集申请的注册 证监会应当自受理基金募集申请之日起6个月内做出注册或不予注册的决定。 基金份额的发售 基金管理人应当自收到核准文件之日起6个月内进行基金份额的发售。 基金的募集期限自基金份额发售之日起计算，募集期限一般不得超过3个月。 基金的合同生效 封闭式基金需满足募集的基金份额总额达到核准规模的80%以上，并且基金份额持有人达到200人以上； 开放式基金需满足募集份额总额不少于2亿份，募集金额不少于2亿元人民币，基金份额持有人的人数不少于200人。 2，基金的认购开放式基金的认购采取金额认购的方式，即投资者在办理认购申请时，不是直接以认购数量提出申请，而是以金额申请。 3，封闭式基金折（溢）价率投资者常常使用折（溢）价率反映封闭式基金份额净值与其二级市场之间的关系。折（溢）价率的计算公式为： $$ 折（溢）价率 = \\dfrac{二级市场价格 - 基金份额净值} {基金份额净值} = (\\dfrac{二级市场价格}{基金份额净值} - 1) * 100\\% $$ 当基金二级市场价格高于基金份额净值时，为溢价交易，对应的是溢价率；当二级市场价格低于基金份额净值时，为折价交易，对应的是折价率。 4，开放式基金的申购和赎回原则股票基金、债券基金的申购和赎回原则 未知价交易原则 金额申购、份额赎回原则 货币市场基金的申购和赎回原则 确定价原则。货币市场基金申购和赎回基金份额以1元人民币为基准进行计算。 金额申购、份额赎回原则。 5，申购份额及赎回金额的计算某投资者通过场外（某银行）投资1万元申购某上市开放式基金，假设基金管理人规定的申购费率为1.5%，申购当日基金份额净值为1.025元，则其申购手续费和可得到的申购份额为： 净申购金额 = 10000/(1+1.5%) = 9852.22 元 申购手续费 = 10000 - 9852.22 = 147.78 元 申购份额 = 9852.22/1.025 = 9611.92 份 某投资者赎回上市开放式基金1万份基金单位，持有时间为1年半，对应的赎回费率为0.5%。假设赎回当日基金单位净值为1.0250元，则其可得净赎回金额为： 赎回总金额 = 10000 * 1.025 = 10250 元 赎回手续费 = 10250 * 0.005 = 51.25 元 净赎回金额 = 10250 -51.25 = 10198.75 元 实现后端收费模式的基金，还应扣除后端认购/申购费，才是投资者最终得到的赎回金额。即： 赎回金额 = 赎回总额 - 赎回费用 - 后端收费金额 6，ETF基金份额折算的方法基金管理人通常会以某一选定日期作为基金份额折算日，以标的指数的千分之一（或1%）作为份额净值，对原来的基金份额及其净值进行折算。 假设某投资者在某ETF基金募集期内认购了5000份ETF，基金份额折算日的基金资产净值为3127000230.95元，折算前的基金份额总额为3013057000份，当日标的指数收盘值为966.45元。 （1）折算比例 = (3127000230.95/3013057000) / (966.45/1000) = 1.07384395 （2）该投资者折算后的基金份额 = 5000 * 1.07384395 = 5369.22 份 第17章 基金的投资交易与清算1，大宗交易大宗交易是指单笔数额较大的证券买卖。我国现行有关交易制度规定，如果证券单笔买卖申报达到一定数额的，证券交易所可以采用大宗交易方式进行。 上海证券交易所接受大宗交易的时间为每个交易日9:30 - 11:30、13:00 - 15:30。 2，开盘价和收盘价 开盘价为当日证券的第一笔成交价，证券的开盘价通过集合竞价方式产生。不能产生开盘价的，以连续竞价方式产生。按集合竞价产生开盘价后，未成交的买卖申报仍然有效，并按原申报顺序自动进入连续竞价。 收盘价，上交所的收盘价为当日该证券最后一笔交易前1分钟所有交易的成交量加权平均价（含最后一笔交易）。深交所的收盘价通过集合竞价的方式产生。收盘集合竞价不能产生收盘价或未进行收盘集合竞价的，以当日该证券最后一笔交易前1分钟所有交易的成交量加权平均价（含最后一笔交易）为收盘价。 除权与除息 因送股或配股而形成的剔除行为称为除权，因派息而引起的剔除行为称为除息。 3，回购 质押式回购，是指一方（正回购方）在讲回购债券出质给另一方（逆回购方），逆回购方在首期结算日向正回购方支付首期资金结算额的同时，交易双方约定在将来某一日期（即到期结算日）由正回购方向逆回购方支付到期资金结算额，同时逆回购方解除在回购债券上设定的质权的交易。 买断式回购，是指一方（正回购方）在将回购债券出售给另一方（逆回购方），逆回购方在首期结算日向正回购方支付首期资金结算额的同时，交易双方约定在将来某一日期（即到期结算日）由正回购方以约定价格（即到期资金结算额）从逆回购方购回回购债券的交易。期限最长不得超过91天。 第18章 基金的估值、费用与会计核算1，基金资产估值基金份额净值 = 基金资产净值 / 基金总份额 2，基金资产估值频率开放式基金，于每个交易日估值，并于次日公告基金份额净值。封闭式基金每周披露一次基金份额净值，但每个交易日也都进行估值。 3，基金费用 管理费率 我国股票基金大部分按照1.5%的比例计提基金管理费，债券基金的管理费率一般低于1%，货币市场基金的管理费率不高于0.33% 托管费 我国股票型封闭式基金按照0.25%的比例计提基金托管费；开放式基金根据基金合同的规定比例计提，通常低于0.25%；股票基金的托管费率要高于债券基金及货币市场基金的托管费率。 基金销售服务费 目前只有货币市场基金和一些债券型基金收取，费率一般为0.25%。收取销售服务费的基金通常不收取申购和赎回费。 计提方法和支付方式 目前，我国的基金管理费、基金托管费及基金销售服务费均是按前一日基金资产净值的一定比例逐日计算，按月支付。计算方法如下： $$ H = \\dfrac{E·R}{当年实际天数} $$ 式中：H表示每日计提的费用；E表示前一日的基金资产净值；R表示年费率。 4，基金会计核算会计年度为公历每年1月1日至12月31日 基金会计核算的内容主要包括以下业务： 证券和衍生工具交易核算 权益核算 利息和溢价核算 费用核算 基金申购与赎回核算 估值核算 利润核算 基金财务会计报告 基金会计核算的复核 第19章 基金的利润分配与税收1，与基金利润有关的财务指标 本期利润 是基金在一定时期内全部损益的总和，包括计入当期损益的公允价值变动损益。 本期已实现收益 指基金本期利息收入、投资收益、其他收入（不含公允价值变动损益）扣除相关费用后的余额，是将本期利润扣除本期公允价值变动损益后的余额，反映基金本期已经实现的损益。 期末可供分配利润 该指标是指期末可供基金进行利润分配的金额，为期末资产负债表中未分配利润与未分配利润中已实现部分的孰低数。 未分配利润 是基金进行利润分配后的剩余额。未分配利润将转入下棋分配。 2，利润分配《公开募集证券投资基金运作管理办法》规定：封闭式基金的收益分配，每年不得少于一次；封闭式基金年度收益分配比例不得低于基金年度可供分配利润的90%。基金收益分配后基金份额净值不得低于面值。 封闭式基金只能采用现金分红。 开放式基金可采用现金分红方式和分红后再投资转换为基金份额。 第20章 基金的信息披露1，信息披露时间在每年结束后90日内，在指定报刊上披露年度报告摘要，在管理人网站上披露年度报告全文。在上半年结束后60日内，在指定报刊上披露半年度报告摘要，在管理人网站上披露半年度报告全文。在每季结束后15个工作日内，在指定报刊和管理人网站上披露基金季度报告。 2，基金运作信息披露文件主要包括 基金净值公告 基金定期公告 基金上市交易公告书 3，基金招募说明书内容 基金投资目标 投资范围 投资策略 业绩比较基准 风险收益特征 投资限制 基金销售行为规范及信息管理赎回费 对持续持有期少于7日的投资人收取不低于1.5%的赎回费 对持续持有期少于30日的投资人收取不低于0.75%的赎回费，并将上述赎回费全额计入基金财产 对于持续持有期少于3个月的投资人收取不低于0.5%的赎回费，并将不低于赎回费总额的75%计入基金财产 对于持续持有期长于3个月但少于6个月的投资人收取不低于0.5%的赎回费，并将不低于赎回费总额的50%计入基金财产 *对持续持有期长于6个月的投资人，应当将不低于赎回费总额的25%计入基金财产。 第26章 基金的国际化1，欧盟的基金法规可转让证券集合投资计划（Undertakings for Collective Investment in Transferable Securities， UCITS），是欧盟跨境投资基金的主要模式。 另类投资基金管理人指令（Alternative Investment Fund Managers Directive， AIFMD） 2，QFII合格境外机构投资者（qualified foreign institutional investors， QFII），是我国在资本项目未完全开放的背景下选择的一种过渡性资本市场开放制度。 考试重点：保证金交易在我国，保证金交易被称为“融资融券”。融资即投资者借入资金购买证券，也叫买空交易。融券即投资者借入证券卖出，也称卖空交易。1维持保证金比例 = （现金+信用证券账户内证券市值总和）/ （融资买入金额 + 融券卖出证券数量 * 当前市价 + 利息及费用总和） 贝塔系数$$ \\beta{p} =p{p,m} o{p}/o{m} $$证券投资组合p的收益率的标准差为0.49，市场收益率的标准差为0.32，投资组合p与市场收益的相关系数为0.6，则该投资组合的贝塔系数为：beta = 0.6 0.49/0.32 = 0.92 时间加权收益率前半年的收益率为15%，后半年的收益率为9.77%，则其一年的时间加权收益率为：1(1+15%)*(1+9.77%) -1 = 26.23% 费雪方程式物价上涨时，名义利率一般比实际利率高1实际利率 = 名义利率 - 通货膨胀率 不变增长模型股利贴现公式上年末某公司支付每股股息为2元，预计在未来其股息按每年4%的速度增长，必要收益率为12%，则该公司股票的价值为多少？V = 2 * (1 + 4%）/ ( 12% - 4%) = 26 元 资本资产定价模型 假设资本资产定价模型成立，某股票的预期收益率为16%，贝塔系数为2，如果市场预期收益率为12%，市场的无风险利率为：$$ E(r{i}) = r{F} + (E(r{M} - r{F}) * \\beta $$ 16% = r + (12% - r) * 2 ，解得 r = 8 詹森指数假定在样本期内无风险利率为6%，市场资产组合的平均收益率为18%，基金A的平均收益率为17.6%，贝塔值为1.2；则詹森指数为：17.6% - [6% + (18%-6%) * 1.2] = -2.8% 除权（息）参考价 = ( 前收盘价 - 现金红利 + 配股价格 * 股份变动比例) / ( 1 + 股份变动比例)股利贴现模型久期 与 债券内在价值某3年期债券的面值为1000元，票面利率为8%，每年付息一次，现在市场收益率为10%，其市场价格为950.25元，则其久期为：内在价值：V = (10008%)/(1+8%) + (10008%)/(1+8%)^2 + (1000*8%)/(1+8%)^3 + 1000/(1+8%)^3久期： D = V / 950.25 = 2.78 当期收益率假定某投资者按940原的价格购买了面额为1000元、票面利率为10%、剩余期限为6年的债券，那么该投资者的当期收益率为：$$ I = \\dfrac{C}{P} $$ 式中：I表示当期收益率，C表示年息票利息，P表示债券市场价格。 I = 1000 * 10% / 940 = 10.64% 到期收益率一般用y表示，债券市场价格和到期收益率的关系式为： $$ P = \\sum_{t=1}^{n}\\dfrac{C}{(1+y)^t} + M(\\dfrac{1}{1+y})^n $$ 式中：P表示债券市场价格；C表示每期支付的利息，n表示时期数；M表示债券面值。 例： 票面金额为100元的2年期债券，第一年支付利息6元，第二年支付利息6元，当前市场价格为95元，则该债券的到期收益率和当前价格之间的关系可表达为： 95 = 6/(1+y) + 106/(1+y)^2 求解得y=8.836% 国内第一家投资基金1992年11月，淄博乡镇企业投资基金（简称“淄博基金”）正式设立，1993年8月在上交所挂牌上市。该基金委公司型封闭式基金。 中国证券投资基金试点的序幕1998年3月27日，南方基金和国泰基金公司分别发起的两只封闭式基金——基金开元和基金金泰。 #国内第一只开放式基金 2001年9月，华安创新 国内第一只货币市场基金2003年12月推出的华安现金富利投资基金 对冲基金起源于20世纪50年代初的美国。 世界上第一只开放式投资基金1924年由200多名哈佛大学教授出资5万美元在波士顿成立的“马萨诸塞投资信托基金”被公认为美国开放式公司型共同基金的鼻祖。 世界上最早的证券投资基金英国“海外及殖民地政府信托基金”成立于1868年 世界上第一只ETF基金1990年，加拿大多伦多证券交易所推出了世界上第一只ETF指数参与份额。 证券投资基金法实施日期2004年6月1日，修订后的证券投资基金法于2013年6月1日正式实施。 中国证券投资基金业协会成立日期2012年6月6日 ETF折算比例假设某投资者在基金募集期内认购了100万份ETF，基金份额折算日的基金资产净值为320亿元，折算日的基金份额总额为310万亿，当日标的指数收盘值为966.45元，则该投资者折算后的份额为多少万份？ 解： 折算比例 = (折算日的基金资产净值 / 折算日的基金份额总额) / （当日标的指数收盘值 / 1000) = (320/310) / (966.45/1000) = 1.068 ； 折算后的基金份额 = 原持有份额 折算比例 = 100 1.068 = 106.8 万份。","tags":[{"name":"基金","slug":"基金","permalink":"http://www.kekefund.com/tags/基金/"},{"name":"考试","slug":"考试","permalink":"http://www.kekefund.com/tags/考试/"}]},{"title":"Scrapy框架初探","date":"2016-03-31T09:00:03.000Z","path":"2016/03/31/scrapy-learn/","text":"scrapy爬虫框架在业内大大有名，自己写过静态网页和动态网页的爬虫，一直没拿scrapy来写，近来看了scrapy的官方文档，了解了大致的流程，故拿来练手实践了一个项目。 本文主要抓取股吧的文章，内容包括: 定义抓取Spider 数据字段的定义 内容解析 数据存储到mysql PyCharm调试scrapy 一、定义抓取Spider创建一个新的Spider 12scrapy startproject tutorialscrapy genspider guba_spider eastmoney.com 默认创建的Spider是继承与BaseSpider，一般我们继承功能更多的CrawlSpider。 1234567from scrapy.spiders import CrawlSpider, Rulefrom scrapy.linkextractors import LinkExtractorclass GubaSpider(CrawlSpider): name = 'guba' allowed_domains = ['eastmoney.com'] 定义好GubaSpider类后，然后要指定开始网页start_urls和rules抓取网页规则。 12345678910start_urls = [ 'http://guba.eastmoney.com/default_%d.html' % index for index in range(1, 100)]rules = ( # 提取匹配 文章 的链接并使用spider的parse_article方法进行分析 Rule(LinkExtractor(allow=(r'^http://guba.eastmoney.com/news.', )), callback='parse_article'), Rule(LinkExtractor(allow=(r'^http://iguba.eastmoney.com/\\d+.',)), callback='parse_auther')) 我们主要爬取的是下面这块区域： rules规则定义了两类链接，一个是文章链接，一个是作者链接。 链接的规则可以通过Chrome的开发者工具得到，如下： Rule中的LinkExtractor是从网页(scrapy.http.Response)中抽取满足allow条件的链接，callback回调至指定函数。 启动Spider，执行的流程是： 从start_urls中开始爬取网页， 找到满足文章链接的规则，跳转到self.parse_article()函数进一步处理。 二、定义数据字段在scrapy目录下的items.py中定义结构化数据字段。 1234567891011121314import scrapyclass ArticleItem(scrapy.Item): uuid = scrapy.Field() # 唯一标识符 user_id = scrapy.Field() user_name = scrapy.Field() title = scrapy.Field() classify = scrapy.Field() content = scrapy.Field() readed_count = scrapy.Field() comment_count = scrapy.Field() href = scrapy.Field() source = scrapy.Field() published_date = scrapy.Field() scrapy_date = scrapy.Field() # 抓取日期 三、内容解析scrapy默认的是用xpath解析网页，由于对Beautifulsoup更熟悉，我在本文中用的Beautifulsoup来解析网页内容，道理都是一样的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from bs4 import BeautifulSoup as bsimport refrom hashlib import md5from gubademo.items import ArticleItemdef parse_article(self, response): print \"content news:%s\" % response.url soup = bs(response.body, 'lxml') item = ArticleItem() item['title'] = response.url item['href'] = response.url div_name = soup.find('div',&#123;'id':'zwconttbn'&#125;) if div_name: item['user_name'] = div_name.find('a').text if div_name.find('a').has_attr('data-popper'): item['user_id'] = div_name.find('a')['data-popper'] span_stockname = soup.find('span', &#123;'id':'stockname'&#125;) if span_stockname: item['classify'] = span_stockname.find('a').text # 内容 c_div = soup.find('div', &#123;'class':'stockcodec'&#125;) if c_div: item['content'] = '' for s in c_div.strings: item['content'] += s # 时间 t_div = soup.find('div', &#123;'class':'zwfbtime'&#125;) if t_div: s1 = re.search('\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125; \\d&#123;2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;',t_div.text) if s1: item['published_date'] = s1.group() # 阅读数和评论数 m = re.search('num=(\\d+).*?var count=\\d+', response.body) if m: item['readed_count'] = m.group(1) else: item['readed_count'] = u'0' m = re.search('var pinglun_num=(\\d+)', response.body) if m: item['comment_count'] = m.group(1) else: item['comment_count'] = u'0' item['source'] = 'guba_eastmoney' item['scrapy_date'] = GetNowTime() item['uuid'] = md5(item['href']).hexdigest() print item yield item 四、写入数据库解析完数据，接下来是要保存数据以便以后分析使用。 自定义Pipeline，spider将item传递到pipeline，默认调用的是process_item()函数，我们可以在processs_item中根据item的类型进行差异化处理。 需要先在setting.py中设置如下内容，scrapy才能走着这一步。 123456# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.htmlITEM_PIPELINES = &#123; #'wealth_tech.pipelines.DuplicatePipeline':200, 'wealth_tech.pipelines.MySQLStorePipeline': 300,&#125; MySQLStorePipeline定义了一个article_items集合用于存储spider爬到的item，当items数量达到1000时，批量写入数据库。如果接受到item就单条写入数据库，会比批量写入慢很对，爬虫的效率会慢一个数量级。存入mysql之前，先查询数据库，若不存在则insert，存在则update。数据库的host,port等信息一般存在setting.py中。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import MySQLdbimport MySQLdb.cursorsimport pandas as pdimport wealth_tech.settings as settingsfrom sqlalchemy import create_engineengine = create_engine('mysql+mysqldb://%s:%s@%s:%d/%s' % (settings.MYSQL_USER, settings.MYSQL_PASSWD, settings.MYSQL_HOST, 3306, settings.MYSQL_DBNAME), connect_args=&#123;'charset':'utf8'&#125;)class MySQLStorePipeline(object): \"\"\" 写入mysql数据库 \"\"\" def __init__(self): self.article_items = &#123;&#125; # pipeline默认调用 def process_item(self, item, spider): print spider.name if type(item) is ArticleItem: self.process_article_item(item, spider) # 文章 return item def process_article_item(self, item, spider): \"\"\" 保存文章 \"\"\" table = 'article_guba_easymoney' self.article_items.setdefault(spider.name, []) self.article_items[spider.name].append(item) if len(self.article_items[spider.name]) &gt;= 1000: # 积累到1000条就写入数据库 conn=MySQLdb.connect(host=settings.MYSQL_HOST,user=settings.MYSQL_USER,passwd= settings.MYSQL_PASSWD,db=settings.MYSQL_DBNAME,charset=\"utf8\") cursor = conn.cursor() df = pd.read_sql('select uuid from &#123;&#125;'.format(table), engine) uuids = df['uuid'].get_values() uuids = set(uuids) for item in self.article_items[spider.name]: try: if item['uuid'] not in uuids: # 插入 sql = 'insert into &#123;&#125; values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)'.format(table) param = (item['uuid'],item['user_id'],item['user_name'],item['title'],item['classify'],item['content'], item['readed_count'],item['comment_count'],item['href'],item['source'],item['published_date'],item['scrapy_date']) n = cursor.execute(sql,param) print 'insert ',n, item['uuid'] else: #更新 sql = \"update &#123;&#125; set fans_count=%s, article_count=%s,visit_count=%s,comment_count=%s,scrapy_date=%s where user_id='&#123;&#125;'\".format(table, item['user_id']) #print sql param = (item['fans_count'],item['article_count'],item['visit_count'],item['comment_count'],item['scrapy_date']) n = cursor.execute(sql,param) print 'update', n,item['uuid'] except Exception,e: print e #提交 conn.commit() #关闭 conn.close() self.article_items[spider.name] = [] 数据库呈现的结果： 五、PyCharm Debug调试scrapyscrapy通常在命令行里运行，但仅通过log显示的信息来调试时非常费劲的，程序猿需要的是单步调试，step by step。 在PyCharm中调试也是很容易的。 在scrapy项目的根目录下（与scrapy.cfg同级）新建一个文件run.py，内容如下： 1234#!/usr/bin/pythonfrom scrapy.cmdline import executeexecute() 新建一个Run/Debug Configurations，Script选择run.py，Script parameters输入crawl guba。其中guba是在GubaSpider中定义的name。这样启动Debug就能单步调试了。","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"scrapy","slug":"scrapy","permalink":"http://www.kekefund.com/tags/scrapy/"}]},{"title":"Pandas数据分析基础","date":"2016-02-23T02:27:49.000Z","path":"2016/02/23/pandas-anlysis-basic/","text":"使用pandas，首先导入包： 12from pandas import Series, DataFrameimport pandas as pd 一、创建Series，DataFrame1，创建Seriesa，通过列表创建12obj = Series([4, 7, -5, 3]) obj2 = Series([4, 7, -5, 3], index=['d','b','a','c']) #指定索引 b，通过字典创建Series12sdata = &#123;'Ohio':35000, 'Texas':7100, 'Oregon':1600,'Utah':500&#125;obj3 = Series(sdata) c，通过字典 + 索引12states = ['California', 'Ohio', 'Oregon', 'Texas']obj4 = Series(sdata, index=states) 指定索引时，跟states索引匹配的那3个值会被找出并放到相应的位置，‘California’对应的sdata值找不到，其结果为NaN。 2，创建DataFramea，词典生成12345678data = &#123;'state':['Ohio', 'Ohio', 'Ohio', 'Nevada','Nevada'], 'year':[2000, 2001, 2002, 2011, 2002], 'pop':[1.5, 1.7, 3.6, 2.4, 2.9]&#125;frame = DataFrame(data)frame2 = DataFrame(data, columns=['year', 'state', 'pop']) #指定列frame3 = DataFrame(data, columns=['year', 'state', 'pop']， index=['one', 'two', 'three', 'four', 'five']) #指定列和索引 b，列表生成123456789101112&gt;&gt;&gt; errors = [('c',1,'right'), ('b', 2,'wrong')]&gt;&gt;&gt; df = pd.DataFrame(errors)&gt;&gt;&gt; df 0 1 20 c 1 right1 b 2 wrong&gt;&gt;&gt; df = pd.DataFrame(errors, columns=['name', 'count', 'result']) #指定列名&gt;&gt;&gt; df name count result0 c 1 right1 b 2 wrong c, 嵌套词典（也就是词典的词典）12345678pop = &#123;'Nevada':&#123;2001:2.4, 2002:2.9&#125;, 'Ohio':&#123;2000:1.5, 2001:1.7, 2002:3.6&#125;&#125;frame4 = DataFrame(pop)Out[138]: Nevada Ohio2000 NaN 1.52001 2.4 1.72002 2.9 3.6 d，Series组合按行生成DataFrame12345678In [4]: a = pd.Series([1,2,3]) In [5]: b = pd.Series([2,3,4])In [6]: c = pd.DataFrame([a,b]) In [7]: cOut[7]: 0 1 20 1 2 31 2 3 4 按列生成DataFrame1234567In [8]: c = pd.DataFrame(&#123;'a':a,'b':b&#125;)In [9]: cOut[9]: a b0 1 21 2 32 3 4 二，选取对于一组数据DataFrame： 1234567data = DataFrame(np.arange(16).reshape((4,4)),index=['Ohio', 'Colorado','Utah','New York'],columns=['one','two','three','four'])&gt;&gt;&gt; data one two three fourOhio 0 1 2 3Colorado 4 5 6 7Utah 8 9 10 11New York 12 13 14 15 1，选取列，返回一个Series123456&gt;&gt;&gt; data['two']Ohio 1Colorado 5Utah 9New York 13Name: two, dtype: int64 2，选取行，返回一个Series123456&gt;&gt;&gt; data.ix['Ohio']one 0two 1three 2four 3Name: Ohio, dtype: int64 3， 选取行和列, 可以是行名，列名，或列的序号1234&gt;&gt;&gt; data.ix['Ohio', ['two','three']]two 1three 2Name: Ohio, dtype: int64 12345&gt;&gt;&gt; data.ix[data.three &gt; 3, :3] one two threeColorado 4 5 6Utah 8 9 10New York 12 13 14 三、遍历与汇总1，按行遍历1for ix, row in df.iterrows(): 2，按列遍历1for ix, col in df.iteritems(): 3，汇总123456In[95]: frame = DataFrame(&#123;'b':[4, 7, -3, 2], 'a':[0, 1, 0, 1]&#125;)In[99]: frame.sum()Out[99]: a 2b 10dtype: int64 四、排序1，对索引排序对轴索引排序 Series用sort_index()按索引排序，sort()按值排序； DataFrame用sort_index()和sort()是一样的。 123456789101112131415161718192021222324252627In[73]: obj = Series(range(4), index=['d','a','b','c'])In[74]: obj.sort_index() Out[74]: a 1b 2c 3d 0dtype: int64In[78]: frame = DataFrame(np.arange(8).reshape((2,4)),index=['three', 'one'],columns=['d','a','b','c'])In[79]: frameOut[79]: d a b cthree 0 1 2 3one 4 5 6 7In[86]: frame.sort_index()Out[86]: d a b cone 4 5 6 7three 0 1 2 3In[87]: frame.sort()Out[87]: d a b cone 4 5 6 7three 0 1 2 3 2，按行排序12345In[89]: frame.sort_index(axis=1, ascending=False)Out[89]: d c b athree 0 3 2 1one 4 7 6 5 3，按列排序（只针对Series）12345678In[90]: obj.sort()In[91]: objOut[91]: d 0a 1b 2c 3dtype: int64 4，按值排序Series: 12345678In[92]: obj = Series([4, 7, -3, 2])In[94]: obj.order()Out[94]: 2 -33 20 41 7dtype: int64 DataFrame: 12345678In[95]: frame = DataFrame(&#123;'b':[4, 7, -3, 2], 'a':[0, 1, 0, 1]&#125;)In[97]: frame.sort_index(by='b')Out[97]: a b2 0 -33 1 20 0 41 1 7 五、删除1，删除指定轴上的项即删除 Series 的元素或 DataFrame 的某一行（列）的意思，通过对象的 .drop(labels, axis=0) 方法： 删除Series的一个元素: 1234567In[11]: ser = Series([4.5,7.2,-5.3,3.6], index=['d','b','a','c'])In[13]: ser.drop('c')Out[13]: d 4.5b 7.2a -5.3dtype: float64 删除DataFrame的行或列： 1234567891011121314151617181920In[17]: df = DataFrame(np.arange(9).reshape(3,3), index=['a','c','d'], columns=['oh','te','ca'])In[18]: dfOut[18]: oh te caa 0 1 2c 3 4 5d 6 7 8In[19]: df.drop('a')Out[19]: oh te cac 3 4 5d 6 7 8In[20]: df.drop(['oh','te'],axis=1)Out[20]: caa 2c 5d 8 .drop() 返回的是一个新对象，元对象不会被改变。 六、DataFrame连接1，算术运算（+，-，*，/）是df中对应位置的元素的算术运算 1234567891011In[5]: df1 = DataFrame(np.arange(12.).reshape((3,4)),columns=list('abcd'))In[6]: df2 = DataFrame(np.arange(20.).reshape((4,5)),columns=list('abcde'))In[9]: df1+df2Out[9]: a b c d e0 0 2 4 6 NaN1 9 11 13 15 NaN2 18 20 22 24 NaN3 NaN NaN NaN NaN NaN 传入填充值 1234567In[11]: df1.add(df2, fill_value=0)Out[11]: a b c d e0 0 2 4 6 41 9 11 13 15 92 18 20 22 24 143 15 16 17 18 19 2，pandas.mergepandas.merge可根据一个或多个键将不同DataFrame中的行连接起来。 默认情况下，merge做的是“inner”连接，结果中的键是交集，其它方式还有“left”，“right”，“outer”。“outer”外连接求取的是键的并集，组合了左连接和右连接。 内连接12345678910111213In[14]: df1 = DataFrame(&#123;'key':['b','b','a','c','a','a','b'],'data1':range(7)&#125;)In[15]: df2 = DataFrame(&#123;'key':['a','b','d'],'data2':range(3)&#125;)In[18]: pd.merge(df1, df2) #或显式: pd.merge(df1, df2, on='key')Out[18]: data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 0 外连接1234567891011In[19]: pd.merge(df1, df2, how='outer')Out[19]: data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 06 3 c NaN7 NaN d 2 轴向连接这种数据合并运算被称为连接（concatenation）、绑定（binding）或堆叠（stacking）。 对于Series 1234567891011121314In[23]: s1 = Series([0, 1], index=['a','b'])In[24]: s2 = Series([2, 3, 4], index=['c','d','e'])In[25]: s3 = Series([5, 6], index=['f','g'])In[26]: pd.concat([s1,s2,s3])Out[26]: a 0b 1c 2d 3e 4f 5g 6dtype: int64 默认情况下，concat是在axis=0（行）上工作的，最终产生一个新的Series。如果传入axis=1（列），则变成一个DataFrame。 12345678910In[27]: pd.concat([s1,s2,s3], axis=1)Out[27]: 0 1 2a 0 NaN NaNb 1 NaN NaNc NaN 2 NaNd NaN 3 NaNe NaN 4 NaNf NaN NaN 5g NaN NaN 6 DataFrame连接 123456dfs = []for classify in classify_finance + classify_other: sql = \"select classify, tags from &#123;&#125; where classify='&#123;&#125;' length(tags)&gt;0 limit 1000\".format(mysql_table_sina_news_all, classify) df = pd.read_sql(sql,engine) dfs.append(df)df_all = pd.concat(dfs, ignore_index=True) 七、数据转换数据过滤、清理以及其他的转换工作。 1，移除重复数据（去重）duplicated()DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行： 1234567891011121314151617181920212223In[12]: df = DataFrame(&#123;'k1':['one']*3 + ['two']*4, 'k2':[1,1,2,3,3,4,4]&#125;)In[13]: dfOut[13]: k1 k20 one 11 one 12 one 23 two 34 two 35 two 46 two 4In[14]: df.duplicated()Out[14]: 0 False1 True2 False3 False4 True5 False6 Truedtype: bool drop_duplicates()1234567In[15]: df.drop_duplicates()Out[15]: k1 k20 one 12 one 23 two 35 two 4 2，利用函数或映射进行数据转换对于数据: 1234567891011121314In[16]: df = DataFrame(&#123;'food':['bacon','pulled pork','bacon','Pastraml','corned beef', 'Bacon', 'pastraml','honey ham','nova lox'],'ounces':[4,3,12,6,7.5,8,3,5,6]&#125;)In[17]: dfOut[17]: food ounces0 bacon 4.01 pulled pork 3.02 bacon 12.03 Pastraml 6.04 corned beef 7.55 Bacon 8.06 pastraml 3.07 honey ham 5.08 nova lox 6.0 增加一列表示该肉类食物来源的动物类型，先编写一个肉类到动物的映射： 123456In[18]: meat_to_animal = &#123;'bacon':'pig', 'pulled pork':'pig', 'pastraml':'cow', 'corned beef':'cow', 'honey ham':'pig', 'nova lox':'salmon'&#125; mapSeries的map方法可以接受一个函数或含有映射关系的字典型对象。 12345678910111213In[20]: df['animal'] = df['food'].map(str.lower).map(meat_to_animal)In[21]: dfOut[21]: food ounces animal0 bacon 4.0 pig1 pulled pork 3.0 pig2 bacon 12.0 pig3 Pastraml 6.0 cow4 corned beef 7.5 cow5 Bacon 8.0 pig6 pastraml 3.0 cow7 honey ham 5.0 pig8 nova lox 6.0 salmon 也可传入一个函数，一次性处理： 123456789101112In[22]: df['food'].map(lambda x : meat_to_animal[x.lower()])Out[22]: 0 pig1 pig2 pig3 cow4 cow5 pig6 cow7 pig8 salmonName: food, dtype: object apply 和 applymap对于DataFrame： 123456789In[21]: df = DataFrame(np.random.randn(4,3), columns=list('bde'),index=['Utah','Ohio','Texas','Oregon'])In[22]: dfOut[22]: b d eUtah 1.654850 0.594738 -1.969539Ohio 2.178748 1.127218 0.451690Texas 1.209098 -0.604432 -1.178433Oregon 0.286382 0.042102 -0.345722 apply将函数应用到由各列或行所形成的一维数组上。 作用到列： 1234567In[24]: f = lambda x : x.max() - x.min()In[25]: df.apply(f)Out[25]: b 1.892366d 1.731650e 2.421229dtype: float64 作用到行/轴： 1234567In[26]: df.apply(f, axis=1)Out[26]: Utah 3.624390Ohio 1.727058Texas 2.387531Oregon 0.632104dtype: float64 作用到每个元素： 123456789In[70]: frame = DataFrame(np.random.randn(4,3), columns=list('bde'),index=['Utah','Ohio','Texas','Oregon'])In[72]: frame.applymap(lambda x : '%.2f' % x)Out[72]: b d eUtah 1.19 1.56 -1.13Ohio 0.10 -1.03 -0.04Texas -0.22 0.77 -0.73Oregon 0.22 -2.06 -1.25 numpy的ufuncsNumpy的ufuncs（元素级数组方法）也可用于操作pandas对象。 取绝对值操作 123456In[23]: np.abs(df)Out[23]: b d eUtah 1.654850 0.594738 1.969539Ohio 2.178748 1.127218 0.451690Texas 1.209098 0.604432 1.178433 3，替换值替换的几种形式 123456789101112131415161718192021222324252627282930313233343536373839404142In[23]: se = Series([1, -999, 2, -999, -1000, 3])In[24]: se.replace(-999, np.nan)Out[24]: 0 11 NaN2 23 NaN4 -10005 3dtype: float64In[25]: se.replace([-999, -1000], np.nan)Out[25]: 0 11 NaN2 23 NaN4 NaN5 3dtype: float64In[26]: se.replace([-999, -1000], [np.nan, 0])Out[26]: 0 11 NaN2 23 NaN4 05 3dtype: float64# 字典In[27]: se.replace(&#123;-999:np.nan, -1000:0&#125;)Out[27]: 0 11 NaN2 23 NaN4 05 3dtype: float64 4，重命名轴索引、列名对于数据: 12345678In[28]: df = DataFrame(np.arange(12).reshape((3,4)), index = ['Ohio', 'Colorado', 'New York'], columns=['one','two','three', 'four'])In[29]: dfOut[29]: one two three fourOhio 0 1 2 3Colorado 4 5 6 7New York 8 9 10 11 就地修改轴索引 1234567In[30]: df.index = df.index.map(str.upper)In[31]: dfOut[31]: one two three fourOHIO 0 1 2 3COLORADO 4 5 6 7NEW YORK 8 9 10 11 如果要创建数据集的转换版（而不是修改原始数据），比较实用的方法是rename： 123456In[32]: df.rename(index=str.title, columns=str.upper)Out[32]: ONE TWO THREE FOUROhio 0 1 2 3Colorado 4 5 6 7New York 8 9 10 11 特别说明一下，rename可以结合字典型对象实现对部分轴标签的更新： 123456In[33]: df.rename(index=&#123;'OHIO':'INDIANA'&#125;, columns=&#123;'three':'peekaboo'&#125;)Out[33]: one two peekaboo fourINDIANA 0 1 2 3COLORADO 4 5 6 7NEW YORK 8 9 10 11 如果希望就地修改某个数据集，传入inplace=True即可： 1234567In[34]: _ = df.rename(index=&#123;'OHIO':'INDIANA'&#125;, inplace=True)In[35]: dfOut[35]: one two three fourINDIANA 0 1 2 3COLORADO 4 5 6 7NEW YORK 8 9 10 11 5，离散化和面元划分pd.cut为了便于分析，连续数据常常离散化或拆分为“面元”（bin）。比如： 1In [106]: ages = [20, 22,25,27,21,23,37,31,61,45,41,32] 需要将其划分为“18到25”, “26到35”，“36到60”以及“60以上”几个面元。要实现该功能，需要使用pandas的cut函数。 1234567n[37]: bins = [18, 25, 35, 60, 100]In[38]: cats = pd.cut(ages, bins)In[39]: catsOut[39]: [(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]Length: 12Categories (4, object): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]] 可以通过right=False指定哪端是开区间或闭区间。 12345678In[41]: cats = pd.cut(ages, bins, right=False)In[42]: catsOut[42]: [[18, 25), [18, 25), [25, 35), [25, 35), [18, 25), ..., [25, 35), [60, 100), [35, 60), [35, 60), [25, 35)]Length: 12Categories (4, object): [[18, 25) &lt; [25, 35) &lt; [35, 60) &lt; [60, 100)] 也可以指定面元的名称： 12345678910111213141516171819In[43]: group_name = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']In[45]: cats = pd.cut(ages, bins, labels=group_name)In[47]: catsOut[47]: [Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult]Length: 12Categories (4, object): [Youth &lt; YoungAdult &lt; MiddleAged &lt; Senior]In[46]: pd.value_counts(cats)Out[46]: Youth 5MiddleAged 3YoungAdult 3Senior 1dtype: int64 pd.qcutqcut是一个非常类似cut的函数，它可以根据样本分位数对数据进行面元划分，根据数据的分布情况，cut可能无法使各个面元中含有相同数量的数据点，而qcut由于使用的是样本分位数，可以得到大小基本相等的面元。 123456789101112131415161718In[48]: data = np.random.randn(1000)In[49]: cats = pd.qcut(data, 4)In[50]: catsOut[50]: [(0.577, 3.564], (-0.729, -0.0341], (-0.729, -0.0341], (0.577, 3.564], (0.577, 3.564], ..., [-3.0316, -0.729], [-3.0316, -0.729], (-0.0341, 0.577], [-3.0316, -0.729], (-0.0341, 0.577]]Length: 1000Categories (4, object): [[-3.0316, -0.729] &lt; (-0.729, -0.0341] &lt; (-0.0341, 0.577] &lt; (0.577, 3.564]]In[51]: pd.value_counts(cats)Out[51]: (0.577, 3.564] 250(-0.0341, 0.577] 250(-0.729, -0.0341] 250[-3.0316, -0.729] 250dtype: int64 6，检测和过滤异常值异常值（oulier）的过滤或变换运算在很大程度上其实就是数组运算。 对于数据： 12345678910111213In[52]: np.random.seed(12345)In[53]: data = DataFrame(np.random.randn(1000,4))In[54]: data.describe()Out[54]: 0 1 2 3count 1000.000000 1000.000000 1000.000000 1000.000000mean -0.067684 0.067924 0.025598 -0.002298std 0.998035 0.992106 1.006835 0.996794min -3.428254 -3.548824 -3.184377 -3.74535625% -0.774890 -0.591841 -0.641675 -0.64414450% -0.116401 0.101143 0.002073 -0.01361175% 0.616366 0.780282 0.680391 0.654328max 3.366626 2.653656 3.260383 3.927528 找出某列绝对值大于3的值 1234567In[55]: col = data[3]In[56]: col[np.abs(col) &gt; 3]Out[56]: 97 3.927528305 -3.399312400 -3.745356Name: 3, dtype: float64 要选出全部含有“超过3或-3的值”的行，可以利用布尔型DataFrame以及any方法： 1234567891011121314In[60]: data[(np.abs(data)&gt;3).any(1)]Out[60]: 0 1 2 35 -0.539741 0.476985 3.248944 -1.02122897 -0.774363 0.552936 0.106061 3.927528102 -0.655054 -0.565230 3.176873 0.959533305 -2.315555 0.457246 -0.025907 -3.399312324 0.050188 1.951312 3.260383 0.963301400 0.146326 0.508391 -0.196713 -3.745356499 -0.293333 -0.242459 -3.056990 1.918403523 -3.428254 -0.296336 -0.439938 -0.867165586 0.275144 1.179227 -3.184377 1.369891808 -0.362528 -3.548824 1.553205 -2.186301900 3.366626 -2.372214 0.851010 1.332846 根据这些条件，可以轻松对值进行设置，下面代码将值限制在区间-3到3以内： 123456789101112In[62]: data[np.abs(data)&gt;3] = np.sign(data)*3In[63]: data.describe()Out[63]: 0 1 2 3count 1000.000000 1000.000000 1000.000000 1000.000000mean -0.067623 0.068473 0.025153 -0.002081std 0.995485 0.990253 1.003977 0.989736min -3.000000 -3.000000 -3.000000 -3.00000025% -0.774890 -0.591841 -0.641675 -0.64414450% -0.116401 0.101143 0.002073 -0.01361175% 0.616366 0.780282 0.680391 0.654328max 3.000000 2.653656 3.000000 3.000000","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"pandas","slug":"pandas","permalink":"http://www.kekefund.com/tags/pandas/"},{"name":"DataFrame","slug":"DataFrame","permalink":"http://www.kekefund.com/tags/DataFrame/"},{"name":"数据分析","slug":"数据分析","permalink":"http://www.kekefund.com/tags/数据分析/"}]},{"title":"python 制作标签云","date":"2016-02-15T01:19:38.000Z","path":"2016/02/15/py-tag-cloud/","text":"标签云是比较直观的频率分布表现方式，很多网站和APP在年度盘点和总结时会使用。Python生成标签云有一个比较易用的库 pytagcloud。 1，导入头文件12from pytagcloud import create_tag_image, make_tagsfrom pytagcloud.lang.counter import get_tag_counts 2，生成标签云123456789def finance_cloud(): tag = 'cc xx xx china cc keke keke keke' tags = make_tags(get_tag_counts(tag),maxsize=100) # Set your output filename create_tag_image(tags,\"cloud.png\", size=(1280,800),background=(0, 0, 0, 255), fontname=\"SimHei\")finance_cloud() 生成的图片cloud.png可以指定尺寸size，设置背景background，指定字体fontname。 pytagcloud库默认的字体不支持中文，生成的图片中，中文是乱码。 解决办法是在py文件开始处指定图片输出的字体： 123from pylab import mplmpl.rcParams['font.sans-serif'] = ['SimHei']#['FangSong'] # 指定默认字体mpl.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题 3，字体名称Windows的字体对应名称黑体 SimHei微软雅黑 Microsoft YaHei微软正黑体 Microsoft JhengHei新宋体 NSimSun新细明体 PMingLiU细明体 MingLiU标楷体 DFKai-SB仿宋 FangSong楷体 KaiTi仿宋_GB2312 FangSong_GB2312楷体_GB2312 KaiTi_GB2312 宋体：SimSuncss中中文字体（font-family）的英文名称新細明體：PMingLiU細明體：MingLiU標楷體：DFKai-SB黑体：SimHei新宋体：NSimSun仿宋：FangSong楷体：KaiTi仿宋_GB2312：FangSong_GB2312楷体_GB2312：KaiTi_GB2312微軟正黑體：Microsoft JhengHei微软雅黑体：Microsoft YaHei装Office会生出来的一些：隶书：LiSu幼圆：YouYuan华文细黑：STXihei华文楷体：STKaiti华文宋体：STSong华文中宋：STZhongsong华文仿宋：STFangsong方正舒体：FZShuTi方正姚体：FZYaoti华文彩云：STCaiyun华文琥珀：STHupo华文隶书：STLiti华文行楷：STXingkai华文新魏：STXinwei Mac OS的字体名称：华文细黑：STHeiti Light [STXihei]华文黑体：STHeiti华文楷体：STKaiti华文宋体：STSong华文仿宋：STFangsong儷黑 Pro：LiHei Pro Medium儷宋 Pro：LiSong Pro Light標楷體：BiauKai蘋果儷中黑：Apple LiGothic Medium蘋果儷細宋：Apple LiSung Light 参考：http://www.it610.com/article/2569995.htm","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"pytagcloud","slug":"pytagcloud","permalink":"http://www.kekefund.com/tags/pytagcloud/"},{"name":"标签云","slug":"标签云","permalink":"http://www.kekefund.com/tags/标签云/"}]},{"title":"Python朴素贝叶斯分类器 - 自然语言处理","date":"2016-01-27T07:45:55.000Z","path":"2016/01/27/naive-bayes-classifier/","text":"概念朴素贝叶斯算法是一个直观的方法，使用每个属性属于某个类的概率来做预测。你可以使用这种监督性学习方法，对一个预测性建模问题进行概率建模。 给定一个类，朴素贝叶斯假设每个属性归属于此类的概率独立于其余所有属性，从而简化了概率的计算。这种强假定产生了一个快速、有效的方法。给定一个属性值，其属于某个类的概率叫做条件概率。对于一个给定的类值，将每个属性的条件概率相乘，便得到一个数据样本属于某个类的概率。 1、贝叶斯定理假设对于某个数据集，随机变量C表示样本为C类的概率，F1表示测试样本某特征出现的概率，套用基本贝叶斯公式，则如下所示： 上式表示对于某个样本，特征F1出现时，该样本被分为C类的条件概率。 对该公式，有几个概念需要熟知： 先验概率（Prior） 。P(C)是C的先验概率，可以从已有的训练集中计算分为C类的样本占所有样本的比重得出。 证据（Evidence）。即上式P(F1)，表示对于某测试样本，特征F1出现的概率。同样可以从训练集中F1特征对应样本所占总样本的比例得出。 似然（likelihood）。即上式P(F1 | C)，表示如果知道一个样本分为C类，那么他的特征为F1的概率是多少。 对于多个特征而言，贝叶斯公式可以扩展如下： 分子中存在一大串似然值。当特征很多的时候，这些似然值的计算是极其痛苦的。 2、朴素的概念为了简化计算，朴素贝叶斯算法假设：“朴素的认为各个特征相互独立”。这样一来，上式的分子就简化成了： 这个假设是认为各个特征之间是独立的，看上去确实是个很不科学的假设。因为很多情况下，各个特征之间是紧密联系的。然而在朴素贝叶斯的大量应用实践表明其工作的相当好。 其次，由于朴素贝叶斯的工作原理是计算","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"贝叶斯分类器","slug":"贝叶斯分类器","permalink":"http://www.kekefund.com/tags/贝叶斯分类器/"},{"name":"nltk","slug":"nltk","permalink":"http://www.kekefund.com/tags/nltk/"},{"name":"贝叶斯原理","slug":"贝叶斯原理","permalink":"http://www.kekefund.com/tags/贝叶斯原理/"}]},{"title":"人工智能和系统化交易招聘","date":"2016-01-26T07:48:31.000Z","path":"2016/01/26/js-recruit/","text":"某大型资产管理公司人工智能和系统化交易招聘简历发送到： [js_recruit@163.com] 1． 人工智能/机器学习算法研究员学历要求：博士职位数量：2人 岗位职责： 使用统计学习方法，挖掘投资者行为特征的数据。 对金融海量数据，针对不同的学习问题，建立起一般性的数据源选择框架。 对金融相关分类和预测性问题建模，并负责系统回测。 与技术人员合作，参与大数据计算框架的设计和运行 职位要求： 具有人工智能，机器学习方面较为深厚的理论研究背景。深入理解机器学习的各种算法。能独立从事大数据和人工智能方面的研究工作。 对数据的预处理，分类，预测等方面有实践经验。熟练使用R，Python，C/C++, Java, Scala 等编程语言中至少一种。 工作勤奋，有团队精神，能承担一定的工作压力。 2. 数据管理员学历要求：本科以上职位数量：1人 岗位职责： 负责团队数据库的运行和维护，管理，扩展工作 根据业务需求，参与数据库的架构设计和数据结构的优化 负责数据库的日常监控，维护，备份和恢复 研究系统可能存在的性能瓶颈并进行优化 背景要求： 具有数据库运维的3年经验。熟悉Oracle，MySQL, SQLServer等各种数据库的存储，查询，和优化。 有一定的脚本语言开发能力，能用C/C++，Java，Python 其中至少一种开发数据库接口。有一定的大数据运行经验，能配合开发人员优化数据库访问语句。 了解非关系型数据库NoSQL的工作原理。 工作细致耐心，踏实敬业，有团队合作精神。 3． 大数据架构技术经理学历要求：本科以上职位数量：1人 岗位职责： 根据项目需求，分析，设计，并实现系统的架构方案。使系统架构具有合理性和可扩展性 控制大数据项目的架构质量，协助解决项目开发过程中的技术困难。 追踪大数据和云计算技术的最新科技成果，并协调团队应用于金融实践 参与代码的实现，并编写技术文档，对通用技术实现复用。 背景要求： 有独立开发和部署分布式存储和计算架构，比如Hadoop，Hive，Pig，Spark的经验。熟悉海量数据的查询和计算。 精通C/C++, Scala, Python, Java至少两种编程语言，有较强的分布式计算基础和软件工程能力 精通Linux操作系统， 对开源软件，云计算，数据仓库类产品有比较深入的认识。 良好的沟通协调能力。 4． 大数据工程师学历要求：本科以上职位数量：1~2人 岗位职责： 参与开发和建设实时、离线的数据存储和计算平台，并在该平台上实现数据的预测，分类和展现。 参与构建金融类相关数据业务模型，并使用并行式计算架构实现和优化。 参与开发数据可视化过程。 背景要求： 精通C/C++, Scala, Python, Java中至少两种编程语言，有较强的软件工程能力，理解面向对象编程思想（类，对象，重载，继承，多态）。 了解分布式存储和计算架构，比如Hadoop，Hive，Pig，Spark。 精通Linux操作系统和Shell的使用， 会编写script. 熟练使用关系型数据库， 熟悉Oracle，MySQL，SQLServer等各种数据库的存储和优化。 5． 金融工程研究员学历要求：硕士或以上职位数量：1人 岗位职责： 设计和优化二级市场被动性和主动性投资策略，并负责系统回测 与团队成员合作，设计金融数据挖掘逻辑 参与金融产品的开发和风险控制 职位要求： 深入理解二级市场投资的各种产品，包括股票，债券，期货，指数基金等 理解实证资产定价过程，对于量化投资有一定了解。 理解公司财务理论，并能对上市公司进行财务分析。 能够使用Matlab，R，等任意一种脚本语言，编写程序 简历发送到： js_recruit@163.com","tags":[{"name":"招聘","slug":"招聘","permalink":"http://www.kekefund.com/tags/招聘/"},{"name":"机器学习","slug":"机器学习","permalink":"http://www.kekefund.com/tags/机器学习/"},{"name":"人工智能","slug":"人工智能","permalink":"http://www.kekefund.com/tags/人工智能/"},{"name":"系统化交易","slug":"系统化交易","permalink":"http://www.kekefund.com/tags/系统化交易/"}]},{"title":"python多线程与多进程 超简单使用","date":"2016-01-22T07:47:07.000Z","path":"2016/01/22/python-multiprocess/","text":"Python 的GIL限制了多核CPU的性能，对于IO密集型的程序，采用多线程能显著提高运行速度；但对于计算密集型的程序，多线程就没多少用了，采用多进程编程，就能充分利用多核CPU的性能，CPU占用率能达到100%。 下面是在阿里云服务器上测试的数据： 配置：CPU：Xeon, E5-2680, 2.5GHz, 4核; 内存：16G, DDR4; 硬盘：100G, SSD 123456789101112131415161718192021def run(): pool = multiprocessing.Pool(processes = 8) result = [] contents = [] for ix, row in df.iterrows(): content = row['title'] + \" \" + row['content'] contents.append(content) result = pool.map(get_one_article_keys, contents) pool.close() pool.join() t1 = time.time() print 'time pass:&#123;:.3f&#125;'.format(t1-t0)def get_one_article_keys(content): try: tags2 = jieba.analyse.textrank(content, topK=20) print multiprocessing.current_process() return ','.join(tags2) except Exception,e: print \"get_one_article_keys():%s\" % str(e) return '' 执行计算密集型任务的结果： multi_way Processes Time(s) 多进程 4 378 多进程 8 381 多进程 20 464 多线程 8 3174 对于一台四核的机器，设置进程数为对应的内核数，效率是最高的；当设置比内核多的进程时，在创建python进程时开销占时比较多，造成设置为20个进程数时，时间比4个进程多了1分钟多。因此，设置进程数与实际内核数相同，运行最快。 获取内核数: multiprocessing.cpu_count() 一、多进程编程1，map方式123456789101112131415import multiprocessingimport timedef do_something(i): time.sleep(i) print 'good:%d' % i return 'good:%d' % iprint multiprocessing.cpu_count()pool = multiprocessing.Pool(processes = 4)result = pool.map(do_something, range(10)）pool.close()pool.join()for res in result: print res result 可以得到多进程执行do_something返回的结果，为list 1，map方式为阻塞模式，主进程必须等待所有子进程执行完毕了才能继续；2，还有一种方式为非阻塞模式，map_async，主进程不等待子进程是否完毕，接着向下执行。 2，apply 方式12345...result = []for i in range(10): result.append(pool.apply(do_something, (i,)))... 同map_async一样，还有非阻塞模式 apply_async。 二、多线程编程多线程实现也非常简单，跟多进程基本一样，只是创建pool时略有不同。1234567891011121314from multiprocessing.dummy import Pool as ThreadPoolimport timedef do_something(i): time.sleep(i) print 'good:%d' % i return 'good:%d' % ipool = ThreadPool(processes = 4)result = pool.map(do_something, range(10)）pool.close()pool.join()for res in result: print res","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"multiprocessing","slug":"multiprocessing","permalink":"http://www.kekefund.com/tags/multiprocessing/"},{"name":"多进程","slug":"多进程","permalink":"http://www.kekefund.com/tags/多进程/"},{"name":"多线程","slug":"多线程","permalink":"http://www.kekefund.com/tags/多线程/"}]},{"title":"网页爬虫之cookie自动获取","date":"2016-01-21T01:05:36.000Z","path":"2016/01/21/spider-cookie/","text":"本文实现cookie的自动获取，及cookie过期自动更新。 社交网站中的很多信息需要登录才能获取到，以微博为例，不登录账号，只能看到大V的前十条微博。保持登录状态，必须要用到Cookie。以登录www.weibo.cn 为例：在chrome中输入：http://login.weibo.cn/login/ 分析控制台的Headers的请求返回，会看到weibo.cn有几组返回的cookie。 实现步骤： 1，采用selenium自动登录获取cookie，保存到文件; 2，读取cookie，比较cookie的有效期，若过期则再次执行步骤1； 3，在请求其他网页时，填入cookie，实现登录状态的保持。 1，在线获取cookie采用selenium + PhantomJS 模拟浏览器登录，获取cookie； cookies一般会有多个，逐个将cookie存入以.weibo后缀的文件。 12345678910111213141516171819202122232425def get_cookie_from_network(): from selenium import webdriver url_login = 'http://login.weibo.cn/login/' driver = webdriver.PhantomJS() driver.get(url_login) driver.find_element_by_xpath('//input[@type=\"text\"]').send_keys('your_weibo_accout') # 改成你的微博账号 driver.find_element_by_xpath('//input[@type=\"password\"]').send_keys('your_weibo_password') # 改成你的微博密码 driver.find_element_by_xpath('//input[@type=\"submit\"]').click() # 点击登录 # 获得 cookie信息 cookie_list = driver.get_cookies() print cookie_list cookie_dict = &#123;&#125; for cookie in cookie_list: #写入文件 f = open(cookie['name']+'.weibo','w') pickle.dump(cookie, f) f.close() if cookie.has_key('name') and cookie.has_key('value'): cookie_dict[cookie['name']] = cookie['value'] return cookie_dict 2，从文件中获取cookie从当前目录中遍历以.weibo结尾的文件，即cookie文件。采用pickle解包成dict，比较expiry值与当前时间，若过期则返回为空； 123456789101112131415161718def get_cookie_from_cache(): cookie_dict = &#123;&#125; for parent, dirnames, filenames in os.walk('./'): for filename in filenames: if filename.endswith('.weibo'): print filename with open(self.dir_temp + filename, 'r') as f: d = pickle.load(f) if d.has_key('name') and d.has_key('value') and d.has_key('expiry'): expiry_date = int(d['expiry']) if expiry_date &gt; (int)(time.time()): cookie_dict[d['name']] = d['value'] else: return &#123;&#125; return cookie_dict 3，若缓存cookie过期，则再次从网络获取cookie1234567def get_cookie(): cookie_dict = get_cookie_from_cache() if not cookie_dict: cookie_dict = get_cookie_from_network() return cookie_dict 4，带cookie请求微博其他主页123456789101112131415def get_weibo_list(self, user_id): import requests from bs4 import BeautifulSoup as bs cookdic = get_cookie() url = 'http://weibo.cn/stocknews88' headers = &#123;'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.57 Safari/537.36'&#125; timeout = 5 r = requests.get(url, headers=headers, cookies=cookdic,timeout=timeout) soup = bs(r.text, 'lxml') ... # 用BeautifulSoup 解析网页 ...","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"requests","slug":"requests","permalink":"http://www.kekefund.com/tags/requests/"},{"name":"cookie","slug":"cookie","permalink":"http://www.kekefund.com/tags/cookie/"},{"name":"crawler","slug":"crawler","permalink":"http://www.kekefund.com/tags/crawler/"},{"name":"selenium","slug":"selenium","permalink":"http://www.kekefund.com/tags/selenium/"}]},{"title":"requests初步使用","date":"2016-01-07T08:52:33.000Z","path":"2016/01/07/use-reqeusts/","text":"基本用法一、发送无参数的get请求1234567891011121314151617import requestsIn [67]: r =requests.get('http://httpbin.org/get')In [68]: print r.text&#123; \"args\": &#123;&#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.7.0 CPython/2.7.10 Darwin/14.5.0\" &#125;, \"origin\": \"220.231.47.169\", \"url\": \"http://httpbin.org/get\"&#125; 返回一个名为 r 的Response对象。可以从这个对象中获取所有我们想要的信息。 二、发送带参数的get请求将key与value放入一个字典中，通过params来传递，其作用相当于urllib.urlencode 12345In [69]: pyqload = &#123;'q':'cbb'&#125;In [70]: r = requests.get('http://www.so.com/s', params=pqyload, , timeout=10) # 10sIn [71]: r.urlOut[71]: u'http://www.haosou.com/s?q=%E7%AB%A0%E6%A5%A0%E6%A5%A0' 注意字典里值为 None 的键都不会被添加到 URL 的查询字符串里。 三、发送post请求，通过data参数来传递123456789101112131415161718192021222324In [72]: payload = &#123;'a':'嘎子', 'b':'hello'&#125;In [73]: r = requests.post('http://httpbin.org/post', data=payload)In [74]: print r.text&#123; \"args\": &#123;&#125;, \"data\": \"\", \"files\": &#123;&#125;, \"form\": &#123; \"a\": \"\\u560e\\u5b50\", \"b\": \"hello\" &#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Content-Length\": \"28\", \"Content-Type\": \"application/x-www-form-urlencoded\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.7.0 CPython/2.7.10 Darwin/14.5.0\" &#125;, \"json\": null, \"origin\": \"220.231.47.169\", \"url\": \"http://httpbin.org/post\"&#125; data不仅可以接受字典类型的数据，还可以接受json等格式： 1234567891011121314151617181920212223In [75]: import jsonIn [76]: r = requests.post('http://httpbin.org/post', data=json.dumps(payload))In [77]: print r.text&#123; \"args\": &#123;&#125;, \"data\": \"&#123;\\\"a\\\": \\\"\\\\u560e\\\\u5b50\\\", \\\"b\\\": \\\"hello\\\"&#125;\", \"files\": &#123;&#125;, \"form\": &#123;&#125;, \"headers\": &#123; \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip, deflate\", \"Content-Length\": \"35\", \"Host\": \"httpbin.org\", \"User-Agent\": \"python-requests/2.7.0 CPython/2.7.10 Darwin/14.5.0\" &#125;, \"json\": &#123; \"a\": \"\\u560e\\u5b50\", \"b\": \"hello\" &#125;, \"origin\": \"220.231.47.169\", \"url\": \"http://httpbin.org/post\"&#125; 可以看出，json参数时直接存为字符串保存为data字段，而字典类型参数放在form表单中。 四、发送文件的post类型， 向网站上传一张图片，文档等1234In [78]: url = 'http://httpbin.org/post'In [79]: files = &#123;'file':open('touxiang.png','rb')&#125;In [80]: r = requests.post(url, files=files) 五、编码Requests会自动解码来自服务器的内容。大多数unicode字符集都能被无缝地解码 请求发出后，Requests会基于HTTP头部对响应的编码作出有根据的推测。当你访问 r.text 之时，Requests会使用其推测的文本编码。你可以找出Requests使用了什么编码，并且能够使用r.encoding 属性来改变它: 123456789101112In [11]: r = requests.get('http://www.baidu.com')In [12]: r.encodingOut[12]: 'utf-8'In [18]: r.encoding = 'GBK' 六、保存图片：二进制响应内容对于非文本请求，以字节的方式访问请求响应体。Requests会自动为你解码gzip和deflate传输编码的响应数据。 12345678910In[25]: from PIL import ImageIn[26]: from StringIO import StringIOIn[27]: r = requests.get('http://7xo67b.com1.z0.glb.clouddn.com/1449027323885.jpg')In[28]: i = Image.open(StringIO(r.content))In[30]: i.show() 显示图片: 七、JSON响应内容Requests中也有一个内置的JSON解码器，助你处理JSON数据: 1234567891011In[31]: r = requests.get('https://github.com/timeline.json')In[32]: r.json()Out[32]: &#123;u'documentation_url': u'https://developer.github.com/v3/activity/events/#list-public-events', u'message': u'Hello there, wayfaring stranger. If you\\u2019re reading this then you probably didn\\u2019t see our blog post a couple of years back announcing that this API would go away: http://git.io/17AROg Fear not, you should be able to get what you need from the shiny new Events API instead.'&#125; 如果JSON解码失败， r.json 就会抛出一个异常。例如，相应内容是 401 (Unauthorized) ，尝试访问 r.json 将会抛出 ValueError: No JSON object could be decoded 异常。 八、响应状态码123456In[40]: r = requests.get('http://httpbin.org/get')In[41]: r.status_codeOut[41]: 200 九、Cookies获取cookies 1234In[49]: r.cookiesOut[49]: &lt;RequestsCookieJar[]&gt; 发送cookies到服务器 12345678910url = 'http://httpbin.org/cookies'In[45]: cookies = dict(cookies_are='working')In[47]: r = requests.get(url, cookies=cookies)In[48]: r.textOut[48]: u'&#123;\\n \"cookies\": &#123;\\n \"cookies_are\": \"working\"\\n &#125;\\n&#125;\\n' 十、错误与异常遇到网络问题（如：DNS查询失败、拒绝连接等）时，Requests会抛出一个 ConnectionError 异常。 遇到罕见的无效HTTP响应时，Requests则会抛出一个 HTTPError 异常。 若请求超时，则抛出一个 Timeout 异常。 若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。 所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。 高级用法一、会话对象会话对象让你能够跨请求保持某些参数。它也会在同一个Session实例发出的所有请求之间保持cookies。 跨请求保留一些cookies12345678910111213In[50]: s = requests.Session()In[51]: s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')Out[51]: &lt;Response [200]&gt;In[52]: r = s.get('http://httpbin.org/cookies')In[53]: r.textOut[53]: u'&#123;\\n \"cookies\": &#123;\\n \"sessioncookie\": \"123456789\"\\n &#125;\\n&#125;\\n' 会话也可用来为请求方法提供缺省数据。这是通过为会话对象的属性提供数据来实现的。12345678s = requests.Session()s.auth = ('user', 'pass')s.headers.update(&#123;'x-test': 'true'&#125;)# both 'x-test' and 'x-test2' are sents.get('http://httpbin.org/headers', headers=&#123;'x-test2': 'true'&#125;) 二、代理123456789import requestsproxies = &#123; \"http\": \"http://10.10.1.10:3128\", \"https\": \"http://10.10.1.10:1080\",&#125;requests.get(\"http://example.org\", proxies=proxies) 你也可以通过环境变量 HTTP_PROXY 和 HTTPS_PROXY 来配置代理。 123456$ export HTTP_PROXY=\"http://10.10.1.10:3128\"$ export HTTPS_PROXY=\"http://10.10.1.10:1080\"$ python&gt;&gt;&gt; import requests&gt;&gt;&gt; requests.get(\"http://example.org\") 若你的代理需要使用HTTP Basic Auth，可以使用 http://user:password@host/ 语法: 1234proxies = &#123; \"http\": \"http://user:pass@10.10.1.10:3128/\",&#125; 参考 http://www.yangyanxing.com/?p=1079 http://docs.python-requests.org/zh_CN/latest/user/advanced.html#advanced","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"requests","slug":"requests","permalink":"http://www.kekefund.com/tags/requests/"}]},{"title":"IP代理池的实现框架(安装包)","date":"2016-01-04T07:58:00.000Z","path":"2016/01/04/python-ip-proxy-frame/","text":"上一篇 IP代理池的实现 讲解了IP代理池的实现细节。 由于爬虫多个项目都需要用到IP代理，打造一个公用的IP代理库就很有必要。本文主要讲解公用的IP代理库的实现框架。 实现思路如下：1，数据抓取：从各个IP代理网站抓取大量IP数据；2，数据筛选：Ping每个IP，连接速度","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.kekefund.com/tags/爬虫/"},{"name":"IP代理","slug":"IP代理","permalink":"http://www.kekefund.com/tags/IP代理/"}]},{"title":"日本蜡烛线理论","date":"2015-12-17T08:11:06.000Z","path":"2015/12/17/japan-candle-diagram-theory/","text":"一、基本理论及原则 信息量 “信息量”的概念对应着包含在价格数据中的“含金量”： 任何市场资料所信息量均有限 任何技术分析手段所提取的信息量只可能少于信息量的极限，绝不可能多于信息量的极限； 对于相同的市场资料，虽然可能采取不同的技术分析手段，但是所得信息量均限于上述极限之内，因此它们的结果有可能是相互重叠的。 杰西·利弗莫尔：只有离开研究对象一定的距离，才能更好地观察它的全貌。技术分析就是让我们退后一步来观察市场，如此才使我们对市场获得了一份不同寻常的，或许也更加贴切的观感。 如果我们要观察总体的供给–需求关系，那么，观察价格变化显然是最直观、最容易进行的一种方法，有些基本面的消息，普通的投资大众可能根本无缘得知，但是您可以正确地预期，它们一定已经包含在价格信息之内了。如果有人先于大家掌握了某种推动市场变化的情报，那么，他极可能抢先在市场上买进或者卖出，直到价格变化抵消了他的情报才会罢手。如此一来，在有些情况下，这类消息可能早在当初事件的时候就被市场消化吸收掉了。一言以蔽之，当前的市场价格应当充分反应了当前发送的一切市场信息，不论这些信息是普通大众已经知晓的，还是仅仅掌握在极少数人手重的。 “市场永远不会错”，绝不可带着“市场错了”的成见进行交易。 千万不要将自己的主观臆断强加于市场。举例来说，即使您坚定地判断原油市场即将上涨也必须等到市场趋势果真向上出头的时候才能买进。 绝不可将自己的意愿强加于市场，一定要做一个追随趋势者，不要做一个预测趋势者。如果您怀着看涨的预期，那么就在上升趋势中入市；如果您持有看跌的预期，那么就在下降趋势中入市。 采取保护性止损措施的重要性 我们应当在最初建立交易头寸的时候，就设置好止损水平，只有在这个时候，我们才是最冷静、客观的。只有在市场的演变符合我们本来的预期的条件下才能继续持有当初的交易头寸。如果后来的价格变化与我们的预期相反，或者哪怕只是未能证实我们的预期，届时应当当机立断，平仓出市。 - 请不要忘记下列两项事实： &gt; 1. 所有的长期趋势都是由短期趋势开头的 &gt; 2. 市场上绝无一厢情愿的余地，市场自行其是，既不在意您的想法，也不关心您的头寸。 市场并不在乎您是不是顺从了它的趋势。有一件事比犯错误还要命，那便是坚持错误。宁可放弃您的高见，不要丧失您的金钱。有能力及早纠正自己的错误，其实是一件值得自豪的事。被市场落实了止损指令，就意味着认错服输。人们对承认错误深恶痛绝，因为这里面往往牵扯上了个人的自尊心和名声等。优秀的交易商从不过于固执己见。据说，沃伦巴菲特信奉两项原则： 保全资本金。 绝不可忘记原则1. 拿日本人的话来说，“只要钓得上蛙鱼，丢个把鱼钩是值得的。”如果您被止损出市，那就当失去一把鱼钩，没准，下一钩就能钓上一条大家伙。 下面这句话常常挂在房地产经纪人的嘴边：“对房地产最有影响力的三项因素是：地点，地点，地点。”。如果我们把这句话借用到我们这个行业来，那就是，市场最重要的三个方面是：趋势、趋势、趋势。 二、技术指标1，极限转换原则日本人有句谚语：“大红的真漆盘子无需另加装饰”。这种“简单的就是美好的”的概念，道破了市场技术分析理论的真谛。 这一原则既简单明白，又犀利、得力——过去的支撑水平演化为新的阻挡水平；过去的阻挡水平演化为新的支撑水平。这就是所说的“极性转换原则”。 下图左卫支撑水平转化为阻挡水平的情形。下图右室过去的阻挡水平转化为新的支撑水平的情形。 极性转换现象的技术潜力的大小与以下几个方面成正比： 1、市场对过去的支撑/阻挡水平曾进行试探的次数； 2、每次试探时的交易量、持仓量的大小。 2，百分比回撤水平通常，市场既不会直线上升，也不会径直跌落，而是进两步、退一步。在当前趋势继续发展之前，市场通常先要对已经形成的上涨进程或下跌进程作出一定程度的回撤。 在这类回撤水平中，较为常用的是50%回撤水平，以及38%和62%的斐波那契回撤水平。 3，移动平均线在比较流行的移动平均线中，对短线的交易商来说，包括4天、9天、18天移动平均线；对操作长期头寸的市场参与者来说，包括13周、26周、40周移动平均线。 在日本，13周和40周移动平均线最为常用。 移动平均线的用法 1、通过比较价格与移动平均线的相对位置，构成一个趋势目标。举例来说，如果我们判断市场正处于中期的上升趋势中，那么有一个很好的衡量标准，就看价格是不是居于65天移动平均线的上方。而对于更长期的上升趋势来说，价格必须高于40周移动平均线。 2、利用移动平均线构成支撑水平或阻挡水平。当收市价向上超越某一条特定的移动平均线时，可能构成看涨的信号。而当收市价向下低于某个移动平均线时，构成看跌的信号。 3、跟踪移动平均线波幅带（也称为包络线）。这些波幅带是将移动平均线向上或向下平移一定的百分比后形成的，它们也起到支撑或阻挡作用。 4、观察移动平均线的斜率。举例来说，如果移动平均线在一段持续稳步的上升之后转向水平发展，乃至开始下降，那么可能构成了一个看跌信号。在移动平均线上作趋势线，是监测其斜率变化的一个简单易行的办法。 5、利用双移动平均线系统来交易。 4，双移动平均线两根移动平均线组合在一起，可构造成一个超买/超卖指标，也就是摆动指数。 把较短期的移动平均线减去较长期的移动平均线，就得到了这个摆动指数的值。该指数既可以是正值，也可以为负值。 1、如果短期移动平均线相对来说较大幅度地高于（或低于）长期移动平均线，那么我们就认为市场处于超买状态（或超卖状态）。 2、如果短期移动平均线向上穿越了长期移动平均线，这就是一个看涨信号。在日本，这样的移动平均线交叉信号称为黄金交叉。与上述相反的情形，当短期移动平均线向下穿越长期移动平均线时，构成了一个看跌的死亡交叉信号。 5，相对力度指数在期货交易商中，相对力度指数（RSI）是最流行的技术工具之一。在RSI研究中9天和14天是最常用的两种时间参数。 如何计算RSI 首先分布累加出一定时间之内上涨的价格幅度之和与下跌的价格幅度之和，然后再求得两者的比值。在计算过程中，一般仅采用收市价作为价格数据。它的计算公式是： RSI = 100 - 100 / ( 1 + RS ) 其中，RS = 该时期上涨价格幅度的平均值/该时期内下跌价格幅度的平均值。 如何运用RSI RSI的两个主要用途是：构成超买/超卖指标，作为监测相互背离现象的工具。 当RSI向上趋近其取值范围的上边界时（即，当它高于70或80时），表明市场处于超买状态。在这样的情况下，市场也许变得比较脆弱，容易引发向下回落的过程，或者即将转入横向调整阶段；与此相反，当RSI处于其取值范围的下边界时（通常低于30或20），则认为RSI反映了超卖状态。在这样的环境下，市场有可能形成空头买入平仓行情。 利用RSI揭示相互背离现象的具体方法是：当价格向上创出当前趋势的新高时，RSI却没有形成新高，未能与之配合，这就构成了一个负面相互背离信号，可能是一个看跌信号；当价格向下跌出当前趋势的新低时，RSI却没有形成新低，则构成了正面的相互背离现象。 6，随机指数随机（摆动）指数，既提供了超买和超卖状态的读数指示，也能够揭示相互背离现象，还提供了一套比较短期趋势与长期趋势的机制。 随机指数把最近的收市价格，同一定时间范围内市场的总的价格范围进行比较。随机指数的数值处在0到100之间。 当随机指数的读数较高时，就意味着当前的收市价在一定时期的整个价格范围中接近上端的水平。 当随机指数的读数较低时，就意味着当前的收市价在一定时期的整个价格范围中接近下端的水平。 随机指数的设计思想是：当市场向上运动时，收市价格倾向于接近上述价格区间的高点；当市场向下运动时，价格往往集中在上述价格区间的低点附近。 如何计算随机指数 随机指数的图表由两根曲线组成，它们分别是%K线和%D线。其中，%K线称为原始随机线，或者称为快%K线。这条曲线最为灵敏，%K线值的计算公式是： %K =（收市价 - N天内的最低价） / （N天内的最高价 - N天内的最低价）* 100% N值，取9、14、21个时间单位是几个较为常见的时间参数。 以快%K线为基础，每三个数值计算出一个移动平均值，得出一条较为平滑的三时间单位移动平均线。这条%K线的三时间单位移动平均线就称为慢%K线。 以慢%K线再进行一次三时间单位的移动平均，得到了慢%K线的三时间单位移动平均线，称为%D线。 如何应用随机指数 当慢%K线向下穿过%D线时，构成卖出信号； 当慢%K线向上穿过%D线时，构成买入信号。 具体来说，为了构成一个买入信号，需要满足以下三个条件： 首先，市场必须处在超卖状态（随机指数的%D值为25%或更低）； 其次，市场已经发生了正面相互背离现象； 最后，慢%K线向上穿越了%D线。 7，动力指数又称为价格速度指数。该指数度量的是，当前的收市价与一定天数之前的收市价之差。如果我们研究10日动力指数，那么我们比较的就是今天的收市价与10日之前的收市价。 在上升趋势的发展过程中，上述价格差应当以逐步加快的速度增长。这样的情况显示了该上升趋势具有逐步增长的驱动力。如果价格上涨，而动力指数转为持平，那么说明当前的价格趋势处于减速状态。这可能构成了一个早期的警告信号，说明当前的价格趋势可能会结束。如果动力指数向下越过零线，则形成了一个看跌信号；而当它向上穿越零线时，则是一个看涨信号。 动力指数也是一种现成的超买/超卖指标。举例来说，当动力指数为一个相对较大的正值时，市场可能处在超买状态，容易发生回落行情。动力指数通常在价格达到顶峰之前预先达到顶点。根据这一特点，当动力指数处于极为超买的读数状况时，可能预示着一个价格高峰的形成。 动力指数的另一种用途是，它能够为市场的超买或超卖状体提供一个具体尺度。 8，交易量交易量的技术意义是，一段行情的交易量越重，则这段行情背后的市场力量越强大。只要交易量保持增长的态势，那么，当前的价格趋势就将持续下去。但是如果在价格趋势发展时，交易量不增反降，那么，判断当前趋势仍将继续发展的理由就不充分了。另外，我们还可以通过交易量来验证市场的顶部或底部过程。当市场向下试探某一支撑水平时，如果交易量较轻，则意味着卖出压力减弱了，因此构成了一个看涨信号。反过来，当市场向上试探先前的高点时，如果交易量较轻，则证明买进力量减弱了，构成了一个看跌信号。 9，权衡交易量（OBV）权衡交易量（OBV）是一种交易量的累计净值。从一个基准日开始，当某一天的收市价高于前一天的收市价时，则将该日的交易量数值加到前一天的权衡交易量数值上；如果某一天的收市价低于前一日的收市价，则该日的交易量就从前一天累计的权衡交易量数值中减去。OBV验证趋势。OBV应当朝着当前主要趋势的方向运动。如果价格与OBV双双上涨，则说明由买方引起的交易量正在上升，即便在较高的价位上，也能达成较高的交易量。这可能构成一个看涨信号。 如果情况与上述方向相反，价格与OBV一齐下跌，就反映出在较低的价位上，由卖方引起的交易量也在增长，说明价格下降的过程仍将持续。 在横向交易区间中，也可以应用OBV。如果在价格保持稳定时（最好是在较低的价位上保持稳定），OBV开始升高，这可能揭示了一个筹码搜集的过程。这种局面预示着上涨行情的到来。 如果价格持平运动，而OBV正在下跌，则反映出一个筹码派发的过程。这种局面可能具有看跌的意义，特别是当它发生在高价格水平时。 10，持仓量持仓量的作用与交易量有些类似，也有助于衡量价格运动背后的市场力度。根据持仓量的增减，我们就可以判断资金是流入某一市场，还是流出某一市场。因此，它就具备了上述用途。持仓量的上升或下降，取决于两方面因素的消长对比：新入市的买入者或卖出者的数额，与新出市的原有交易者的数额。 如果在上升趋势中持仓量是上升的，则一般来说，牛方占据主动，上涨行情仍将持续；如果在下降趋势中持仓量是上升的，则说明熊方占据主动，下跌行情仍将持续。 如果在市场上扬时，持仓量下降，那么这段上涨行情是由于空头买入平仓行为所引起的（以及原来的多头者趁机对冲头寸）。等到原有的这群空头者逃离了该市场，那么上述上涨行情（即空头买进平仓行情）背后的驱动力也随之瓦解，这就意味着当前的市场较为脆弱，容易诱发进一步的疲弱行情。 我们不妨打个比方，假定有一根水管接在自来水干线上。在自来水干线与这条水管之间安装了一个水龙头。上升的持仓量相当于打开了水龙头，从自来水干线向水管里输入自来水，那么，只要水龙头是开着的，就会不断地从水管里面流出水来（这就相当于上升的持仓量将价格进一步推高或推低）。下降的持仓量好像是关上了水龙头，开始，还有水从水管子里面流出来（因为水管于里面还有一些剩水），但是一旦剩水全部滴出来了，就不再有新的来源维持水流了。结果，水流（相当于价格趋势）就会枯竭。 当市场处于新高水平时，如果持仓量恰巧也达到了异乎寻常的高水平，可能预示着市场即将陷入困境，这是因为，上升的持仓量意味着新的空头者和新的多头者正在进入市场。如果市场处于稳步上涨的上升趋势中，那么，这些新的多头者将会随着价格水平的逐步上升，逐步提高卖出止损指令的水平。万一价格突然下滑，将引发执行止损指令的连锁反应，有可能导致价格的狂泻。 11， 期权为了确定一份期货期权合约的理论价格，我们需要五个方面的参数。它们分别是： - 执行价格 - 有效期限 - 基本市场的预测价格 - 基本市场的波动性 - 短期利率的水平 在这些自变量中，有三项是已知的（有效期限、执行价格、短期利率）。 在决定期权价格的所有因素中，绝不可以低估波动性的重要性。事实上，在某些情况下，波动性的变化对期权价格的影响，甚至超过了相应的期货合约的价格变化所带来的影响。 所谓波动性，指的是人们预期今后一年之内对应基本市场的价格变化范围（波动性水平式按年计算的）。 在期权交易中，必须对未来的市场波动性作出预测。预测市场波动性的一种方法是，由市场价格提供这样的预测值。事实上，这就是所谓蕴含波动性的来历。蕴含波动性，就是市场对期权有效期内相应的基本期货合约的波动性的估计，是蕴含在当前期权价格之中的波动性水平。","tags":[{"name":"期货","slug":"期货","permalink":"http://www.kekefund.com/tags/期货/"},{"name":"股票","slug":"股票","permalink":"http://www.kekefund.com/tags/股票/"},{"name":"K线","slug":"K线","permalink":"http://www.kekefund.com/tags/K线/"},{"name":"蜡烛图","slug":"蜡烛图","permalink":"http://www.kekefund.com/tags/蜡烛图/"}]},{"title":"日本蜡烛线形态","date":"2015-12-10T03:09:01.000Z","path":"2015/12/10/japan-candle-diagram-technique/","text":"日本蜡烛线形态 1，纺锤线实体较短，说明熊方与牛方正处于胶着状态，一时难分高下。 2，锤子线与上吊线下图中的蜡烛图线具有明显的特点：它们的下影线较长，而实体较小并且在其全天价格区间里，实体处在接近顶端的位置上。 锤子线：出现在下降趋势中时，成为锤子线，意思是说“市场正用锤子穷砸底部”。在日语中，这类蜡烛线原来的名称是“深水竿”，大体的意思是“试一下水的深浅”。 上吊线：出现在上冲行情之后，就表明之前的市场运动也许结束，称为上吊线。这类蜡烛线看上去像吊在绞刑架上双腿晃荡的一个死人。 注：锤子线或上吊线的实体颜色不重要，可以是白色或黑色。 锤子线和上吊线的识别标准 1、实体处于整个价格区间的上端。而实体本身的颜色是无所谓的。 2、下影线的长度至少达到实体高度的2倍。 3、在这类蜡烛线中，应当没有上影线，即使有上影线，其长度也是极短的。 4、锤子线之前，必定先有一段下降趋势（哪怕是较小规模的下降趋势），这样锤子线才能逆转这个趋势。 5、上吊线必须出现在一段上升趋势之后。 6、在上吊线出现后，还需要其它看跌信号的验证。这一原则的重要性，在下图中也得到了体现。在上吊线的看跌验证信号中，有一种情况是，次日的开市价低于上吊线的实体，这是为上吊线求得证实的第一个方法。 上吊线的实体与上吊线次日的开市价之间向下的缺口越大，那么上吊线就越有可能构成市场的顶部。在上吊线之后，如果市场形成了一条黑色的实体，并且它的收市价低于上吊线的收市价，那么，这也可以看作上吊线成立的一种佐证。 PS：上吊线的下影线长度并不是非得达到实体高度的2倍不可，才足以构成反转信号。一般来说，在这类形态中，下影线越长，形态就越完美。 3，吞没形态 看涨吞没形态 下图左中，市场本来处于下降趋势之中，但是后来出现了一根坚挺的白色实体，这个白色实体将它前面的那根黑色实体“抱进怀里了”，或者说把它吞没了。这种情形说明市场上买进的压力已经压倒了卖出压力。 看跌吞没形态 下图右中，市场原本正向着更高的价位趋升，但是当前一个白色实体被后一个黑色实体吞没后，就构成了顶部反转信号。这种情形说明，熊方已经从牛方手中夺走了统治权。 吞没形态的判别标准: 在吞没形态之前，市场必须处在清晰可辨的上升趋势或下降趋势中，哪怕这个趋势只是短期的。 吞没形态必须由2条蜡烛线组成。其中第二根蜡烛线的实体必须覆盖第一根蜡烛线的实体（但是不一定需要吞没前者的上下影线）。 吞没形态的第二个实体必须与第一个实体的颜色相反。这一条标准有例外的情况，条件是，第一条蜡烛线的实体必须非常小，小得几乎构成了一根十字线（或者它就是一根十字线）。如此一来，如果在长期的下降趋势之后，一个小小的白色实体为一个巨大的白色实体所吞没，那么也可能构成了底部反转形态。反之，在上升趋势中，如果一个小小的黑色实体为一个巨大的黑色实体所吞没，那么也可能构成顶部反转形态。 如果吞没形态具有下面的特征，那么它们构成重要反转信号的可能性将大大地增强: 在吞没形态中，第一天的实体非常小，而第二天的实体非常大。这种情况可能说明原有趋势的驱动力正在消退，而新趋势的潜在力量正在壮大。 吞没形态出现在超长期的或非常急剧的市场运动之后。如果存在超长期的上升趋势，则增加了以下这种可能性，潜在的买家已经入市买进，持有多头。在这种情况下，市场可能缺少足够的新的多头头寸的供应，无力继续推动市场上升。如果存在非常急剧的市场运动，则市场可能已经朝一个方向走得太远，容易遭受获利平仓头寸的打击。 在吞没形态中，第二个实体伴有超额的交易量这种情形可能属于胀爆现象。 在吞没形态中，第二天的实体向前吞没的实体不止一个。 4，乌云盖顶形态（乌云线形态）这种形态是由两根蜡烛线组成的，属于顶部反转形态。它们一般出现在上升趋势之后，在有些情况下也可能出现在水平调整区间的顶部。在这一形态中，第一天是一根坚挺的白色实体；第二天的开市价超过了第一天的最高价（这就是超过了第一天的上影线的顶端），但是，市场却收市在接近当日的最低价的水平，并且收市价明显地向下扎入到第一天白色实体的内部。 有些日本技术分析师要求，第二天黑色实体的收市价必须向下穿过前一天白色实体的50%。如果黑色实体的收市价没有向下穿过白色蜡烛线的中点，那么，我们最好等一等，看看是否还有进一步的看跌验证信号。 如果乌云盖顶形态具有这样的特征，则有助于增强其技术分量： 1、在乌云盖顶形态中，黑色实体的收市价向下穿入前一个白色实体的程度越深则该形态构成市场顶部的机会越大。如果黑色实体覆盖了前一天的整个白色实体，那就是看跌吞没形态。 2、乌云盖顶形态发生在一个超长期的上升趋势中，它的第一天是一根坚挺的白色实体，其开市价就是最低价（就是说，是秃脚的），而且其收市价就是最高价（就是说，是秃头的）；它的第二天是一根长长的黑色实体，其开市价位于最高价，而但收市价位于最低价（这是一个秃头秃脚黑色蜡烛线）。 3、在乌云盖顶形态中，如果第二个实体（即黑色的实体）的开市价高于某个重要的阻挡水平，但是市场未能成功地坚守住，那么可能证明牛方已经无力控制市场了。 4、如果在第二天开市的时候，市场的交易量非常大，那么这里就可能发生胀爆现象。具体说来，当日开市价创出了新高。而且开市时的成交量极重，可能意味着很多新买家终于下决心入市，踏上了牛市的“船”。随后，市场却发生了抛售行情。对期货交易商来说，极高的持仓量也是一种警告信号。 5，刺透形态（斩回线形态）刺透形态是乌云盖顶形态的反面形态，是底部反转信号。 斩回线形态出现在下跌的市场上，也是由两根蜡烛线组成的。其中第一根蜡烛线具有黑色实体，而第二根蜡烛线则具有长长的白色实体。在白色蜡烛线这一天，市场的开市价曾急剧地下跌至前一个黑色蜡烛线的最低价之下，但是不久市场又将价格推升回来，形成了一根相对较长的白色实体，并且其收市价已经向上超越了前一天的黑色实体的中点。 在斩回线形态中，白色蜡烛线的实体必须向上推进到黑色蜡烛线实体的中点之上。 6，星线星线的实体较小，并且在它的实体与它前面的较大的蜡烛线的实体之间形成了价格跳空。只要星线的实体与前一个实体没有任何重叠，那么这个星线就是成立的。 星线本身的颜色并不重要。星线既可能出现在市场的顶部，也可能出现在市场的底部。如果星线的实体已经缩小为十字线，称为十字星线。 当星线，尤其是十字星线出现时，就是一个警告信号，表明当前的趋势获取好景不长了。星线的较小的实体显示，熊方和牛方的较量已经转入僵持状态。 a、启明星形态启明星形态属于底部反转形态。它的名称的由来是，这个形态预示着价格的上涨就像启明星（水星）预示着太阳的升起一样。在本形态中，先是一根长长的黑色实体，随后是一根小小的实体，并且在这两个实体之间形成了一个向下跳空（这两条蜡烛线组成了基本的星线形态）。第三天是一根白色实体，它明显地向上推进到了第一天的黑色实体之内。 在理想的启明星形态中，中间蜡烛线（即星线）的实体，与它前、后两个实体之间均有价格跳空。后面的那个价格跳空较为少见，不过，即使没有后面这个价格跳空，似乎也不会削减启明星形态的技术效力。 b、黄昏星形态黄昏星是启明星的顶部对等形态，是看跌的、它的名称的由来也是显而易见的。因为黄昏星（金星）恰好出现在夜幕即将降临之际，既然黄昏星是顶部反转形态，那么，它只有出现在上升趋势之后，才能发挥其技术效力。 黄昏星形态是由三根蜡烛线组成的。在前面两根蜡烛线中，第一根是一根长长的白色实体，后一根是一根星线。星线的出现，是顶部形态的第一个征兆。第三根蜡烛线证实了顶部过程的发生，完成了这个三线形态的黄昏星形态。第三根蜡烛线具有黑色实体，它剧烈地向下扎入第一天的白色实体的内部。 黄昏星形态或启明星形态兼具这样的特征。则有助于增加它们构成反转信号的机会。 1、如果在第一根蜡烛线的实体与星蜡烛线的实体之间存在价格跳空，并且在星线的实体与第三根蜡烛线的实体之间也存在价格跳空。 2、如果第三根蜡烛线的收市价深深地向下扎入第一根蜡烛线的实体之内。 3、如果第一根蜡烛线的交易量较轻，而第三根蜡烛线的交易量较重。这一点表明了原先趋势力量的衰减，以及新趋势力量的增长。 c、十字线形态如果在上升趋势中出现了一根十字线，并且这个十字线与前一个实体之间形成了向上的价格跳空；或者在下降趋势中出现了一根十字线，并且与前一个实体之间形成了向下的价格跳空，那么这根十字线就称为十字星线。 十字星线的出现，构成了潜在的警告信号，表明市场的当前趋势已经易于发生变化了。 在上升趋势中，如果在十字星线后跟随着一根长长的黑色实体，并且它的收市价深深地向下扎入十字星线之前的白色实体的内部，那么，这根黑色实体就构成了市场顶部反转过程的验证信号。这样的形态就称为十字黄昏星形态。 在下降趋势中，如果在一根黑色实体之后，跟随着一根十字星线，第三根蜡烛线是一根坚挺的白色蜡烛线，并且它的收市价显著地向上穿入第一根黑色实体之内，那么，该底部反转信号就得到了第三根蜡烛线的验证，这个三蜡烛线形态称为十字启明星形态。 如果市场上出现了一根向上跳空的十字星线，它的后面再跟着一条向下跳空的黑色蜡烛线，并且在这根黑色蜡烛线的上影线与十字星线的下影线直接也形成了价格跳空，那么，这根十字星线就构成了一个主要顶部反转信号。这种形态称为弃婴顶部形态。非常罕见！ 弃婴底部形态与顶部对应。 d、流星形态流星形态是一种二蜡烛线形态，它发出警告信号，表明市场顶部就再眼前。它的外观如其名称，象一颗流星，本形态的技术意义不如黄昏星形态强，通常不构成主要反转信号。 在流星形态中，流星线具有较小的实体，而且实体处于其价格区间的下端，同时，流星线的上影线较长。 在理想的流星形态中，流星线的实体与前一根蜡烛线的实体之间存在价格跳空。不过价格跳空并不是非有不可。 e、倒锤子线在下降趋势后，如果出现了与流星线外观一致的蜡烛线，则可能构成一个看涨信号。这样的蜡烛线称为倒锤子线。 倒锤子线看上去与流星线颇为相象，它也具有较长的上影线，较小的实体，并且实体居于整个价格范围的下端。但不同的是，流星线是一根顶部反转蜡烛线，而倒锤子线却是一根底部反转蜡烛线。如果倒锤子线出现在下降趋势之后，则构成一个看涨的蜡烛图形态。 在分析倒锤子线时，有一点非常重要：当倒锤子线出现后，必须等待下一个时间单位的看涨信号对它加以验证。 倒锤子线的验证信号可能采取下面的形式： 倒锤子线次日的开市价向上跳空，超过了倒锤子线的实体。向上跳空的距离越大，验证信号就越强烈。 倒锤子线次日是一根白色蜡烛线，并且它的价格均处在较高的水平。 7，孕线形态后一根蜡烛线的实体较小，并且被前一根相对较长的实体包容进去。 孕线形态与吞没形态相比，两根蜡烛线的顺序恰好颠倒过来。在吞没形态中，后面是一根长长的实体，它将前一个小实体覆盖进去了。而在孕线形态中，前一个是非常长的实体，它将后一个小实体包容起来。 在吞没形态中，两根蜡烛线的实体的颜色应当互不相同，而在孕线形态中，这一点倒不是一项必要条件。 十字孕线形态指的是在孕线形态中，第二天是一根十字线，而不是一个小实体。因为十字孕线形态包含了一根强有力的十字蜡烛线，所以这类形态被视为主要反转信号。 十字孕线形态所蕴含的技术意义，比普通的孕线形态重要得多。一般的孕线形态并不属于主要反转形态，但是，十字孕线形态恰恰是一种主要反转形态。 8，平头顶部形态和平头底部形态平头形态是由具有几乎相同水平的最高点的两根蜡烛线组成，或者是由具有几乎相同的最低点的两根蜡烛线组成的。 图6.13，在上升趋势中，先是一根长长的白色蜡烛线，后是一根十字线。这个二蜡烛线形态既是一个十字孕线形态，又是一个平头形态，因为这两根蜡烛线具有同样的最高点。综合来看，这个形态可能构成了重要的反转信号。 图6.14，在这个平头形态中，先是一根长长的白色蜡烛线，后是一根上吊线。下一天，如果市场开市于上吊线的实体之下，那么，把这个形态判断为一个顶部反转信号，就有了很大的胜算。只要市场的收市价不高于这个平头形态的顶部，那么这种看跌的态度就不可动摇。 图6.15，在这个平头顶部形态中，第二根蜡烛线同时又是一根看跌的流星线，尽管它不是一根真正的流星线，但根据形成这根蜡烛线的价格变化过程，这根线本身是看跌的。 9，捉腰带线捉腰带线形态是单独一根蜡烛线构成的，既可能具有看涨的意义，也可能具有看跌的意义。 看涨捉腰带线形态是一根坚挺的白色蜡烛线，其开市价位于当日的最低点（或者，这根蜡烛线只有极短的下影线），然后市场一路上扬。如果市场处于低价区域，出现了一根长长的看涨捉腰带线，则预示着上冲行情的到来。 看跌捉腰带线形态是一根长长的黑色蜡烛线，它的开市价位于当日的最高点（或者这根蜡烛线只有极短的上影线），然后市场一路下跌。在市场处于高价区的条件下，看跌捉腰带形态的出现，构成了顶部反转信号。 10，向上跳空二只乌鸦下图为向上跳空二只乌鸦形态。“向上跳空”指的是图示的小黑色实体与它们之前的实体（即第一个小黑色实体之前的实体，通常是一根长长的白色实体）之间的价格跳空。 在理想的向上跳空二只乌鸦形态中，第二个黑色实体的开市价高于第一个黑色实体的开市价，并且它的收市价低于第一个黑色实体的收市价 这个形态在技术上看跌的理论依据大致如下：市场本来处于上升趋势中，并且这一天的开市价同前一天的收市价相比，是向上跳空的，可是市场不能维持这个新高水平，结果当天反而形成了一根黑色蜡烛线。 11，铺垫形态其外形与向上跳空二只乌鸦形态相似，但不同的是，在上升行情中，这类形态是看涨的。在这个形态中，头三根蜡烛线与向上跳空二只乌鸦形态相似，但此后，又跟了一根黑色蜡烛线。如果接下来的一根蜡烛线是白色的，并向上跳空，向上超过了上述最后一根黑色蜡烛的上影线，或者这根白色蜡烛线的收市价高于最后一根黑色蜡烛的最高价，则形成了买入信号。 在铺垫形态中，可以有2根、3根乃至4根黑色蜡烛线。 相对而言，向上跳空二只乌鸦形态和铺垫形态都很少见。 建议：当向上跳空二只乌鸦形态出现后，应当在该形态的第二根黑色蜡烛线的最高点上方设置止损保护指令，以防备市场收市于该水平之下。 P166 12，三只乌鸦类似于向上跳空两只乌鸦形态，连续出现了三根依次下降的黑色蜡烛线，则构成了所谓的三只乌鸦形态。如果三只乌鸦出现在高价格水平上，或者出现在经历了充分发展的上涨行情中，就预示着价格即将下跌。 13，反击线形态（约会线形态）当两根颜色相反的蜡烛线具有相同的收市价时，就形成了一个反击线形态（也称为约会线形态）。 刺透形态与本图所示的看涨反击线形态一样，也是由两根蜡烛线组成的。它们之间主要的区别是，看涨反击线通常并不把收市价向上推进到前一天的白色实体的内部，而是仅仅回升到前一天的收市价的位置。而在透刺形态，第二根蜡烛线深深地向上穿入了前一个黑色实体之内。因此，透刺形态与看涨反击线形态相比较，透刺形态是一种更为重要的底部反转信号。 在看跌反击线形态中，第二天的开市价高于前一天的最高点，这一点与乌云盖顶形态是一致的。但是，与乌云盖顶形态不同的是，这一天的收市价并没有向下穿入前一天的白色蜡烛线之内。由此看来，乌云盖顶形态所发出的顶部反转信号，比看跌反击线形态更强。 在反击线形态中，一项重要的考虑因素是，第二天的开市价是否强劲地上升到较高的水平（在看跌反击线形态中），或者是否剧烈地下降到较低的水平（在看涨反击线形态中）。其核心思想是，在该形态第二天开市时，市场本来已经顺着既有趋势向前迈了一大步，但是后来，却发生了意想不到的变故！到当日收市时，市场竟然完全返回到了前一天收市价的水平！ 14，塔形顶部形态 塔形顶部形态属于顶部反转形态。市场本来处在上升趋势中，在某个时刻，出现了一根坚挺的白色蜡烛线（或者出现了一系列高高的白色蜡烛线）。后来，市场先是放缓了上涨的步调，然后，蜡烛线的高点开始下降。最后，市场上出现了一根或数根长长的黑色蜡烛线，于是，塔形顶部形态就完成了。在本形态中，两侧的长长的蜡烛线形似高塔，因此得名。 塔形底部形态发生在低价格水平上，市场在形成了一根或数根长长的黑色蜡烛线之后，经历了一阵短暂的平静。然后，出现了一根或数根长长的白色蜡烛线。 15，窗口所谓窗口，是指在前一根蜡烛线的端点与后一根蜡烛线的端点之间存在一个价格缺口。如图左是在上升趋势中形成的一个“打开的”窗口。图右是在下降趋势中的一个窗口。 同时，窗口还将演化为支撑区或阻挡区。因此，在上涨行情中，如果出现了一个窗口，则意味着价格将进一步上升。并且，今后当市场向下回撤时，这个窗口将形成其底部支撑水平。 如果连续出现了8个新高而没有发生任何有意义的调整，那么，日本分析师便将这种市场状况描述为“胃口已经填满了八成”。 传统的日本技术分析理论断言：在一个趋势中，如果已经出现了3个向上的或向下的窗口（即所谓三空形态），那么，市场即将形成顶部（如果这三个窗口发生在上升趋势中）或底部（如果这三个窗口发生在下降趋势中）的机会是极大的，特别是在第三个窗口之后，如果出现了某种转折性的蜡烛图形态或蜡烛线（比如说十字线，透刺形态，或者乌云盖顶形态等），那么市场见顶或见底的机会更大了。 日本分析师认为，如果一个窗口（在上升趋势过程中）没有在三天之内被市场关闭，那么市场将上涨。** 16，向上跳空和向下跳空并列阴阳线形态是一种持续形态，它的形成过程大体是这样的。市场本处于上升趋势中，这时，出现了一根向上跳空的白色蜡烛线。在这根白色蜡烛线后，紧跟着另一根黑色的蜡烛线。这根黑色蜡烛线的开市价位于前一个白色实体之内，收市价位于前一个白色实体之下，在这样的情况下，这根黑色蜡烛线的收市价，就构成了一个买入点。如果在市场回头填补了这里的跳空（即关闭了该窗口）后，抛售压力依然很明显的话，那么这个向上跳空并列黑白蜡烛线形态的看涨意义就不再成立了。 在向下跳空并列阴阳线形态中，基本概念与上述向上跳空并列阴阳线是相同的，只不过方向相反。 17，高价位和低价位跳空突破形态在上升趋势中，当市场经历了一两个急剧上涨的交易日后，在正常情况下都需要一个调整消化的过程。有时，这个整理过程是通过一系列小实体来完成的。如果在一根坚挺的蜡烛线之后，出现了一群小实体的蜡烛线则表明市场已经变得犹豫不决了。然而，一旦后来某一天的开市价从这群小实体处向上跳空（也就是说，形成了一个窗口），那么买进的时机就成熟了。 低价位跳空突破形态正是高价位跳空突破形态的反面角色。 18，跳空并列白色蜡烛线形态在上升趋势中，先出现了一根向上跳空的白色蜡烛线，随后又是一根白色蜡烛线，并且后面这根线与前一根大小相当，两者的开市价也差不多处在同样的水平上，这样就形成了一种看涨的持续状态。这种二蜡烛线形态称为向上跳空并列白色蜡烛线形态（或者称为向上跳空并列阳线形态）。如果市场收市在并列白色蜡烛线的最高点之上，则意味着下一波上涨行情即将展开。 在下降趋势中，这类并列的白色蜡烛线也构成了一个持续状态。也就是说，当这类形态出现时，价格将继续走低。为什么这种形态不是看涨的，而是看跌的呢？这是因为在下降的市场中，这两根白色蜡烛线是由空头平仓过程造成的。一旦空头平仓的过程完成了，价格就要进一步下跌。 这两种形态非常少见。 19，上升三法和下降三法形态上升三法形态的判别标准，包括以下几个方面： 1，首先出现的是一根长长的白色蜡烛线 2，在这根白色蜡烛线之后，紧跟着一群依次下降的小实体蜡烛线。这群小实体蜡烛线的理想数目是3根。而且这群小实体蜡烛线基本上都局限在前面那根长长的白色蜡烛线的价格范围之内。小蜡烛线既可以是白色的，也可以是黑色的，不过，黑色蜡烛线最常见。 3，最后一天应当是一根具有坚挺的白色实体的蜡烛线，并且它的收市价高于第一天的收市价。同时，最后这根蜡烛线的开市价也应当高于前一天的收市价。 下降三法形态与上升三法形态在图形上完全是对等的，只不过方向相反而已。 20，前进白色三兵形态 前进白色三兵形态 本形态由接连出现的三根白色蜡烛线组成，它们的收市价依次上升。当市场在某个低价位稳定了一段时间后，如果出现了这样的形态，就标志着市场即将转强。 前方受阻形态 如果其中第二根和第三根蜡烛线，或者仅仅是第三根蜡烛线，表现出上涨势头减弱的迹象，就构成了一个前方受阻（白色三兵）形态。这就意味着这轮上涨行情碰到了麻烦，持有多头头寸者应当采取一些保护性措施。在前方受阻形态中，作为上涨势头减弱的具体表现，既可能是其中的白色实体一个比一个小，也可能是后两根蜡烛线具有相对较长的上影线。 停顿状态 如果在后两根蜡烛线中，前一根为长长的白色实体，并且向上刨出了新高，后一根只是一个小的白色蜡烛线，那么就构成了一个（白色三兵）停顿形态。当这一形态出现时，说明牛方的力量至少暂时已经消耗尽了。当停顿形态发生时，便构成了多头头寸平仓获利的紧要时机。 虽然前方受阻形态与停顿形态在一般情况下都不属于顶部反转形态，但是有时候，它们也能引起不容忽视的下跌行情。我们应当利用前方受阻形态和停顿形态来平仓了结已有的多头头寸，或者为多头头寸采取保护措施，但是不可据之开立空头头寸。一般来说，如果这两类形态出现在较高的价格水平上，则更有预测意义。 21，分手蜡烛线形态反击线形态是一种二蜡烛线形态，前后两根蜡烛线颜色相反，并且后一根蜡烛线的收市价与前一根的收市价处于同一水平。这一形态属于反转信号。如下图的分手线形态也是由两根颜色相反的蜡烛线组成的，但是同反击线形态不同的是，分手线形态的两根蜡烛线具有相同的开市价。分手蜡烛线形态属于持续信号。 22，十字线十字线是一种不同凡响的趋势反转信号。如果十字线之后的蜡烛线发出了验证信号，证实了它的反转信号的话，就进一步加大了趋势反转的可能性。 需要指出的是，只有在一个市场不经常出现十字线的条件下，十字线才具有重要意义。如果在某张蜡烛图上有许多十字线，那么当这个市场形成了一根新的十字线的时候，我们就不应当将它视为一条有意义的技术线索。 十字线之所以极具价值，是因为它在揭示市场顶部方面有过人之长。在上升趋势中，如果前面出现一根长长的白色蜡烛线，后面跟着一根十字线，这种情况尤其值得注意。为什么十字线出现在上升趋势中具有负面意义呢?这是因为十字线代表着市场处于犹豫不决心理状态。 但是根据我们的经验来看，在下降趋势中，十字线往往丧失了发挥反转作用的潜力。其中的原因可能是这样的：十字线反映了买方与卖方在力量对比上处于相对平衡状态。由于市场参与者抱着骑墙的态度，市场往往因为自身的重力而下坠。因此，当十字线出现时，在上升趋势中，市场可能向下反转，而在下降趋势中，市场则可能继续下跌。 长腿十字线 位于市场顶部的长腿十字线是一种特别重要的十字线。这类十字线具有长长的上影线和下影线，鲜明地表露出市场举棋不定的心理状态。 如果当日的开市价和收市价正好处在全日价格范围的中点，那么这种蜡烛线就称为黄包车夫（线）。 如果某根非十字线的蜡烛线具有很长的上影线，或者具有很长的下影线，并且其实体较小，这种蜡烛线就称为风高浪大线。如果出现了一群风高浪大线，也构成了一种反转形态。对日本分析师来说，非常长的上影线或非常长的下影线的形成——借用他们的话来描述——就表示市场“失去了方向感”。 墓碑十字线 当开市价和收市价位于当日的最低点时，就形成了一根墓碑十字线。 这类形态最突出的长处在于昭示市场顶部。在上涨行情中，该形态的上影线越长，所处的价格水平愈高，那么，这根墓碑十字线的技术意义就愈疲弱。 墓碑十字线与流星形态颇为相像。发生在市场顶部的墓碑十字线，实际上是流星形态的一种特殊情况。流星蜡烛线具有较小的实体，而墓碑十字线——作为一根十字线，甚至没有实体。墓碑十字线比流星形态更为疲软。 三星形态 三星形态非常罕见，但是是一种意义极其重大的反转形态。三星形态是由三根十字线组成的，中间的十字线是一根十字星蜡烛线。理想的三星形态如下图：","tags":[{"name":"期货","slug":"期货","permalink":"http://www.kekefund.com/tags/期货/"},{"name":"股票","slug":"股票","permalink":"http://www.kekefund.com/tags/股票/"},{"name":"K线","slug":"K线","permalink":"http://www.kekefund.com/tags/K线/"},{"name":"蜡烛图","slug":"蜡烛图","permalink":"http://www.kekefund.com/tags/蜡烛图/"}]},{"title":"PyQt4 生成exe打包文件","date":"2015-11-26T09:00:51.000Z","path":"2015/11/26/pyqt4-build-exe/","text":"PyQt4 生成exe打包文件 pyqt是跨平台的GUI平台，本文的UI设计，代码编写在mac下进行，编译成exe，并打包在win7下做的。python脚本语言，图形化平台不是其擅长的领域，一般都是直接运行脚本，这次因为客户需要一个“成型”的程序去外面给别人展示，故有了此文的背景。QT作为一个跨平台的开发环境，编写出一个窗口程序，然后打包成python文件是比较迅速的。麻烦的是打包成windows的exe文件，试过py2exe,pyinstaller，都不是很好用，py2exe根本出不来图形界面，最后用到cxfreeze这个工具，才得以顺利打包。 前言环境搭建参考：http://www.cnblogs.com/zouzf/p/4308912.html 一、Qt Designer设计界面安装Qt Designer，我的版本是5.2.1。设计出的界面如下，保存为.ui文件。 Qt的界面布局和MFC的比较类似，但它多了一层容器的概念。控件都放在容器Layout中，这点又和Android的手机布局比较相近。 UI布局教程参考：PyQt4 精彩实例分析 二、Qt布局文件.ui转换成.py文件mac和win系统下先将pyuic4命令加入环境变量。mac中我是直接把/etc/paths拷贝到桌面，添加pyuic4路径后再拷贝覆盖回去。（ps: etc下不能直接修改）在终端中执行： pyuic4 -x aaaaaaa.ui -o bbbbbb.py 即可将.ui文件转成py文件。 三、添加按钮动作这里实现了三个功能：上传文件，运行py脚本，打开另一个Qt窗口 1，上传文件a，post上传dlg = QFileDialog() filename = dlg.getOpenFileName() from os.path import isfile if isfile(filename): filename = str(filename) print type(filename) #dir_f = os.path.dirname(str(filename)) # ------ web post ----- # 在 urllib2 上注册 http 流处理句柄 register_openers() # headers 包含必须的 Content-Type 和 Content-Length # datagen 是一个生成器对象，返回编码过后的参数 datagen, headers = multipart_encode({&quot;myfile&quot;: open(str(filename), &quot;rb&quot;)}) # 创建请求对象 request = urllib2.Request(&quot;http://yourwebsite:8080/upload&quot;, datagen, headers) # 实际执行请求并取得返回 print urllib2.urlopen(request).read() b，通过 ftp 上传dlg = QFileDialog() filename = dlg.getOpenFileName() from os.path import isfile if isfile(filename): filename = str(filename) # ------- ftp -------- from ftplib import FTP ftp=FTP() ftp.set_debuglevel(2)#打开调试级别2，显示详细信息;0为关闭调试信息 ftp.connect(&apos;127.0.0.1&apos;,&apos;21&apos;)#连接 ftp.login(&apos;Administrator&apos;,&apos;password&apos;)#登录，如果匿名登录则用空串代替即可 print ftp.getwelcome()#显示ftp服务器欢迎信息 #ftp.cwd(dir_f) #选择操作目录 #filename=&apos;keys.xlsx&apos; bufsize = 1024#设置缓冲块大小 file_handler = open(filename,&apos;rb&apos;)#以读模式在本地打开文件 ftp.storbinary(&apos;STOR %s&apos; % os.path.basename(filename),file_handler,bufsize)#上传文件 ftp.set_debuglevel(0) file_handler.close() ftp.quit() print &quot;ftp up OK&quot; 2，运行py脚本在服务器上用web.py搭建web服务器，通过网页请求运行py文件。 #运行 def on_run_clicked(self): self.pushButton_run.setText(_translate(&quot;Dialog&quot;, &quot;运行中&quot;, None)) self.pushButton_run.setEnabled(False) response = urllib2.urlopen(&apos;http://%s:8080/run_video_search&apos; % self.ip, timeout=3) print response 这里采用的是同步的方式请求，服务器端的py脚本没执行完，则程序一直等待。不过设置了超时，过了3s返回超时错误，这种情况适合不需要得到服务器的反馈，只是执行远程py脚本而已。 3，打开另一个Qt窗口pyqt 用起来的比较简单，直接run qt对应的py类 #结果 def on_show_result(self): Dialog = QtGui.QDialog() ui = Ui_Result_Dialog() ui.setupUi(Dialog) Dialog.show() Dialog.exec_() 跳转过来的窗口如下，是一个数据库的查询界面。 四、打包cx_Freeze 支持跨平台，可在windows、linux，mac下使用。下载地址为(http://sourceforge.net/projects/cx-freeze/files/),,也可以直接通过pip安装 pip install cx_freeze 安装成功后，在C:\\Python27\\Lib\\site-packages\\cx_Freeze\\samples\\PyQt4中找到pyqt4的使用例子。查看setup.py import sys from cx_Freeze import setup, Executable base = None if sys.platform == &apos;win32&apos;: base = &apos;Win32GUI&apos; options = { &apos;build_exe&apos;: { &apos;includes&apos;: &apos;atexit&apos; } } executables = [ Executable(&apos;PyQt4app.py&apos;, base=base) ] setup(name=&apos;simple_PyQt4&apos;, version=&apos;0.1&apos;, description=&apos;Sample cx_Freeze PyQt4 script&apos;, options=options, executables=executables ) 把这个setup.py文件拷贝到你要打包py文件的目录，然后将setup.py中的“PyQt4app.py”改成你要打包的py文件。在cmd命令行，cd到当前目录，运行: python setup.py build 打包exe成功后，在当前目录下会生成build文件夹，在\\build\\exe.win32-2.7\\中找到exe后缀的文件，执行。 五、制作安装包采用的是Inno Setup 制作安装包，按照向导来生成.iss脚本，傻瓜化操作。","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"PyQt","slug":"PyQt","permalink":"http://www.kekefund.com/tags/PyQt/"},{"name":"exe","slug":"exe","permalink":"http://www.kekefund.com/tags/exe/"}]},{"title":"IP代理池的Python实现","date":"2015-11-17T09:41:46.000Z","path":"2015/11/17/pytho-ip-proxy/","text":"爬虫采集数据时，如果频繁的访问某个网站，会被封IP，有些是禁止访问3小时，有些是直接拉黑名单。为了避免被禁，一般采取的措施有三种： 放慢抓取的速度，设置一个时间间隔； 模拟浏览器行为，如采用Selenium + PhantomJS； 设置IP代理，定期更换代理IP，让网站不认为来自一个IP。 本文实现其中的第三种方法。国内提供IP代理的网站有很多，我们以其中的一个为例：http://www.haodailiip.com分为三步来实现这个IP抓取类： 解析网页中的IP和端口 Ping所有IP地址的连接速度 按速度从快到慢排序，保存到文件 一、解析网页中的IP和端口抓取网页采用的是 urlib + BeautifulSoup。解析网站：http://www.haodailiip.com/guonei/page，page=1,2…,10 def parse(url): try: page = urllib.urlopen(url) data = page.read() soup = BeautifulSoup(data, &quot;html5lib&quot;) print soup.get_text() body_data = soup.find(&apos;table&apos;, attrs={&apos;class&apos;:&apos;content_table&apos;}) res_list = body_data.find_all(&apos;tr&apos;) for res in res_list: each_data = res.find_all(&apos;td&apos;) if len(each_data) &gt; 3 and not &apos;IP&apos; in each_data[0].get_text() and &apos;.&apos; in each_data[0].get_text(): print each_data[0].get_text().strip(), each_data[1].get_text().strip() item = IPItem() item.ip = each_data[0].get_text().strip() item.port = each_data[1].get_text().strip() item.addr = each_data[2].get_text().strip() item.tpye = each_data[3].get_text().strip() self.ip_items.append(item) except Exception,e: print e BeautifulSoup默认的解析器是lxml，但对于这个网址，发现网页内容解析的不完整，于是用了解析性最好的 html5lib，速度上会稍慢。关于BeautifulSoup解析器的介绍见http://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id9。BS解析的过程是： 先找到table class=”content_table”的标签； 在从上面的内容中找所有tr 我们需要的信息在tr的td中 结果存入IPItem类。 IPItem的定义 class IPItem: def __init__(self): self.ip = &apos;&apos; # IP self.port = &apos;&apos; # Port self.addr = &apos;&apos; # 位置 self.tpye = &apos;&apos; #类型:http; https self.speed = -1 #速度 二、Ping所有IP地址的连接速度import pexpect def test_ip_speed(ip_items): tmp_items = [] for item in ip_items: (command_output, exitstatus) = pexpect.run(&quot;ping -c1 %s&quot; % item.ip, timeout=5, withexitstatus=1) if exitstatus == 0: print command_output m = re.search(&quot;time=([\\d\\.]+)&quot;, command_output) if m: print &apos;time=&apos;, m.group(1) item.speed = float(m.group(1)) tmp_items.append(item) ip_items = tmp_items 主要是利用pexpect模块调用系统的ping命令，上面代码在mac 10.11.1下测试通过。 三、按速度从快到慢排序，保存至文件保存至文件利用pandas模块，只需一句代码即可搞定。 先把ip_items转换成pandas的DataFrame； 排序，df.sort_index()，按’Speed’列排序； 结果写入Excel文件，to_excel() def save_data(self): df = DataFrame({&apos;IP&apos;:[item.ip for item in ip_items], &apos;Port&apos;:[item.port for item in self.ip_items], &apos;Addr&apos;:[item.addr for item in self.ip_items], &apos;Type&apos;:[item.tpye for item in self.ip_items], &apos;Speed&apos;:[item.speed for item in self.ip_items] }, columns=[&apos;IP&apos;, &apos;Port&apos;, &apos;Addr&apos;, &apos;Type&apos;, &apos;Speed&apos;]) print df[:10] df[&apos;Time&apos;] = GetNowTime() df = df.sort_index(by=&apos;Speed&apos;) now_data = GetNowDate() file_name = self.dir_path +&apos;ip_proxy_&apos; + now_data + &apos;.xlsx&apos; df.to_excel(file_name) 生成的excel文件如下：","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.kekefund.com/tags/爬虫/"},{"name":"IP代理","slug":"IP代理","permalink":"http://www.kekefund.com/tags/IP代理/"}]},{"title":"Python正则表达式","date":"2015-11-10T07:09:20.000Z","path":"2015/11/10/python-regex/","text":"许多语言处理任务都涉及模式匹配。例如,可以使用endswith(‘ed’)找出以“ed”结尾的词。正则表达式提出了一个更加强大和灵活的方法描述感兴趣的字符模式。在Python中使用正则表达式，需要使用import re导入re函数库。 下表为正则表达式基本元字符，其中包括通配符、范围和闭包 贪婪模式与非贪婪模式Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符。例如：正则表达式 “ab”，如果用于查找“abbbc”，将找到“abbb”。而如果使用非贪婪的数量词“ab\\?“，将找到”a“ *? 是一个固定的搭配，.和*代表可以匹配任意无限多个字符，加上？表示使用非贪婪模式进行匹配，也就是我们会尽可能短地做匹配。 (.*?)代表一个分组，在这个正则表达式中我们匹配了五个分组，在后面的遍历item中，item[0]就代表第一个(.*?)所指代的内容，item[1]就代表第二个(.*?)所指代的内容，以此类推。 re.S 标志代表在匹配时为点任意匹配模式，点 . 也可以代表换行符。 re模块一、re.search()使用正则表达式&lt;&gt;查找以ed结尾的词汇。使用函数re.search(p, s) 检查字符串s中是否有模式p。 import re import nltk In[12]: wsj = sorted(set(nltk.corpus.treebank.word())) In[13]: ws = [w for w in wsj if re.search(&apos;ed$&apos;, w)] In[15]: ws[:10] Out[15]: [u&apos;62%-owned&apos;,u&apos;Absorbed&apos;,u&apos;Advanced&apos;,u&apos;Alfred&apos;, u&apos;Allied&apos;, u&apos;Annualized&apos;, u&apos;Arbitrage-related&apos;, u&apos;Asked&apos;,u&apos;Atlanta-based&apos;, u&apos;Bermuda-based&apos;] 通配符“.”可以用来匹配任何单个字符。假设有一个8个字母组成的字谜，j是第三个字母，t是第六个字母。每个空白单元格用句点隔开。 In[16]: ws = [w for w in wsj if re.search(&apos;^..j..t..$&apos;, w)] In[18]: ws Out[18]: [u&apos;adjusted&apos;, u&apos;rejected&apos;] 匹配除元音字母之外的所有字母 [^aeiouAEIOU] ?:如果要使用括号来指定连接的范围，又不想选择要输出字符串，必须添加“?:”。 In[20]: re.findall(r&apos;^.*(?:ing|ly|ed|ies)$&apos;, &apos;processing&apos;) Out[20]: [&apos;processing&apos;] 演示如何使用符号：\\，{}，() 和 | In[20]: ws = [w for w in wsj if re.search(&apos;^[0-9]+\\.[0-9]+$&apos;, w)] In[21]: ws[:5] Out[21]: [u&apos;0.0085&apos;, u&apos;0.05&apos;, u&apos;0.1&apos;, u&apos;0.16&apos;, u&apos;0.2&apos;] In[22]: ws = [w for w in wsj if re.search(&apos;^[A-Z]+\\$$&apos;, w)] In[23]: ws Out[23]: [u&apos;C$&apos;, u&apos;US$&apos;] In[24]: ws = [w for w in wsj if re.search(&apos;^[0-9]{4}$&apos;, w)] In[26]: ws[:5] Out[26]: [u&apos;1614&apos;, u&apos;1637&apos;, u&apos;1787&apos;, u&apos;1901&apos;, u&apos;1903&apos;] In[27]: ws = [w for w in wsj if re.search(&apos;(ed|ing)$&apos;, w)] In[28]: ws[:5] Out[28]: [u&apos;62%-owned&apos;, u&apos;Absorbed&apos;, u&apos;According&apos;, u&apos;Adopting&apos;, u&apos;Advanced&apos;] 二、re.split()按照能够匹配的子串将string分割后返回列表。 re.split(pattern, string[,maxsplit])In[13]: raw = &quot;&quot;&quot;&apos;When I&apos;M a Duchess,&apos; she said to herself, (not in a very hopeful tone ... though), &apos;I won&apos;t have any pepper in my kitchen AT ALL. Soup does very ... well without--Maybe it&apos;s always pepper that makes people ... hot-tempered,&apos;...&quot;&quot;&quot; In[16]: re.split(r&apos; &apos;, raw) Out[16]: [&quot;&apos;When&quot;, &quot;I&apos;M&quot;, &apos;a&apos;, &quot;Duchess,&apos;&quot;,...] In[17]: re.split(&apos;[ \\t\\n]&apos;, raw) Out[17]: [&quot;&apos;When&quot;, &quot;I&apos;M&quot;, &apos;a&apos;, &quot;Duchess,&apos;&quot;,...] split(string[, maxsplit])In [1]: import re In [2]: p = re.compile(r&apos;\\d+&apos;) In [3]: p.split(&apos;one1two2three3four4&apos;) Out[3]: [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;, &apos;four&apos;, &apos;&apos;] 切分字符串Python自带的字符分割函数 &apos;a b c&apos;.split(&apos; &apos;) [&apos;a&apos;, &apos;b&apos;, &apos;&apos;, &apos;&apos;, &apos;c&apos;] 嗯，无法识别连续的空格 import re &gt;&gt;&gt; re.split(r&apos;\\s+&apos;, &apos;a b c&apos;) [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] 使用re，无论多少个空格都可正常分割 三、findall()findall函数返回的总是正则表达式在字符串中所有匹配结果的列表。 In [2]: p = re.compile(r&apos;\\d+&apos;) In [4]: p.findall(&apos;one1two2three3four4&apos;) Out[4]: [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;] In [5]: ss = &quot;adfad asdfasdf asdfas asdfawef asd adsfas &quot; In [6]: p = re.compile(&apos;((\\w+)\\s+\\w+)&apos;) In [7]: p.findall(ss) Out[7]: [(&apos;adfad asdfasdf&apos;, &apos;adfad&apos;), (&apos;asdfas asdfawef&apos;, &apos;asdfas&apos;), (&apos;asd adsfas&apos;, &apos;asd&apos;)] In [8]: p = re.compile(&apos;(\\w+)\\s+\\w+&apos;) In [9]: p.findall(ss) Out[9]: [&apos;adfad&apos;, &apos;asdfas&apos;, &apos;asd&apos;] 当给出的正则表达式中不带括号时，列表的元素为字符串，此字符串为整个正则表达式匹配的内容。 当正则表达式中带有多个括号时，列表的元素为多个字符串组成的tuple，tuple中字符串个数与括号对数相同，字符串内容与每个括号内的正则表达式相对应，并且排放顺序是按括号出现的顺序。 当给出的正则表达式中带有一个括号时，列表的元素为字符串，此字符串的内容与括号中的正则表达式相对应。 四、re.search()re.search函数会在字符串内查找模式匹配，只要找到第一个匹配就返回，如果字符串没有匹配，则返回None。 In [15]: text = &quot;JGood is a handsome boy, he is cool, clever, and so on...&quot; In [16]: m = re.search(r&apos;(\\w+)ome&apos;, text) In [20]: if m: ....: print m.group(0), m.group(1) ....: else: ....: print &apos;not search&apos; 其中 group(0）或group()匹配的是整个字符串，group(1)匹配的是第一个括号中内容。 五、re.match()re.match()和re.search()的区别：re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 In [30]: s1 = &quot;helloworld, i am 30!&quot; In [31]: w1 = &apos;world&apos; In [32]: m1 = re.match(w1, s1) In [33]: if m1: ....: print m1.group() ....: else: ....: print &quot;not find&quot; ....: not find 六、re.sub()re.sub用于替换字符串中的匹配项。下面的例子将字符串中的空格’ ‘替换成’-‘ In [2]: text = &quot;JGood is a handsome boy, he is cool, clever, and so on...&quot; In [3]: re.sub(r&apos;\\s+&apos;, &apos;-&apos;, text) Out[3]: &apos;JGood-is-a-handsome-boy,-he-is-cool,-clever,-and-so-on...&apos; re.sub的函数原型为：re.sub(pattern, repl, string, count) 其中第二个参数时替换后的字符串；第四个参数为替换个数。默认为0，表示每个匹配项都替换。 re.sub还允许使用函数对匹配项的替换进行复杂的处理。如： re.sub(r&apos;\\s&apos;, lambda m : &apos;[&apos; + m.group(0) + &apos;]&apos;, text, 0) 将字符串中的空格’‘替换为’[]’。 七、re.compile()可以把正则表达式编译成一个正则表达式对象。对于经常要用的正则表达式，可以提高一定的效率。 In [4]: text = &quot;JGood is a handsome boy, he is cool, clever, and so on...&quot; In [6]: regex = re.compile(r&apos;\\w*oo\\w*&apos;) In [7]: regex.findall(text) #查找所有包含’oo‘的单词 Out[7]: [&apos;JGood&apos;, &apos;cool&apos;] In [8]: regex.sub(lambda m : &apos;[&apos; + m.group(0) + &apos;]&apos;, text) # 将字符串中含有’oo‘的单词用[]括起来 Out[8]: &apos;[JGood] is a handsome boy, he is [cool], clever, and so on...&apos;","tags":[{"name":"Python","slug":"Python","permalink":"http://www.kekefund.com/tags/Python/"},{"name":"re","slug":"re","permalink":"http://www.kekefund.com/tags/re/"},{"name":"正则表达式","slug":"正则表达式","permalink":"http://www.kekefund.com/tags/正则表达式/"},{"name":"模式匹配","slug":"模式匹配","permalink":"http://www.kekefund.com/tags/模式匹配/"}]},{"title":"期货大作手风云录 读书笔记","date":"2015-11-06T07:09:20.000Z","path":"2015/11/06/fengyunlu-md/","text":"期货大作手风云录读书笔记一、趋势 投机法则之一：放鱼头，弃鱼尾，吃鱼肚。一个品种的趋势行情分为鱼头、鱼肚和鱼尾行情。鱼头行情往往是趋势行情初始起步阶段，你很难分期是反转还是反弹，所以很难吃到。鱼尾行情往往是一波趋势行情的末端，最大的特点是连续的逼空或者逼多，一方认输出局，所谓多头不死空头不止，这种鱼尾行情随时有可能反转或者巨幅震荡，不吃也罢。而在我看来，我们期货作手真正需要去大吃、狠吃的阶段是鱼肚行情，此时趋势已成，行情进三退一，这是才是我们需要重仓且不断浮盈加仓大口吃肉的好时机。 判断大势的方法有很多，有通过技术面判断的，也有通过基本面判断的，但今天要强调只有两点：第一，政治大于经济。当你做多或者做空某商品时，首先考虑的不应该是供需关系，而是该商品的政治因素。简单点说，现在的世界超级大国仍然是美国，如果你发现近期由于政治原因需要石油价格上涨时，即使供需和技术面都支持你做空，你也应该果断放弃。特别是石油、铜、黄金这类关系到美国切身利益的期货品种，尤其不能和美国政府对着干。 关于如何判断大势的第二点，我要强调的是：整体趋势大于个体趋势。具体来说，当大多数商品都处于上涨周期时，不要轻易去做空任何一个商品，特别是对于股票市场来说，如果现在大势处于熊市，即使基本面和技术面再好的股票，也不要去做多。切记这一点。 如果市场处于牛市中，而某个商品或者股票没有上涨，这反而是去做多它的好时机呢？ 错。做多只做龙头，做空只做熊头。记住，股价永远不会因为太高二停止上涨，同样的，股价也永远不会因为太低而停止下跌。重势不重价。只有龙头和熊头才会带给你超额的利润，其他的品种根本没必要去关注他们。 期货市场的行情分为趋势行情和震荡行情。市场的80%的时间都处于震荡行情之中，只有20%的趋势行情才能让我们赚大钱。但遗憾的是，绝大多数投机者总想抓住市场的每一个机会，总想像上班一样每天都能赚到钱，结果是在80%的震荡行情中赔掉了大部分的钱。周而复始，恶性循环。 二、法则 “止语”的修行方法来自佛法的戒定慧：先戒，戒能生定，而定后能生慧。此乃佛法的博大精深也！ 行情在绝望中产生，行情在犹豫中发展，行情在热闹中高潮，行情在兴奋中灭亡。 有人的地方就有江湖，有资金的地方就有大行情。这句话在期货里一点不假。 草原生物链中最厉害的角色是鳄鱼。因为鳄鱼有足够的耐心、信心和狠心。所谓耐心，就是指鳄鱼从来不随便游来游去去寻找猎物，这样多半是白白耗费体力。鳄鱼总是耐心的潜伏在水塘里，趴在猎物必须要喝水的地方，静静地耐心等待猎物自己找上门来；所谓信心，就是指鳄鱼不管几天没有遇到猎物，即使他已经饥肠辘辘，但他还会继续有信心的待在原地等待猎物，直到猎物出现；所谓狠心，就是指一旦猎物出现在鳄鱼的射程之内，它会毫不犹豫的发起攻击，一击致命，绝不嘴软，直到把猎物彻底咬死。鳄鱼的这三个特点，决定了他能在物竞天择的生物链里顽强的活到现在。而这三个特点，也是我们期货作手最需要具备的。 简单的事实是，行情总是先发生变化，然后才有经济新闻，市场不会对经济新闻作出反应。市场是活的，它反映的是将来。因此，企图genuine当前的经济新闻和当前的事件预测股市的走势是非常愚蠢的。贪婪和恐惧一样，都会扭曲理性。股市只讲事实，只讲现实，只讲理性，股市永远不会错，错的是交易者。 投资法则：只相信自己的眼睛，不要相信自己的耳朵！只相信自己的大脑，不要相信别人的建议！ 期货最怕的就是快！很多时候，快就是慢！快的同义词就是风险。当你的账户盈利曲线越陡峭，你越应该引起高度的警惕，因为快速的都是短暂的，只有缓慢的才是稳定的。切记，做期货比的不是谁赢得多，而是看谁活得长。切记！ 人如果犯错误，那用不了一个月就能拥有整个世界。但是反过来，如果一个人不能从自己的错误中汲取经验教训，他迟早一文不名。 投机于赌博的最大区别在于是否拥有大局观。 三、操作 期货的魅力在于浮盈加仓，只有不断的浮盈加仓，我们才能赚取超额的巨大利润。但浮盈加仓的勇气来源于哪里？只有不断的盈利才是我们加仓的勇气！这就像你去带一支军队打仗，什么才是你们官兵不断勇敢冲锋的动力？唯有胜利！只有你的账户不断盈利了，你才有勇气去加仓，去继续以小搏大。 期货行业内有一句话，叫做：赚或赔靠本事，赚多赚少靠运气。也就是说，如果你赔了，那是你分析不到位，说明你没本事，如果你赚了，那么赚多赚少就要靠运气和天意了。 从某种意义上来说，投资并不是一个天道酬勤的行业。这个行业的秘诀在于节奏。会买的是徒弟，会卖的是师父，会空仓休息的是大事。对于期货作手来说，等待和寂寞也许是陪伴他一生的修炼法门。 既然是重仓，为什么利弗莫尔还要逐步建仓？为什么不能一次性重仓呢？那是因为谁也不敢保证自己一定是对的。只有账单上的盈利数字会告诉你对还是错，盈利了就加仓，亏损了就止损，让盈利奔跑，让亏损尽快了结。这是投机的不二法则。 一旦我们试仓之后市场朝着有利于我们的方向行进，我们有了浮盈，那么就开始倒金字塔加仓，就像利弗莫尔那样，1，2，4，8似的加仓，这样你的仓位自然就上来了，记住，股票永远不会因为价格太高而不可买进，或者因为价格太低而不可卖出。在你第一笔交易之后，除非第一笔交易有利润，否则就不可做第二笔。切记！ 股市不是法庭，他不会跟你讲道理。股市更像是战场，他只会用实力去说话。打仗靠枪，炒股靠钱。所以，钱就是股票上涨的最终动力。当你发现一只股票价格开始不断上涨时，我们不用去刻意分析股票上涨的背后原因是什么，不要去打探他的消息，也不用去分析他的基本面有何变化，你只需去判断一个问题，是不是有大资金入场买股票了？只要有钱进来，什么技术面，什么基本面，什么重组消息，都会陆陆续续浮出水面的。原因很简单，大资金绝不会随随便便买入一只股票，他一定会把后面的故事给你讲得圆圆满满的！ 原来人面来到投机市场的目的是不同的：有人是来过瘾的，因为频繁交易本身会带来快感；有的是来逛街的，只看不买，娱乐而已；还有极少数人是来赚钱的，他们需要时刻克制自己交易的冲动。。。 有句话说得好：在市场里，多头可以赚钱，空头也可以赚钱，但滑头早晚会死掉。 市场趋势显示下跌，但政府希望绿豆止跌，当市场和政府观点相反时，我选择空仓观望。期货比的不是谁赚的最多，而是比谁活得最长。方向不明时，空仓就是最好的策略。 期货往往就是这样，一开始就出现浮亏的持仓往往最终都很难赚钱。好的开始是成功的一半，这话在期货市场一点不假。 期货是个概率游戏。我们这些期货作手终生追求的目标只有一个：大赚小赔。也就是说，100次交易中，你失败90次也没关系，关键是如何在你作对的10次交易中，把盈利放大到远远超过你做错的90次亏损。 绝不能让自己盈利的单子变为亏损！这是期货投机的重要原则。以做多为例，随着你的分步建仓，你的持仓成本是在不断提高的，由于你是浮盈加仓，所以你应该是一直处于盈利中的，而盈利中的单子需要设立止盈点，这个止盈点就应该设立在你的持仓成本之上。一旦市场发生反向波动，如果触及到你的止盈点，你就应该毫不犹豫的平仓止盈。绝对不能让你原来盈利的单子面临亏损的可能，这是原则问题。止盈点因人而异，我的个人习惯是把止盈点设在最近的一次加仓点上。我的逻辑很简单：如果市场价格跌破了我的最近一次加仓点，那至少说明我这最后一次加仓的时机是错误的。既然错误了，我就要付出代价，所谓的代价就是全部平仓出局观望。我绝对不允许自己的盈利单变成亏损单。 在我看来，股票基本面分析的精髓不在于分析公司未来的盈利能力，而在于分析公司股票的供求关系。简单来说，就是在股市里，到底是买股票的人多？还是卖股票的人多？如果在一段时间内，主动卖股票的人大大多于卖股票的人，那就是供不应求，股价就会持续上涨，反之，如果主动卖股票的人多于买股票的人，那就是供大于求，股价就会持续下跌。供求决定价格，这才是我理解的基本面分析。","tags":[{"name":"期货","slug":"期货","permalink":"http://www.kekefund.com/tags/期货/"},{"name":"股票","slug":"股票","permalink":"http://www.kekefund.com/tags/股票/"}]}]